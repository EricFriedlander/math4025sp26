---
title: 'MATH 427: Intro to Machine Learning'
author: Eric Friedlander
footer: "[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---

```{r setup}
#| include: false
library(tidyverse)
# library(ggformula)
# library(gridExtra)
library(ISLR2)
library(GGally)
library(gridExtra)
library(kableExtra)
library(plotly)
```

## Pre-Survey

Take 10 minute and fill our [this survey](https://forms.office.com/Pages/ResponsePage.aspx?id=221GfoWP4U6xMj_yZFxlSQJyt-ruO2lEkG4nYFtEh4RUNE80TE85UlZVQ0FGRjVZNVE1S1kwWVM4WiQlQCNjPTEu).

If you chose to opt-out please read as much of [this article](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) as you an in 8 minutes and then fill out [this survey](https://forms.office.com/r/E7nresw72K). 

## Data Generating Process

Suppose we have

-   Features: $\mathbf{X}$
-   Target: $Y$
-   Goal: Predict $Y$ using $\mathbf{X}$

. . .

-   **Data generating process**: underlying, unseen and unknowable
    process that generates $Y$ given $\mathbf{X}$

## Population

More mathematically, the "true"/population model can be represented by

$$Y=f(\mathbf{X}) + \epsilon$$

where $\epsilon$ is a **random** error term (includes measurement error,
other discrepancies) independent of $\mathbf{X}$ and has mean zero.

. . .

*GOAL*: Estimate $f$

## Why Estimate $f(\mathbf{X})$? {.smaller}

We wish to know about $f(\mathbf{X})$ for two reasons:

1.  Prediction: make an educated guess for what $y$ should be given a
    new $x_0$:
    $$\hat{y}_0=\hat{f}(x_0) \ \ \ \text{or} \ \ \ \hat{y}_0=\hat{C}(x_0)$$
2.  Inference: Understand the relationship between $\mathbf{X}$ and $Y$.

. . .

-   An ML algorithm that is developed mainly for predictive purposes is
    often termed as a **Black Box** algorithm.

## Prediction {.smaller}

There are two types of prediction problems:

-   **Regression** (response $Y$ is quantitative): Build a model
    $\hat{Y} = \hat{f}(\mathbf{X})$
-   **Classification** (response $Y$ is qualitative/categorical): Build
    a classifier $\hat{Y}=\hat{C}(\mathbf{X})$

. . .

-   Note: a "hat", $\hat{\phantom{f}}$, over an object represents an
    estimate of that object
    -   E.g. $\hat{Y}$ is an estimate of $Y$ and $\hat{f}$ is an
        estimate of $f$

## Prediction and Inference

**Income dataset**

![Why ML? (from ISLR2)](images/02/2_2-1.png){.r-stretch}

## Prediction and Inference

**Income dataset**

::: {layout-ncol="2"}
![](images/02/2_3-1.png)

![](images/02/2_4-1.png)

Why ML? (from ISLR2)
:::

## [Question!!!]{style="color:blue"} {.smaller}

Based on the previous two slides, which of the following statements are
correct?

::: panel-tabset
## Questions

1.  As `Years of Education` increases, `Income` increases, keeping
    `Seniority` fixed.
2.  As `Years of Education` increases, `Income` decreases, keeping
    `Seniority` fixed.
3.  As `Years of Education` increases, `Income` increases.
4.  As `Seniority` increases, `Income` increases, keeping
    `Years of Education` fixed.
5.  As `Seniority` increases, `Income` decreases, keeping
    `Years of Education` fixed.
6.  As `Seniority` increases, `Income` increases.

## Answers

1.  As `Years of Education` increases, `Income` increases, keeping
    `Seniority` fixed. **TRUE**
2.  As `Years of Education` increases, `Income` decreases, keeping
    `Seniority` fixed. **FALSE**
3.  As `Years of Education` increases, `Income` increases. **TRUE**
4.  As `Seniority` increases, `Income` increases, keeping
    `Years of Education` fixed. **TRUE**
5.  As `Seniority` increases, `Income` decreases, keeping
    `Years of Education` fixed. **FALSE**
6.  As `Seniority` increases, `Income` increases. **TRUE**
:::

## Discussion

What's the difference between these two statements:

1.  As `Years of Education` increases, `Income` increases, keeping
    `Seniority` fixed.
2.  As `Years of Education` increases, `Income` increases.

<!-- ## [Question!!!]{style="color:blue"} {.smaller} -->

<!-- Which of the following statements are correct? -->

<!-- 1. The increase in `Income` resulting from increase in `Years of Education` keeping other variables fixed is **more** than the increase in `Income` resulting from increase in `Seniority` keeping other variables fixed. -->

<!-- 2. The increase in `Income` resulting from increase in `Years of Education` keeping other variables fixed is **less** than the increase in `Income` resulting from increase in `Seniority` keeping other variables fixed. -->

## How Do We Estimate $f(\mathbf{X})$?

Broadly speaking, we have two approaches.

1.  Parametric methods
2.  Non-parametric methods

## Parametric Methods

-   Assume a functional form for $f(\mathbf{X})$
    -   Linear Regression:
        $f(\mathbf{X})=\beta_0 + \beta_1 \mathbf{x}_1 + \beta_2 \mathbf{x}_2 + \ldots + \beta_p \mathbf{x}_p$
    -   Estimate the parameters $\beta_0, \beta_1, \ldots, \beta_p$
        using labeled data
-   Choosing $\beta$'s that minimize some error metrics is called
    **fitting** the model
-   The data we use to fit the model is called our **training data**

## Parametric Methods {.smaller}

![Parametric model fit (from ISLR2)](images/02/2_2-1.png){.r-stretch}

::: incremental
-   What are some potential parametric models that could result in this
    picture?
-   Note: Right line is the true relationship
:::

## Parametric Methods {.smaller}

**Income dataset**

::: {layout-ncol="2"}
![True relationship](images/02/2_3-1.png){width="60%"}

![Parametric model](images/02/2_4-1.png){width="60%"}

From ISLR2
:::

::: incremental
-   What are some functions that could have resulted in the model on the
    right?
-   $\text{Income} \approx \beta_0 + \beta_1\times\text{Years of Education} + \beta_2\times\text{Seniority}$
:::

## Non-parametric Methods {.smaller}

-   Non-parametric approach: no explicit assumptions about the
    functional form of $f(\mathbf{X})$
-   Much more observations (compared to a parametric approach) required
    to fit non-parametric model
    -   **Idea:** parametric model restricts space of possible answers

**Income dataset**

::: {layout-ncol="2"}
![True relationship](images/02/2_3-1.png){width="50%"}

![Non-parametric model fit](images/02/2_5-1.png){width="50%"}

From ISLR2
:::

## Supervised Learning: Flexibility of Models {.smaller}

-   Flexibility: smoothness of functions
-   More theoretically: how many parameters are there to estimate?

```{r}
#| echo: FALSE
#| r-stretch: TRUE
#| fig-align: 'center'

set.seed(208)

# simulate data

x <- runif(n = 100, min = 20, max = 40)   # input/predictor

e <- rnorm(n = 100, mean = 0, sd = 1)  # error

a <- 3

b <- 0.87

c <- 0.5

fx <- a + (b * sqrt(x)) + (c * sin(x))   # true function

y <- fx + e    # observed responses

toy_data <- data.frame(input = x, true_form = fx, response = y)   # create data frame to store values


# plot linear model (red) and non-linear model (blue) 
g3 <- ggplot(data = toy_data, aes(x = input, y = response)) + 
  geom_point() + 
  geom_function(fun = function(x) a+(b*sqrt(x))+(c*sin(x)), aes(color = "true model"), linewidth = 1.5) + 
  geom_smooth(method = "lm", se = FALSE, aes(color = "linear model")) + 
  geom_smooth(formula = y ~ sqrt(x) + sin(x), se = FALSE, aes(color = "non-linear model")) +
  scale_color_manual(values = c("true model" = "red", "linear model" = "blue", "non-linear model" = "green")) + 
  theme(legend.title = element_blank(),
       text = element_text(size=20)) +
  labs(title = "Comparing two models", y = "y", x = "x")

g3

```

[More flexible $\implies$ More complex $\implies$ Less Smooth $\implies$
Less Restrictive $\implies$ Less Interpretable

## Supervised Learning: Some Trade-offs {.smaller}

-   Prediction Accuracy versus Interpretability
-   Good Fit versus Over-fit or Under-fit

![Trade-off between flexibility and interpretability (from
ISLR2)](images/02/2_7-1-01.png)

## Supervised Learning: Selecting a Model {.smaller}

-   Why so many different ML techniques?
-   **There is no free lunch in statistics**: All methods have different
    pros and cons
    -   Must select correct model for each use-case
-   Relevant questions in model selection:
    -   How much observations $n$ and variables $p$?
    -   What is the relative importance is prediction, interpretability, and inference?
    -   Do we expect relationship to be non-linear?
    -   Regression or classification?

## Supervised Learning: Assessing Model Performance {.smaller}

-   When we estimate $f(\mathbf{X})$ using $\hat{f}(\mathbf{X})$, then,

$$\underbrace{E\left[Y-\hat{Y}\right]^2}_{Error}=E\left[f(\mathbf{X})+\epsilon - \hat{f}(\mathbf{X})\right]^2=\underbrace{E\left[f(\mathbf{X})-\hat{f}(\mathbf{X})\right]^2}_{Reducible} + \underbrace{Var(\epsilon)}_{Irreducible}$$

-   $E\left[Y-\hat{Y}\right]^2$: Expected (average) squared difference
    between predicted and actual (observed) response, **Mean Squared Error (MSE)**
-   Goal: find an estimate of $f(\mathbf{X})$ to minimize the reducible
    error

## Supervised Learning: Assessing Model Performance {.smaller}

::: {style="font-size: 75%;"}
-   Labeled training data $(x_1,y_1), (x_2, y_2), \ldots, (x_n,y_n)$
    -   i.e. $n$ training observations
-   Fit/train a model from training data
    -   $\hat{y}=\hat{f}(x)$, regression
    -   $\hat{y}=\hat{C}(x)$, classification\
-   Obtain estimates $\hat{f}(x_1), \hat{f}(x_2), \ldots, \hat{f}(x_n)$
    (or, $\hat{C}(x_1), \hat{C}(x_2), \ldots, \hat{C}(x_n)$) of training
    data
-   Compute error:
    -   **Regression**
        $$\text{Training MSE}=\text{Average}_{Training} \left(y-\hat{f}(x)\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left(y_i-\hat{f}(x_i)\right)^2$$
    -   **Classification** $$
        \begin{aligned}
        \text{Training Error Rate}
        &=\text{Average}_{Training} \ \left[I \left(y\ne\hat{C}(x)\right) \right]\\
        &= \frac{1}{n} \displaystyle \sum_{i=1}^{n} \ I\left(y_i \ne \hat{C}(x_i)\right)
        \end{aligned}
        $$
:::

<!-- ## Supervised Learning: Assessing Model Performance {.smaller} -->

<!-- -   In general, not interested in performance on training data -->
<!-- -   Want: performance on unseen test data... why? -->
<!-- -   Fresh test data: -->
<!--     $(x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \ldots, (x_m^{test},y_m^{test})$. -->
<!-- -   Compute test error: -->
<!--     -   **Regression** -->
<!--         $$\text{Test MSE}=\text{Average}_{Test} \left(y-\hat{f}(x)\right)^2 = \frac{1}{m} \displaystyle \sum_{i=1}^{m} \left(y_i^{test}-\hat{f}(x_i^{test})\right)^2$$ -->
<!--     -   **Classification** -->
<!--         $$\text{Test Error Rate}=\text{Average}_{Test} \ \left[I \left(y\ne\hat{C}(x)\right) \right]= \frac{1}{m} \displaystyle \sum_{i=1}^{m} \ I\left(y_i^{test} \ne \hat{C}(x_i^{test})\right)$$ -->

<!-- ## Supervised Learning: Bias-Variance Trade-off {.smaller} -->

<!-- -   Model fit on training data $\hat{f}(x)$ -->
<!-- -   "True" relationship: $Y=f(x)+\epsilon$ -->
<!-- -   $(x_0^{test}, y_0^{test})$: test observation -->
<!-- -   Bias-Variance Trade-Off (Theoretical) -->
<!--     $$\underbrace{E\left(y_0^{test}-\hat{f}(x_0^{test})\right)^2}_{total \ error}=\underbrace{Var\left(\hat{f}(x_0^{test})\right)}_{source \ 1} + \underbrace{\left[Bias\left(\hat{f}(x_0^{test})\right)\right]^2}_{source \ 2}+\underbrace{Var(\epsilon)}_{source \ 3}$$ -->
<!--     where -->
<!--     $Bias\left(\hat{f}(x_0)\right)=E\left(\hat{f}(x_0)\right)-f(x_0)$ -->

<!-- . . . -->

<!-- -   Question: Where is $\hat{y}_0^{test}$? -->

<!-- ## Supervised Learning: Bias-Variance Trade-off -->

<!-- -   Reducible Error: -->
<!--     -   Source 1: how $\hat{f}(x)$ varies among different randomly -->
<!--         selected possible training data (**Variance**) -->
<!--     -   Source 2: how $\hat{f}(x)$ (when predicting the test data) -->
<!--         differs from its target $f(x)$ (**Bias**) -->
<!-- -   Irreducible Error: -->
<!--     -   Source 3: how $y$ differs from "true" $f(x)$ -->

<!-- ## Bias-Variance Trade-off: Example {.smaller} -->

<!-- -   For now: focus on regression problems (ideas extend to -->
<!--     classification) -->
<!-- -   Consider: three different examples of simulated "toy" datasets and -->
<!--     three types of models ($\hat{f}_i(.)$) -->
<!--     +   Linear Regression [**orange**]{style="color:orange"} -->
<!--     +   Smoothing Spline 1 [**blue**]{style="color:cornflowerblue"} -->
<!--     +   More flexible Smoothing Spline 2 -->
<!--         [**green**]{style="color:green"} -->
<!-- -   "True" (simulated) function $f(.)$ [**black**]{style="color:black"} -->
<!-- -   [**Training Error**]{style="color:grey"} -->
<!-- -   [**Test Error**]{style="color:red"} -->

<!-- ## Bias-Variance Trade-off: Example -->

<!-- ![From ISLR2](images/02/2_9-1.png) -->

<!-- ## Bias-Variance Trade-off: Example -->

<!-- ![From ISLR2](images/02/2_10-1.png) -->

<!-- ## Bias-Variance Trade-off: Example -->

<!-- ![From ISLR2](images/02/2_11-1.png) -->

<!-- ## Bias-Variance Trade-off: Example -->

<!-- ![From ISLR2](images/02/2_12-1.png) -->

<!-- ## [Question!!!]{style="color:blue"} -->

<!-- As flexibility increases: -->

<!-- ::: panel-tabset -->

<!-- ## Questions -->

<!-- 1. its variance (increases/decreases) -->
<!-- 2. its bias  (increases/decreases) -->
<!-- 3. its training MSE  (increases/decreases) -->
<!-- 4. its test MSE  (describe) -->

<!-- ## Answers -->
<!-- 1. its variance  (**increases**) -->
<!-- 2. its bias  (**decreases**) -->
<!-- 3. its training MSE  (**decreases**) -->
<!-- 4. its test MSE  (**decreases at first, then increases and the model starts to overfit, U-shaped**) -->

<!-- ::: -->

## Recap

-   Regression vs. Classification
-   Parametric vs. non-parametric models
-   Training v. test data
-   Assessing regression models: Mean-Squared Error
-   Trade-offs:
    +   Flexibility vs. interpretability
    +   Bias vs. variance