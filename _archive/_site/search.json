[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "",
    "text": "A study of methods to construct and evaluate predictive models. Topics may include lasso and ridge regression, naive Bayes, random forests, support vector machines, gradient boosting, and neural networks. The course will require a significant data analysis and modeling project."
  },
  {
    "objectID": "index.html#catalog-description",
    "href": "index.html#catalog-description",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "",
    "text": "A study of methods to construct and evaluate predictive models. Topics may include lasso and ridge regression, naive Bayes, random forests, support vector machines, gradient boosting, and neural networks. The course will require a significant data analysis and modeling project."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "Instructor",
    "text": "Instructor\nEric is an Assistant Professor in the Department of Mathematics and Physical Sciences at the College of Idaho. His expertise lies in the areas of probability, statistics, and statistics education. He joined the College of Idaho faculty in 2024 after spending three years as an Assistant Professor at St. Norbert College in De Pere, Wisconsin. He received his bachelor’s degree in mathematics and statistics from Rice University in 2011. After earning his degree, he worked for Capital One for two years in their home loans division before enrolling in graduate school. In 2018, Eric received his Ph.D. in statistics and operations research from the University of North Carolina at Chapel Hill studying under professor Amarjit Budhiraja. His dissertation work focused on modeling and analyzing large systems which arise in industrial engineering (e.g. large server and communication networks). Following his Ph.D., Eric did a postdoc in the Department of Ecology & Evolution at the University of Chicago under the direction of professor Matthias Steinrücken where he used stochastic processes to study natural selection and population genetics.\nOutside of school, Eric is an avid fan of the New York Giants, New York Knicks, and North Carolina Tarheels. In addition, he enjoys comic books, the Fast and the Furious franchise, and spending time with his lovely wife Maria and lovable dogs Allie, Tony, and Miriam."
  },
  {
    "objectID": "jobs/job-app-1-rubric.html",
    "href": "jobs/job-app-1-rubric.html",
    "title": "Job Application 1 Rubric",
    "section": "",
    "text": "You overall job application grade will be 15% from the resume, 15% from the cover letter, and 70% from the data analysis.\nTip: When the word “relevant” is used, look at the job advertisement to decide what is relevant.\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nExcellent (4)\nGood (3)\nAverage (2)\nNeeds Improvement (1)\n\n\n\n\nContact Information\nComplete, professional, and easy to find.\nComplete and professional.\nComplete but not easily found.\nIncomplete or unprofessional.\n\n\nSkills\nComprehensive list of relevant technical skills, including proficiency levels.\nGood list of relevant technical skills.\nAdequate list, but missing some key skills.\nLimited list of relevant skills.\n\n\nExperience and/or Projects\nDetailed descriptions of relevant experience, with quantifiable achievements. Detailed descriptions of relevant projects, with clear outcomes and technologies used.\nGood descriptions of relevant experience, with some quantifiable achievements. Adequate descriptions of projects, but lacks detail on outcomes or technologies used.\nAdequate descriptions, but lacks quantifiable achievements.\nLimited descriptions, with few or no quantifiable achievements and few or no relevant projects listed.\n\n\nEducation\nRelevant degrees or certifications, with details on coursework or projects.\nRelevant degrees or certifications.\nAdequate education listed, but lacks detail.\nLimited education listed, with little relevance to the role.\n\n\nSoft Skills\nClear evidence of communication, teamwork, and problem-solving skills.\nGood evidence of soft skills.\nAdequate evidence, but lacks detail.\nLimited evidence of soft skills.\n\n\nFormatting\nProfessional, clean, and easy to read, with consistent formatting.\nProfessional and easy to read, with minor formatting inconsistencies.\nAdequate formatting, but some sections are hard to read.\nPoor formatting, with multiple inconsistencies.\n\n\nOverall Impression\nStrong candidate with a well-rounded resume that stands out.\nGood candidate with a solid resume.\nAdequate candidate, but resume lacks some key elements.\nWeak candidate, with a resume that needs significant improvement.",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Application 1 Rubric"
    ]
  },
  {
    "objectID": "jobs/job-app-1-rubric.html#resume-rubric",
    "href": "jobs/job-app-1-rubric.html#resume-rubric",
    "title": "Job Application 1 Rubric",
    "section": "",
    "text": "Criteria\nExcellent (4)\nGood (3)\nAverage (2)\nNeeds Improvement (1)\n\n\n\n\nContact Information\nComplete, professional, and easy to find.\nComplete and professional.\nComplete but not easily found.\nIncomplete or unprofessional.\n\n\nSkills\nComprehensive list of relevant technical skills, including proficiency levels.\nGood list of relevant technical skills.\nAdequate list, but missing some key skills.\nLimited list of relevant skills.\n\n\nExperience and/or Projects\nDetailed descriptions of relevant experience, with quantifiable achievements. Detailed descriptions of relevant projects, with clear outcomes and technologies used.\nGood descriptions of relevant experience, with some quantifiable achievements. Adequate descriptions of projects, but lacks detail on outcomes or technologies used.\nAdequate descriptions, but lacks quantifiable achievements.\nLimited descriptions, with few or no quantifiable achievements and few or no relevant projects listed.\n\n\nEducation\nRelevant degrees or certifications, with details on coursework or projects.\nRelevant degrees or certifications.\nAdequate education listed, but lacks detail.\nLimited education listed, with little relevance to the role.\n\n\nSoft Skills\nClear evidence of communication, teamwork, and problem-solving skills.\nGood evidence of soft skills.\nAdequate evidence, but lacks detail.\nLimited evidence of soft skills.\n\n\nFormatting\nProfessional, clean, and easy to read, with consistent formatting.\nProfessional and easy to read, with minor formatting inconsistencies.\nAdequate formatting, but some sections are hard to read.\nPoor formatting, with multiple inconsistencies.\n\n\nOverall Impression\nStrong candidate with a well-rounded resume that stands out.\nGood candidate with a solid resume.\nAdequate candidate, but resume lacks some key elements.\nWeak candidate, with a resume that needs significant improvement.",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Application 1 Rubric"
    ]
  },
  {
    "objectID": "jobs/job-app-1-rubric.html#data-analysis-rubric",
    "href": "jobs/job-app-1-rubric.html#data-analysis-rubric",
    "title": "Job Application 1 Rubric",
    "section": "Data Analysis Rubric",
    "text": "Data Analysis Rubric\nYour analysis will be assessed in each of the following four areas on a scale of 0-5. The questions below will be used as a guide to determine the quality of your analysis in each area.\n\nWriting, Organization, Editing, and Professionalism\n\nIs the report well-organized?\nIs the modeling process described in a clear, logical, and engaging manner?\nAre the motivations for different modeling decisions clearly described?\nIs the analysis free of spelling and grammatical errors?\nAre there walls of data?\nAre there large unbroken walls of code?\nAre there warnings and messages that should be suppressed?\nAre all plots labeled and displayed professionally?\nAre tables displayed neatly?\nIs there too much detail and is the report unnecessarily long?\n\n\n\nData Analysis\n\nAre the mathematical and statistical methods used correctly and appropriately?\nAre appropriate predictor variables used in the analysis?\nTo what extent does the analysis showcase skills described in the job advertisement?\nAre the reasons for excluding and/or transforming the data sensible and well-explained?\nHow was the model chosen and validated?\nHow were the parameters of the model tuned (if applicable)?\n\n\n\nCuriosity\n\nIs the analysis complex and insightful, showing evidence that the data have been analyzed in multiple different ways?\nIs the explanation and presentation creative, going beyond superficial observations and simple graphical summaries?\nAre the findings and conclusions supported by sound reasoning and/or additional research?\nAre the possible implications of the results discussed in context?\n\n\n\nSkepticism & Interpretation\n\nIs the handling of missing data, unusual observations, and outliers described and is there sound rationale for it?\nAre multiple explanations given for a particular finding and multiple approaches used to explore surprising results?\nAre limitations of the data and of the analysis identified and are the potential impacts on the conclusions discussed?\nAre suggestions for future work and additional exploration identified and useful?",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Application 1 Rubric"
    ]
  },
  {
    "objectID": "jobs/job-app-2-resources.html",
    "href": "jobs/job-app-2-resources.html",
    "title": "Hack-A-Thon Report Guidelines",
    "section": "",
    "text": "Your data analysis should do the following\n\nTell a coherent story and narrative.\nInclude something similar to the following sections:\n\nIntroduction outlining and motivating the problem\nExploratory data analysis and pre-process/feature engineering\nModeling or Model Selection\nPerformance Analysis\nConclusion summarizing your results\n\nKeep in mind that it is not just important that you know HOW to apply different machine learning methods, but understand WHEN apply different techniques to data. Imagine that you’re being hired to be a construction worker. If you are asked to hammer in a nail, don’t take out a sledgehammer.\nShow that you are able to communicate well. Communicating well involves ensuring that you incorporate the appropriate level of detail (not too much/not too little) and that you are able to interpret the results of your analyses. It goes beyond just having perfect grammar and spelling.\n\nPro-tip: Write up a draft of your report and then go through it once taking out anything that isn’t absolutely necessary.\n\nBe professional and be grammatically correct and devoid of typos.\n\nThings to avoid:\n\nWalls of code without context. You should be explaining what you do every time you write code. However, don’t put too much detail. Assume the reader is familiar with machine learning. For example, say “We now fit the logistic regression model to the training data.” instead of “A logistic regression model is a machine learning model that blah, blah, blah”.\nThere are times when you want to output something for yourself but it shouldn’t go in a final document. For example, if you use glimpse to make sure the new column you created looks the way you want it to, you may not want to include that in the final report.\nWalls of data without context. Sometimes you may want to print out data for yourself to make sure some analysis step was done correctly. You don’t need to include that in your final report. Make sure that anything you display is done with purpose and that you’re talking about that purpose.\nTalking about functions rather than what code is doing. For example, say “We now fit the logistic regression model to the training data.” instead of “We now use the fit function to fit the logistic regression model to the training data.”\nEnsure all plots have professional axis labels.\nWhen talking about your variables, mostly refer to them by what they represent rather than their name. For example say “We will be predicting whether a customer will default on their loan.” instead of “We will be using default as the response variable. There are situation in which it is appropriate to refer to them by their names in R but be careful.\nIncluding a bunch of unnecessary analysis steps to prove that you can do them. If the project you’ve chosen does not call of more advanced analysis steps, choose a different one.",
    "crumbs": [
      "Job Applications & Interviews",
      "Hack-A-Thon Report Guidelines"
    ]
  },
  {
    "objectID": "jobs/job-app-2-resources.html#sample-analysis-tips",
    "href": "jobs/job-app-2-resources.html#sample-analysis-tips",
    "title": "Hack-A-Thon Report Guidelines",
    "section": "",
    "text": "Your data analysis should do the following\n\nTell a coherent story and narrative.\nInclude something similar to the following sections:\n\nIntroduction outlining and motivating the problem\nExploratory data analysis and pre-process/feature engineering\nModeling or Model Selection\nPerformance Analysis\nConclusion summarizing your results\n\nKeep in mind that it is not just important that you know HOW to apply different machine learning methods, but understand WHEN apply different techniques to data. Imagine that you’re being hired to be a construction worker. If you are asked to hammer in a nail, don’t take out a sledgehammer.\nShow that you are able to communicate well. Communicating well involves ensuring that you incorporate the appropriate level of detail (not too much/not too little) and that you are able to interpret the results of your analyses. It goes beyond just having perfect grammar and spelling.\n\nPro-tip: Write up a draft of your report and then go through it once taking out anything that isn’t absolutely necessary.\n\nBe professional and be grammatically correct and devoid of typos.\n\nThings to avoid:\n\nWalls of code without context. You should be explaining what you do every time you write code. However, don’t put too much detail. Assume the reader is familiar with machine learning. For example, say “We now fit the logistic regression model to the training data.” instead of “A logistic regression model is a machine learning model that blah, blah, blah”.\nThere are times when you want to output something for yourself but it shouldn’t go in a final document. For example, if you use glimpse to make sure the new column you created looks the way you want it to, you may not want to include that in the final report.\nWalls of data without context. Sometimes you may want to print out data for yourself to make sure some analysis step was done correctly. You don’t need to include that in your final report. Make sure that anything you display is done with purpose and that you’re talking about that purpose.\nTalking about functions rather than what code is doing. For example, say “We now fit the logistic regression model to the training data.” instead of “We now use the fit function to fit the logistic regression model to the training data.”\nEnsure all plots have professional axis labels.\nWhen talking about your variables, mostly refer to them by what they represent rather than their name. For example say “We will be predicting whether a customer will default on their loan.” instead of “We will be using default as the response variable. There are situation in which it is appropriate to refer to them by their names in R but be careful.\nIncluding a bunch of unnecessary analysis steps to prove that you can do them. If the project you’ve chosen does not call of more advanced analysis steps, choose a different one.",
    "crumbs": [
      "Job Applications & Interviews",
      "Hack-A-Thon Report Guidelines"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMWF 3:00 - 3:50pm\nCruzen-Murray Library (CML) 208\n\n\n(Optional) Homework Lab\nW 4:00 - 4:50pm (Tentative)\nCruzen-Murray Library (CML) 208\n\n\nOffice Hours\nM 9:30 - 10:30am\nBoone 126B\n\n\nOffice Hours\nT 9:00 - 10:00am\nBoone 126B\n\n\nOffice Hours\nW 1:30 - 2:30pm\nBoone 126B\n\n\nOffice Hours\nTH 10:00 - 11:00am\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Dr. Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMWF 3:00 - 3:50pm\nCruzen-Murray Library (CML) 208\n\n\n(Optional) Homework Lab\nW 4:00 - 4:50pm (Tentative)\nCruzen-Murray Library (CML) 208\n\n\nOffice Hours\nM 9:30 - 10:30am\nBoone 126B\n\n\nOffice Hours\nT 9:00 - 10:00am\nBoone 126B\n\n\nOffice Hours\nW 1:30 - 2:30pm\nBoone 126B\n\n\nOffice Hours\nTH 10:00 - 11:00am\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Dr. Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be able to…\n\ntackle predictive modeling problems arising from real data.\nuse R to fit and evaluate machine learning models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate results results through writing and oral presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nCollege of Idaho Honor Code\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code.\n\n\nDisability Accommodation Statement\nThe College of Idaho seeks to provide an educational environment that is accessible to the needs of students with disabilities. The College provides reasonable services to enrolled students who have a documented permanent or temporary physical, psychological, learning, intellectual, or sensory disability that qualifies the student for accommodations under the Americans with Disabilities Act or section 504 of the Rehabilitation Act of 1973. If you have, or think you may have, a disability that impacts your performance as a student in this class, you are encouraged to arrange support services and/or accommodations through the Department of Accessibility and Learning Excellence located in McCain 201B and available via email at accessibility@collegeofidaho.edu. Reasonable academic accommodations may be provided to students who submit appropriate and current documentation of their disability. Accommodations can be arranged only through this process and are not retroactively applied. More information can be found on the DALE webpage (https://www.collegeofidaho.edu/accessibility).\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, mat427fa25.netlify.app.\nPeriodic announcements will be sent via email and will also be available through Canvas and grades will be stored in the Canvas gradebook. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nIn class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g. “you may turn in your homework late due to a sporting event”), you MUST send me a follow-up message. If you don’t, I will almost certainly forget, and our agreement will be considered null and void.\n\n\nGetting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nI am here to help you be successful in the course. You are encouraged to attend office hours and the homework lab to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments can be emailed to me.\n\n\n\nEmail\nIf you have questions about assignment extensions or accommodations, please email efriedlander@collegeofidaho.edu. Please see Late work policy for more information. If you email me about an error please include a screenshot of the error and the code causing the error. Barring extenuating circumstances, I will respond to MAT 427 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\nThe official textbook for this course is:\n\nAn Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hatie, and Robert Tibshirani\n\nColloquiually referred to as “ISLR”, it is considered one of the bibles of machine learning\nIt’s free!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nYou will be assessed based on five components: homework, job applications, job interviews, a hack-a-thon, and project.\n\nHomework\nIn homework, you will apply what you’ve learned during lecture to complete data analysis tasks. Homework will completed in teams of three, must be typed up using Quarto, and submitted as .qmd and .pdf files in via GitHub.\n\n\nJob Applications & Job Interviews\nDuring this course you will apply to two “jobs”. I will generate the job advertisements including real companies and base the job description on the course content and similar job advertisements that I get from online or professional collaborators. Each job application will have three components:\n\nA cover letter.\nA resume.\nA portfolio.\n\nAll three of these should be tailored to the job description and the company to which you are applying. Your portfolio will consist of self-contained data analyses of your choosing. The most straight forward method of creating th to repurpose your homeworks, converting them from a format in which you are responding to exercises to something where you are telling a narrative and demonstrating that you meet the job criteria. To create your portfolio, you will be required to create a website. More details on this will be given during the semester, however the idea of this project is that you will be able to use the things you general when you are applying for jobs.\nAfter you submit your job applications, you will be invited to schedule a one-hour long job interview. It is your job to schedule your job interview with Dr. Friedlander. Each job interview will have three portions. The first, lasting 10-15 minutes, will include typical questions that apply to almost any job interview (e.g. “What are your biggest strengths and weaknesses”). The second, lasting 20-30 minutes, will include questions about the portfolio you submitted and your understanding of the required skills described in the job advertising. The third section will mimic what is called a “case interview”. Case interviews are extremely common for many jobs, especially those requiring quantitative or computational skills, and can be intimidating. During the case interview portion, you will be presented with a “case study” and asked questions on how you would go about approaching it. The cases themselves will be designed so that they can be solved using the content from class. The goal of this whole exercise is to assess your knowledge of the course content in a way that is authentic while also preparing you to get a job.\n\n\nHack-a-thon\nAt some point in the semester we will participate in a “Hack-a-thon” as a class. Namely, you will be given a short period of time (1-3 days) to build a model and make a set of predictions. After the competition is over, you will be required to present on your model. Part of your score will be determine by how well your model performs and extra credit will be given to the top scoring individuals.\n\n\nProject\nDuring the latter portion of the course, you will complete a final project that involves a deep exploration of a problem. More details for the final project will be provided later in the course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n10%\n\n\nJob Application 1\n15%\n\n\nJob Interview 1\n15%\n\n\nJob Interview 2\n20%\n\n\nHack-a-thon + Report\n25%\n\n\nProject\n15%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don’t cheat!\n\nThe job application assignments must be completed individually but you are welcome to discuss the assignment with classmates (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share (i.e. via copy/paste or copying) any code or prose with anyone other than myself.\nFor the hack-a-thon, everyone will submit their predictions and give their own presentations. However, you are encouraged to work together. You are allowed to share code with one another. However, everyone should be able to explain what they did and everyone’s projects should be unique in some way. Point reductions will be given if two individuals submit the exact same predictions.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity. In general if the following two things are not true, you are cheating:\n\nYou understand and can explain all of the code you have written down or you don’t and you have cited the source of that code.\nAll of your prose and narrative were written by yourself.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, just ask.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all College of Idaho policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the Honor Code.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. I understand that things come up periodically that could make it difficult to submit an assignment by the deadline.\n\nLate Homework: Homework is completion based and well be accepted without penalty for a week. However, if your homework is turned in after I begin grading it, you will not receive any feedback.\nSchool-Sponsored Events/Illness: If an assignment or meeting must be missed due to a school-sponsored event, you must let me know at least a week ahead of time so that we can schedule a time for you to make up the work before you leave. If an assignment or meeting must be missed due to illness, you must let me know as soon as it is safe for you to do so and before the assignment or meeting if possible. Failure to adhere to this policy will result in a 35% penalty on the corresponding assignment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html",
    "href": "hw/08-hw-ClassificationTrees.html",
    "title": "Homework 8: Classification Trees",
    "section": "",
    "text": "In this homework we will make our classifications using classification trees which are a type of decision tree. A decision tree works by using splitting rules to divide up the predictor space. For numerical predictors, these rules take the form of linear separators. Linear separators are just linear inequalities of the predictors. For example, consider the problem of trying to predict whether a student will pass their first college mathematics course. The scatterplot below shows the mathematics ACT score and GPA for 57 students. Each student dot is colored by a black dot (0) if they failed their first college mathematics course and a red dot (1) if they passed.\n\nThe node at the top of the tree represents the linear separator \\(ACT &lt; 18.65\\). If a student in the training data has \\(ACT\\) score less than 18.65, then they are sent to the left side of the tree. If a student has ACT score greater than 18.65, they are sent to the right side of the tree. Students on the right side of the tree are then further divided by the linear separator \\(GPA &lt; 2.86111\\). Thus, every student is placed at one of the three leaves of the tree. The leaf labels represent the majority class at each leaf. For example, more than half of the students with \\(ACT &lt; 18.65\\) failed the class, so the leaf at the far left of the tree is labeled by 0. To classify a new student, we apply the linear separators starting at the top of the tree to determine which leaf they belong to. The new student is then classified according to the label of that leaf.\n\n\nIn this assignment, you will…\n\nFit and interpret classification trees\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#learning-goals",
    "href": "hw/08-hw-ClassificationTrees.html#learning-goals",
    "title": "Homework 8: Classification Trees",
    "section": "",
    "text": "In this assignment, you will…\n\nFit and interpret classification trees\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#teams-rules",
    "href": "hw/08-hw-ClassificationTrees.html#teams-rules",
    "title": "Homework 8: Classification Trees",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW8. Your group will consist of 2-3 people and has been randomly generated. The GitHub assignment can be found here. Rules:\n\nYou are all responsible for understanding the work that your team turns in.\nAll team members must make roughly equal contributions to the homework.\nAny work completed by a team member must be committed and pushed to GitHub by that person."
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-1",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-1",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nImport the datasets which contains the iris data set and split the data using a 60-40 split. Be sure that each species is represented proportionally in the test and train sets. Use a seed of 427."
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-2",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-2",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a plot showing the response variable Species. Comment on the relative frequency of each category and what impact the balance will have on modeling."
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-3",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-3",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a scatterplot of Petal.Length versus Sepal.Width colored by Species using the training data`. Notice that these are different features than we used in the KNN hw."
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-4",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-4",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nBased on your scatter plot, determine some simple linear separators that can be used to classify the observations in test. I.e. build a small decision tree by hand. (Remember, you can use the logical operators &lt;, &gt;, & (and), | (or) to subset vectors in R.) For example, if we were given data for new students to classify based on the scatterplot in the introduction, we might predict whether they will pass or fail using simple linear separators as in the code below (though this won’t run, because we don’t have access to the data)\n\nnew_students &lt;- new_students |&gt; \n  mutate(prediction = if_else(ACT &lt; 20 & GPA &lt; 3.25, \"Fail\", \"Pass\"))"
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-5",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-5",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nTo get a sense of how good your simple model predictions are, create a confusion matrix.\n\n\nHopefully, your simple linear separators were fairly successful at making predictions. Again, this is because the iris data set just isn’t that challenging. For more challenging data sets, we will want R to search for the best linear separators and to build us a classification tree.\n:::"
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-6",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-6",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nUsing the training data, train and display a classification tree that predicts Species as a function of the Sepal.Width and Petal.Length."
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-7",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-7",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nUse this classification tree model to classify the species in the test set. Print the confusion matrix and the accuracy.\n\n\nThe classification tree in the above example is kind of boring. To get a better sense of what the decision boundaries from classification trees look like, we’ll try a more interesting example.\nConsider the simulated data set below. We randomly generate points in the grid \\([0,1] \\times [0,1]\\) and split these points into two classes (“1” and “2”) based on whether they are above or below the line \\(y = x\\). These points are stored in the data frame DFsim.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(1)\nss &lt;- 300\nx1 &lt;- runif(ss, min = 0, max = 1)\nx2 &lt;- runif(ss, min = 0, max = 1)\nclass &lt;- rep(1 , ss)\nclass[x1 &gt; x2] &lt;- 2\nDFsim &lt;- tibble(x1 = x1, x2 = x2, CL = as.factor(class) )\nDFsim |&gt; \nggplot(aes(x = x1, y = x2, col = CL)) +\n  geom_point()\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "hw/08-hw-ClassificationTrees.html#exercise-8",
    "href": "hw/08-hw-ClassificationTrees.html#exercise-8",
    "title": "Homework 8: Classification Trees",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nBuild a classification tree on DFsim that predicts class as a function of x1 and x2. Now, apply your model to predict the class of each point in grid, defined below. Plot the points in grid colored by the predicted class.\n\n\n\n# grid points from [0,1] x [0,1]\ng1 &lt;- rep((1:100)*(1/100), 100)\ng2 &lt;- rep((1:100)*(1/100), each = 100)\ngrid &lt;- data.frame(x1 = g1\n                   , x2 = g2)"
  },
  {
    "objectID": "hw/02-hw-mlr.html",
    "href": "hw/02-hw-mlr.html",
    "title": "Homework 2: Intro to Regression",
    "section": "",
    "text": "In this homework, you will practice multiple linear regression by working with the data set Carseats from the package ISLR2. In addition, you will practice collaborating with a team over GitHub.\n\n\nBy the end of the homework, you will…\n\nBe able to collaborate with teammates on the same document using GitHub\nGain practice writing a reproducible report using Quarto\nFit and interpret linear models\nSplit data using tidymodels\nCompare and evaluate different linear models"
  },
  {
    "objectID": "hw/02-hw-mlr.html#learning-goals",
    "href": "hw/02-hw-mlr.html#learning-goals",
    "title": "Homework 2: Intro to Regression",
    "section": "",
    "text": "By the end of the homework, you will…\n\nBe able to collaborate with teammates on the same document using GitHub\nGain practice writing a reproducible report using Quarto\nFit and interpret linear models\nSplit data using tidymodels\nCompare and evaluate different linear models"
  },
  {
    "objectID": "hw/02-hw-mlr.html#teams",
    "href": "hw/02-hw-mlr.html#teams",
    "title": "Homework 2: Intro to Regression",
    "section": "Teams",
    "text": "Teams\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW2. Your group will consist of 2-3 people and has been randomly generated. But first, some rules:\n\nYou and your team members should be in the same physical room when you complete this assignment.\nYou should all contribute to each problem, but to receive credit you must rotate who actually writes down the answers.\nIn order to receive credit, you must have a commit after each exercise by the correct member of your team.\nFor now, don’t try to edit the same document at the same time. We will cover that in later homeworks."
  },
  {
    "objectID": "hw/02-hw-mlr.html#clone-the-repo-start-new-rstudio-project",
    "href": "hw/02-hw-mlr.html#clone-the-repo-start-new-rstudio-project",
    "title": "Homework 2: Intro to Regression",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\nThe following directions will guide you through the process of setting up your homework to work as a group."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-0.1",
    "href": "hw/02-hw-mlr.html#exercise-0.1",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 0.1",
    "text": "Exercise 0.1\n\n\n\n\n\n\nQuestion\n\n\n\nIn your group, decide on a team name. Then have one member of your group:\n\nClick this link to accept the assignment and enter your team name.\nRepeat the directions for creating a project from HW 1 with the HW 2 repository.\n\nOnce this is complete, the other two members can do the same thing, being careful to join the already created team on GitHub classroom."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-0.2",
    "href": "hw/02-hw-mlr.html#exercise-0.2",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 0.2",
    "text": "Exercise 0.2\n\n\n\n\n\n\nQuestion\n\n\n\nHave the first member of your group fill in the blanks below and then render, commit, and push the changes back to your GitHub repository.\n\n\n\nTeam Name: [Make one up and enter here]\nMember 1: [Insert Name]\nMember 2: [Insert Name]\nMember 3: [Insert Name/Delete line if you only have two members]"
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-1",
    "href": "hw/02-hw-mlr.html#exercise-1",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nAll members of your group should install the ISLR2 package if they haven’t. Remember that you only need to do this once on your computer.\nHave the second member of your group pull the changes made by member 1. Before completing this question.\nLoad the ISLR2 package. You should be able to access the dataset Carseats. Based on your knowledge of the world, which features do you think will be most predictive of Sales. Hint: ?Carseats will give you more information on the data set and the variables.\nOnce you have completed this, have the second member render, commit, and push the changes to GitHub."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-2",
    "href": "hw/02-hw-mlr.html#exercise-2",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nHave the second member of your group pull the changes made by member 1 and member 2. Before completing this question.\nUse tidymodels to create a training and test set from the Carseats data using a 70-30 split. Note that this is a random process so you will get different partitions every time you split your data. As a result, it is considered good practice to set your seed so that the results a reproducible. For this homework please use the seed 427. For the training set, what quantitative variable is most highly correlated with Sales, our target variable?\nOnce you have completed this, have the third member render, commit, and push the changes to GitHub."
  },
  {
    "objectID": "hw/02-hw-mlr.html#committing-changes",
    "href": "hw/02-hw-mlr.html#committing-changes",
    "title": "Homework 2: Intro to Regression",
    "section": "Committing Changes",
    "text": "Committing Changes\n\nYou should continue in this manner, rotating who completes each question in order from member 1 to member 2 to member 3 and back to member 1 for the remainder of the assignment. In order to receive credit, you must have a commit after each exercise by the correct member of your team."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-3",
    "href": "hw/02-hw-mlr.html#exercise-3",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nFit a linear regression model predicting Sales using the variable you identified in Exercise 2. Write down the resulting model in the form: \\[Price = \\beta_0 + \\beta_1\\times Variable\\] Don’t forget to use your training set rather than the full data to train your model."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-4-hard",
    "href": "hw/02-hw-mlr.html#exercise-4-hard",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 4 (Hard)",
    "text": "Exercise 4 (Hard)\n\n\n\n\n\n\nQuestion\n\n\n\nRe-estimate the parameters (i.e. the intercept and slope) for the model above using gradient descent. Your estimates should be similar to those in Exercise 3 but likely won’t be EXACTLY the same. Your solution should include the following:\n\nInitialize the values of your \\(\\beta\\)’s.\nInitialize the step size.\nInitialize your tolerance (i.e. the stopping criteria).\nEnter a loop. For each iteration in the loop:\n\nCompute the partial derivatives for \\(\\beta_0\\) and \\(\\beta_1\\).\nUpdate the values of \\(\\beta_0\\) and \\(\\beta_1\\).\nCheck to see if the change in your estimates is smaller than your tolerance. If it is, exit the loop, otherwise continue."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-5",
    "href": "hw/02-hw-mlr.html#exercise-5",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the RMSE for your baseline model on both the training and test set and report them. Try and report your results inline.\n\n\nThe second primary metric that we can use to assess the accuracy of regression models is called the coefficient of determination, denoted \\(R^2\\). \\(R^2\\) is the proportion of variance (information) in our target variable that is explained by our model and can be computed by squaring \\(R\\), the correlation coefficient between the target variable \\(y\\) and the predicted target \\(\\hat{y}\\). The lm function actually computes the \\(R^2\\) of our training data for us which we can access using the glance function from the broom package which is included in tidymodels so you don’t need to load it."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-6",
    "href": "hw/02-hw-mlr.html#exercise-6",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nWhat proportion of the variation in Sales is explained by our baseline model for the training and validation sets?"
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-7",
    "href": "hw/02-hw-mlr.html#exercise-7",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nUsing tidymodels, build a two-input linear model for Sales by adding US in addition to the variable you selected above. Save your model as lmfit1. Use tidy and kable to output the model. Is the coefficient for Price the same or different than it was in our baseline model?\n\n\nNotice that the only coefficient added for US is called USYes. When you build a linear model with a categorical variable, R will introduce dummy variables which encode each category as a vector of 0’s and 1’s. In data science, this is sometimes called one-hot encoding. One level is always lumped into the intercept coefficient and is called the reference level. In this case, the reference level is No. When including a categorical variable in a linear model, you can interpret the resulting line being shifted up or down based on the category of a given observation.\nLet’s now assess the accuracy of our new model. To make computation of RMSE and \\(R^2\\) easier let’s take advantage of the rmse and r2 functions in the yardstick package (also included in tidymodels)."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-8",
    "href": "hw/02-hw-mlr.html#exercise-8",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nUse the rmse and rsq functions from theyardstick package to compute the RMSE and \\(R^2\\) values for this new model on both the training and validation sets. How do these compare to the baseline model?"
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-9",
    "href": "hw/02-hw-mlr.html#exercise-9",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nFit a model using all of the predictors in your training data. Call the model lmfull. Assess the model’s accuracy on the training data and the test data, comparing it to the previous models we’ve fit, and comment on your results.\n\n\nYou should notice that while the accuracy metrics on the training data drastically improve, there is little to no difference in the metrics for the test set. This is because of a phenomenon known as overfitting. Overfitting occurs when your model starts matching the training TOO well. A good visualization of an overfit model is Figure 2 in the Wikipedia article for overfitting. As you include more variables/information in your model, your performance will ALWAYS increase on your training data. This is one of the reasons we always use holdout sets. Eventually your model will begin to over-align to the noise in your training data and the accuracy on holdout sets will be level off and in most cases begin to degrade.\nWhen modeling there are two related trade-offs that you need to consider. The first is the trade-off between prediction accuracy and interpretability. In general, one can typically create models with better prediction accuracy by sacrificing interpretability (e.g. by including more variables in your model, transforming these variables, etc.). Another trade-off is something called the bias-variance trade-off. As we increase the complexity of a model, we allow it to account for more and more intricate patterns in our data. In theory, this allows the model to mimic more complex relationships between our predictors and our target variables, reducing bias. On the other hand, more complex models typically have more parameters which need to be estimated which require more data to estimate accurately. When you increase the complexity of a model you usually increase the variance of the estimates of model parameters and the predictions the model makes. In other words, the model will be much more sensitive to the noise in the data that you have. Bias and variance will both decrease the accuracy of your model so you should try to minimize both. However, past a certain point, it will be a trade-off between the two."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-10",
    "href": "hw/02-hw-mlr.html#exercise-10",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nFind and fit a model on the training data which outperforms all the models we fit so far when evaluated on the test set. Briefly summarize your results. The “best” model will receive a high-five from Dr. Friedlander. Feel free to use any techniques we’ve learned in this class, up to this point. I encourage you to try out different data transformations like polynomials."
  },
  {
    "objectID": "hw/01-hw-eda.html",
    "href": "hw/01-hw-eda.html",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "",
    "text": "Adapted from “Start teaching with R,” created by R Pruim, N J Horton, and D Kaplan, 2013, “Interactive and Dynamic Graphics for Data Analysis,” by Dianne Cook and Deborah F. Swayne, Colby Long’s DATA 325 Course at Wooster College and Maria Tackett’s STA-210 course at Duke University."
  },
  {
    "objectID": "hw/01-hw-eda.html#introduction",
    "href": "hw/01-hw-eda.html#introduction",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this homework we will familiarize ourselves with the tools that we’ll use throughout the course and refresh ourselves on topic related to exploratory data analysis."
  },
  {
    "objectID": "hw/01-hw-eda.html#course-toolkit",
    "href": "hw/01-hw-eda.html#course-toolkit",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Course Toolkit",
    "text": "Course Toolkit\nThe primary tools we’ll be using in this course are R, RStudio, git, and GitHub. We will be using them throughout the course both to learn the concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like Dropbox but much better for code).\n\n\nTo make versioning simpler, this homework will be completed individually. In the future, you’ll learn about collaborating on GitHub and producing a single homework for your team, but for now, concentrate on getting the basics down."
  },
  {
    "objectID": "hw/01-hw-eda.html#exploratory-data-analysis",
    "href": "hw/01-hw-eda.html#exploratory-data-analysis",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nOne of the most important components of data science is exploratory data analysis. I really like the following definition, which comes from this article (though it’s probably not the original source).\n\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, spot anomalies, to test hypotheses and to check assumptions with the help of summary statistics and graphical representations.\n\nBefore you begin your exploratory analysis, you may already have a particular question in mind. For example, you might work for an online retailer and want to develop a model to predict which purchased items will be returned. Or, you may not have a particular question in mind. Instead, you might just be asked to look at browsing data for several customers and figure out some way to increase purchases. In either case, before you construct a fancy model, you need to explore and understand your data. This is how you gain new insights and determine if an idea is worth pursuing."
  },
  {
    "objectID": "hw/01-hw-eda.html#learning-goals",
    "href": "hw/01-hw-eda.html#learning-goals",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the homework, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to create numerical and visual summaries of data\nUse those summaries"
  },
  {
    "objectID": "hw/01-hw-eda.html#clone-the-repo-start-new-rstudio-project",
    "href": "hw/01-hw-eda.html#clone-the-repo-start-new-rstudio-project",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/mat427sp25 organization on GitHub. Click on the repo with the prefix hw-01-. It contains the starter documents you need to complete the lab.\n\nIf you do not see your hw-01 repo, click here to create your repo.\n\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick 01-hw-eda.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/01-hw-eda.html#r-and-r-studio",
    "href": "hw/01-hw-eda.html#r-and-r-studio",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It is rumored that it stood for “Yet Another Markup Language” but it officially stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments I will nudge you when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "hw/01-hw-eda.html#understanding-your-data",
    "href": "hw/01-hw-eda.html#understanding-your-data",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Understanding your data",
    "text": "Understanding your data\nToday we will be working with the TIPS data set which is in the regclass package. The data in the TIPS dataset is information recorded by one waiter about each tip he received over a period of a few months working in a restaurant. We would like to use this data to address the question, “What factors affect tipping behavior?”\n\nExercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nInstall the regclass package by either typing install.packages(\"regclass\") in the console or by clicking “Tools &gt; Install Packages” and selecting the package. Once you have done this, the code chunk below will load the package and data set. Notice that a bunch of unnecessary output is included when you knit the document. Change the Quarto chunk options so that this is not displayed.\n\n\n\nlibrary(regclass)\n\nLoading required package: bestglm\n\n\nLoading required package: leaps\n\n\nLoading required package: VGAM\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nLoading required package: rpart\n\n\nLoading required package: randomForest\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nImportant regclass change from 1.3:\nAll functions that had a . in the name now have an _\nall.correlations -&gt; all_correlations, cor.demo -&gt; cor_demo, etc.\n\ndata(\"TIPS\")\n\nWhen exploring a new data set, it’s important to first understand the basics. What format is our data in? What types of information are included in the data set? How many observations are there?\n\n\nExercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nIn R, data sets are usually stored in a 2-dimensional structure called a data frame. The tidyverse provides a lot of useful functions for a variety of applications including data exploration and the particular flavor of data frame that the tidyverse uses is called a tibble. After loading the tidyverse library, you can get an idea of the structure of a data set using the syntax str(dataset) or glimpse(data), and you can peak at the first few rows and columns with head(dataset). Create a code chunk below, and use these functions (and others) in the R chunk below to better understand the data. How many tips are recorded in this data set? Which days of the week did the waiter work?\n\n\nOften, a data set will come with a code book which gives more complete information about the structure of the data, the meaning of variables, and how the data were collected. In this case, most of the column names are pretty self explanatory.\n\n\n\nVariable\nDescription\n\n\n\n\nTipPercentage\nthe gratuity, as a percentage of the bill\n\n\nBill\nthe cost of the meal in US dollars\n\n\nTip\nthe tip in US dollars\n\n\nGender\ngender of the bill payer\n\n\nSmoker\nwhether the party included smokers\n\n\nWeekday\nday of the week\n\n\nTime\ntime the bill was paid\n\n\nPartySize\nsize of the party\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nEven though the column names are self-explanatory, we might have more questions about the data. For example, we might conjecture that people tip differently for breakfast and lunch, but our data only tells us if the bill was paid at “Day” or “Night.” State another reasonable conjecture about a factor that might affect tipping behavior. What additional information would be helpful to explore that conjecture?\n\n\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g. “Completed exercises 1 - 3”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "hw/01-hw-eda.html#graphical-summaries",
    "href": "hw/01-hw-eda.html#graphical-summaries",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Graphical Summaries",
    "text": "Graphical Summaries\nGraphical summaries are a key tool in exploratory data analysis to to help you understand your data. They also help you communicate insights about your data to others. For example, we might want to display relationships about some of our categorical variables. So we could start by graphing different party sizes in our data set.\n\nTIPS |&gt; \n  ggplot(aes(x = PartySize)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nOr we could explore the question about the percentage of tables that are smokers on different days of the week visually.\n\nTIPS |&gt; \n  ggplot(aes(x = Weekday, fill = Smoker)) +\n    geom_bar()\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x = Weekday, fill = Smoker)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nWe might summarize a numerical variable with a histogram. For example, here is a histogram of all of the tips in the data set.\n\nTIPS |&gt; \n  ggplot(aes(x = Tip)) +\n    geom_histogram(bins = 100)\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nNotice that there are a few “spikes” in the histogram above. What do you think is causing this?\n\n\nWe can also summarize this numerical data broken down by one of the categorical variables using boxplots, violin plots, or sina plots. Note that to create sina plots we need the ggforce package.\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_boxplot() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_violin() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nlibrary(ggforce)\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_sina() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\n\nOr we can visualize the relationship between a lot of our numerical variables at once.\n\n# Using pairs (only numerical allowed)\npairs(~ Bill + TipPercentage + Tip\n    , data = TIPS\n    , main=\"Scatterplot Matrix for TIPS\")\n\n\n\n\n\n\n\n# Using ggpairs from GGally package (preferable even though more syntax)\nlibrary(GGally)\nTIPS |&gt; \n  select(Bill, TipPercentage, Tip, Weekday) |&gt; \n  ggpairs()\n\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any clear linear relationships in the scatterplots above? What do you think is the explanation for these relationships?\n\n\nThere are lots of other interesting graphical summaries available for interpreting and displaying data. In addition, there are lots of R packages that allow you to draw these graphics and to further customize some of the ones we discussed here. In your projects, you are welcome to use any of these that you think are appropriate.\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g. “Completed exercises 7 - 8”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\n\nExercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nState a reasonable conjecture about tipping behavior that you would like to explore in the data set. For example, you might think that people on dates tip more or that the waiter gets smaller tips when he has too many tables. Give at least one numerical and one graphical summary to explore this conjecture. Is there any evidence to support your conjecture?\nIt’s okay if your conjecture is not supported or if you are just wrong–that’s often the case in exploratory data analysis.\n\n\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g. “Completed exercises 9”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBefore you’re done with you work, make sure you look it over one last time to make sure the rendered document looks like you want it to! I can’t tell you how often students turn in work and their output doesn’t match their prose or the output definitely doesn’t look the way they wanted it to."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html",
    "href": "hw/07-hw-regression-trees.html",
    "title": "Homework 7: Regression Trees",
    "section": "",
    "text": "In this homework you will practice fitting regression trees and and using model tuning to select models.\n\n\nIn this assignment, you will…\n\nFit and interpret regression trees\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#learning-goals",
    "href": "hw/07-hw-regression-trees.html#learning-goals",
    "title": "Homework 7: Regression Trees",
    "section": "",
    "text": "In this assignment, you will…\n\nFit and interpret regression trees\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#teams-rules",
    "href": "hw/07-hw-regression-trees.html#teams-rules",
    "title": "Homework 7: Regression Trees",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW7. Your group will consist of 2-3 people and has been randomly generated. The GitHub assignment can be found here. Rules:\n\nYou are all responsible for understanding the work that your team turns in.\nAll team members must make roughly equal contributions to the homework.\nAny work completed by a team member must be committed and pushed to GitHub by that person."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#cleaning-and-preprocessing",
    "href": "hw/07-hw-regression-trees.html#cleaning-and-preprocessing",
    "title": "Homework 7: Regression Trees",
    "section": "Cleaning and Preprocessing",
    "text": "Cleaning and Preprocessing"
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-1",
    "href": "hw/07-hw-regression-trees.html#exercise-1",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nLoad the data set CommViolPredUnnormalizedDataCleaned.csv and clean it. Hint: review Exercises 1-4 of your previous homework."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-2",
    "href": "hw/07-hw-regression-trees.html#exercise-2",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nSplit the data into a training and test set using an 80-20 split. Use the seed 427."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-3",
    "href": "hw/07-hw-regression-trees.html#exercise-3",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate a recipe that can be used with a regression tree."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-4",
    "href": "hw/07-hw-regression-trees.html#exercise-4",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nWhat was the best model from your last homework? Write out the model below in a neat format. What was it’s RMSE and \\(R^2\\)?\n\n\nIdeally we’d like any model we create to make better predictions that this baseline model. Hopefully, we can do better than this model using a regression tree."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-5",
    "href": "hw/07-hw-regression-trees.html#exercise-5",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nBefore you fit any trees, do you think a decision tree will have better performance than the baseline model? Describe the differences between linear models and decision trees including the advantages and disadvantages of each."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-6",
    "href": "hw/07-hw-regression-trees.html#exercise-6",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nFind a good Regression Tree to model the data. Use a grid search and cross-validation to find a good complexity parameter."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-7",
    "href": "hw/07-hw-regression-trees.html#exercise-7",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nFit your tree on the entire training set and use the rpart functions from class to display it."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-8",
    "href": "hw/07-hw-regression-trees.html#exercise-8",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nBased on your tree, which variable do you think is most important for determining the number of Violent Crimes Per Capita?."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-9",
    "href": "hw/07-hw-regression-trees.html#exercise-9",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nHow many different predictions are possible from your regression tree model? Why? How does this compare with the baseline model you selected in Exercise 4."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-9-1",
    "href": "hw/07-hw-regression-trees.html#exercise-9-1",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the root mean squared error for your regression tree model applied to the test set."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-10",
    "href": "hw/07-hw-regression-trees.html#exercise-10",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nHow does the test error compare to the baseline model? Which model has better performance? Compare the interpretability of each model."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-11",
    "href": "hw/07-hw-regression-trees.html#exercise-11",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 11",
    "text": "Exercise 11\n\n\n\n\n\n\nQuestion\n\n\n\nLook at the documentation for decision_tree by typing ?decision_tree in your console. Notice that there are two other tuning parameters, tree_depth (defaults to 30) and min_n (defaults to 2). Use an irregular grid with at least 100 points and cross-validation to find an optimal combination of your three tuning parameters. Why do you think we want to use an irregular grid, rather than a regular grid here?"
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-12",
    "href": "hw/07-hw-regression-trees.html#exercise-12",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 12",
    "text": "Exercise 12\n\n\n\n\n\n\nQuestion\n\n\n\nFit the resulting model to the full training data and estimate it’s performance on the test set."
  },
  {
    "objectID": "hw/07-hw-regression-trees.html#exercise-13-practice-interview-question",
    "href": "hw/07-hw-regression-trees.html#exercise-13-practice-interview-question",
    "title": "Homework 7: Regression Trees",
    "section": "Exercise 13 (Practice interview question)",
    "text": "Exercise 13 (Practice interview question)\n\n\n\n\n\n\nQuestion\n\n\n\nDescribe how tree_depth and min_n should impact the flexibility and bias-variance trade off of the resulting tree."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html",
    "href": "hw/05-hw-preproc-cv.html",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "",
    "text": "In this homework you will practice pre-processing data and using cross-validation to evaluate regression models.\n\n\nIn this assignment, you will…\n\nUse exploratory data analysis to inform feature engineering steps\nPre-process data and impute missing values\nEvaluate and compare models using cross-validation"
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#learning-goals",
    "href": "hw/05-hw-preproc-cv.html#learning-goals",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "",
    "text": "In this assignment, you will…\n\nUse exploratory data analysis to inform feature engineering steps\nPre-process data and impute missing values\nEvaluate and compare models using cross-validation"
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#teams-rules",
    "href": "hw/05-hw-preproc-cv.html#teams-rules",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW5. Your group will consist of 2-3 people and has been randomly generated. You have now been exposed to all of the Git concepts that we will talk about in this class. It is up to you to apply them to complete your homework in any way you see fit. Some rules:\n\nYou are all responsible for understanding the work that you turn in.\nAll team members must make roughly equal contributions to the homework.\nAny work completed by a team member must be committed and pushed to GitHub by that person."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-0",
    "href": "hw/05-hw-preproc-cv.html#exercise-0",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 0",
    "text": "Exercise 0\nAs in your previous homework’s, create your team on GitHub classroom and clone the repository. Here is a link to the homework."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#data-lego",
    "href": "hw/05-hw-preproc-cv.html#data-lego",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Data: LEGO",
    "text": "Data: LEGO\nThe data for this analysis includes information about LEGO sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide and were obtained for this assignment from Peterson and Zieglar (2021).\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are:\n\nItem_Number: a serial code corresponding to the set.\nSet_Name: The name of the LEGO set.\nTheme: Theme of the LEGO set.\nPieces: Number of pieces in the set from brickset.com.\nAmazon_Price: Amazon price of the set scraped from brickset.com (in U.S. dollars).\nYear : Year the LEGO set was produced.\nAges: Variable stating what aged children the set is appropriate for.\nPages: Number of pages in the instruction booklet.\nMinifigures: Number of minifigures (LEGO people) in the set scraped from brickset.com. LEGO sets with no minifigures have been coded as NA. NA’s also represent missing data. This is due to how brickset.com reports their data.\nPackaging: What type of packaging the set came in.\nWeight: The weight of the set.\nUnique_Pieces: The number of unique pieces in each set.\nAvailability: Where the set can be purchased.\nSize: General size of the interlocking bricks (Large = LEGO Duplo sets - which include large brick pieces safe for children ages 1 to 5, Small = LEGO sets which- include the traditional smaller brick pieces created for age groups 5 and - older, e.g., City, Friends).\n\nYour ultimate goal will be to predict Amazon_Price from the other features."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-1",
    "href": "hw/05-hw-preproc-cv.html#exercise-1",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nThe data are contained in lego-sample.csv. Load the data."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-2",
    "href": "hw/05-hw-preproc-cv.html#exercise-2",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nTwo of the variables in the data set shouldn’t be useful because they just serve to identify the different LEGO sets. Which two are they? Remove them."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-3",
    "href": "hw/05-hw-preproc-cv.html#exercise-3",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nNotice that the Weight variable is a bit odd… It seems like it should be numeric but it’s a chr. Why? Write code to extract the true numerical weight in either lbs or Kgs (your choice). You are encouraged to use the internet and generative AI to help you figure out how to do this. However, make sure you are able to explain your code once you are done."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-4",
    "href": "hw/05-hw-preproc-cv.html#exercise-4",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nFor each of the 12 features do the following:\n\n\n\nExercise 4.1\n\n\n\n\n\n\nQuestion\n\n\n\nIdentify if they are the correct data type. Are categorical variables coded as factors? Are the factor levels in the correct order if necessary? Are numerical variables coded as numbers? You will need to read descriptions of the data to make this determination.\n\n\n\n\nExercise 4.2\n\n\n\n\n\n\nQuestion\n\n\n\nIdentify any variables with missing values. Identify and then fix any variables for whom missing values (i.e. NAs) indicate something other than that the data is missing (there is at least one). Fill in this missing values appropriately.\n\n\n\n\nExercise 4.3\n\n\n\n\n\n\nQuestion\n\n\n\nFor all of the categorical variables, identify ones that you think may be problematic because they may have near-zero variance. Decide whether to remove them now, or remove them as part of your pre-processor. Make an argument for why your choice is appropriate.\n\n\n\n\nExercise 4.4\n\n\n\n\n\n\nQuestion\n\n\n\nFor all of the categorical variables, identify ones that you think may be problematic because they have many categories that don’t have a lot of observations and likely need to be “lumped”. Decide whether to remove them now, or remove them as part of your pre-processor. Make an argument for why your choice is appropriate."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-5",
    "href": "hw/05-hw-preproc-cv.html#exercise-5",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nSplit your data into training and test sets. Use your own judgement to determine training to test split ratio. Make sure to set a seed."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-6",
    "href": "hw/05-hw-preproc-cv.html#exercise-6",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate at least three different recipes designed to be used with linear regression that treat preprocessing differently. Hint: you’ll likely want to try out different missing value imputation or lumping strategies. It’s also a good idea to include step_lincolm."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-7",
    "href": "hw/05-hw-preproc-cv.html#exercise-7",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate at least three different recipes designed to be used with \\(K\\)-nearest neighbors that treat preprocessing differently. Hint: you’ll likely want to try out different missing value imputation or lumping strategies."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-7-1",
    "href": "hw/05-hw-preproc-cv.html#exercise-7-1",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a workflow_set that contains 12 different workflows:\n\nthree linear regression workflows: one linear regression model with each of the three recipes you created above\nnine different KNN workflows: choose three different \\(K\\)s for you KNN models and create one workflow for each combination of KNN model and preprocessing recipe"
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-8",
    "href": "hw/05-hw-preproc-cv.html#exercise-8",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nUse 5 fold CV with 5 repeats to compute the RMSE and R-squared for each of the 12 workflows you created above. Note that this step may take a few minutes to execute."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-9",
    "href": "hw/05-hw-preproc-cv.html#exercise-9",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the results of your cross validation and select your best workflow."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-10",
    "href": "hw/05-hw-preproc-cv.html#exercise-10",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nRe-fit your best model on the whole training set and estimate your error metrics on the test set."
  },
  {
    "objectID": "hw/05-hw-preproc-cv.html#exercise-11-sample-interview-question",
    "href": "hw/05-hw-preproc-cv.html#exercise-11-sample-interview-question",
    "title": "Homework 05: Preprocessing and Cross Validation",
    "section": "Exercise 11 (Sample interview question)",
    "text": "Exercise 11 (Sample interview question)\n\n\n\n\n\n\nQuestion\n\n\n\nThe time to complete cross-validation can be substantially improved by using parallel processing. Below is the output for the copilot prompt “Generate pseudo-code in R to do cross-validation with repetition and multiple models”. Which parts of this code can be run in parallel and which can’t. Note any changes that you might need to make for this to be parallelizable.\n\n# Define the number of folds (k) and the number of repetitions (r)\nk &lt;- 5\nr &lt;- 3\n\n# Define the list of models to evaluate\nmodels &lt;- list(\n    model1 = train_model1,\n    model2 = train_model2,\n    model3 = train_model3\n)\n\n# Initialize a list to store the performance metrics for each model\nall_performance_metrics &lt;- list()\n\n# Loop through each model\nfor (model_name in names(models)) {\n    # Initialize a list to store the performance metrics for this model\n    model_performance_metrics &lt;- list()\n    \n    # Loop through each repetition\n    for (rep in 1:r) {\n        # Create k-fold cross-validation indices for this repetition\n        folds &lt;- createFolds(dataset$target_variable, k = k)\n        \n        # Initialize a list to store the performance metrics for this repetition\n        performance_metrics &lt;- list()\n        \n        # Loop through each fold\n        for (i in 1:k) {\n            # Use the i-th fold as the validation set\n            validation_indices &lt;- folds[[i]]\n            validation_set &lt;- dataset[validation_indices, ]\n            \n            # Use the remaining folds as the training set\n            training_set &lt;- dataset[-validation_indices, ]\n            \n            # Train the model on the training set\n            model &lt;- models[[model_name]](training_set)\n            \n            # Evaluate the model on the validation set\n            performance &lt;- evaluate_model(model, validation_set)\n            \n            # Store the performance metric\n            performance_metrics[[i]] &lt;- performance\n        }\n        \n        # Store the performance metrics for this repetition\n        model_performance_metrics[[rep]] &lt;- performance_metrics\n    }\n    \n    # Store the performance metrics for this model\n    all_performance_metrics[[model_name]] &lt;- model_performance_metrics\n}\n\n# Calculate the average performance metric for each model across all repetitions\naverage_performance &lt;- sapply(all_performance_metrics, function(metrics) mean(unlist(metrics)))\n\n# Output the average performance for each model\nprint(\"Average Performance for each model:\")\nprint(average_performance)"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlecture\ndow\ndate\nweek\ntopic\nprepare\nslides\nhw\nproject\njob\nhack-a-thon\nnotes\n\n\n\n\n0\nM\nFeb 3\n1\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nW\nFeb 5\n1\nBig Picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 Assigned\n\n\n2\nF\nFeb 7\n1\nWhat is Statistical Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\nM\nFeb 10\n2\nIntro to Linear Regression and Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nW\nFeb 12\n2\nMultiple Linear Regression and Data Splitting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 Due/ HW 2 Assigned\n\n\n5\nF\nFeb 14\n2\nData Splitting and KNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nM\nFeb 17\n3\nKNN + Preprocessing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\nW\nFeb 19\n3\nClassification and Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 3 Assigned\n\n\n8\nF\nFeb 21\n3\nClassification Metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nM\nFeb 24\n4\nJob Application Intro\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 Due / Job Application 1 Assigned\n\n\n10\nW\nFeb 26\n4\nROC and AUC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nF\nFeb 28\n4\nCross-Validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\nM\nMar 3\n5\nPre-processing, Missing Data, and CV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 3 Due/ HW 4 Assigned\n\n\n13\nW\nMar 5\n5\nPre-processing, Missing Data, and CV Continued\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\nF\nMar 7\n5\nResume/Cover Letter Peer-Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCover Letter and Resume Due in Class (Attendance Mandatory)\n\n\n15\nM\nMar 10\n6\nWorkflow Sets and Feature Selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 4 Due/ HW 5 Assigned\n\n\n16\nW\nMar 12\n6\nFeature Selection and Regularization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17\nF\nMar 14\n6\nRegularization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18\nM\nMar 17\n7\nModel Tuning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 5 Due / HW 6 Assigned\n\n\n19\nW\nMar 19\n7\nRegression Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20\nF\nMar 21\n7\nRegression Trees Continued\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJob Application 1 Due\n\n\n\nM\n\n\nNA\nSpring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\n\n\nNA\nSpring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\n\n\nNA\nSpring Break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n21\nM\nMar 31\n8\nJob Interview Intro, Decision Trees Continued\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 6 Due / HW 7 Assigned / Job Interviews Announced / Project Announced\n\n\n22\nW\nApr 2\n8\nClassification Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n23\nF\nApr 4\n8\nBrian Bava Visit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions for Brian Due\n\n\n24\nM\nApr 7\n9\nClassification Trees Continued\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 7 Due/ HW 8 Assigned\n\n\n25\nW\nApr 9\n9\nBagging/Random Forests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n26\nF\nApr 11\n9\nJob Interview Panel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n27\nM\nApr 14\n10\nBoosting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 8 Due/ HW 9 Assigned\n\n\n28\nW\nApr 16\n10\nMore on Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n29\nF\nApr 18\n10\nMore on Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30\nM\nApr 21\n11\nImbalanced Classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 9 Due/ HW 10 Assigned\n\n\n31\nW\nApr 23\n11\nHigh Performance Computing (Guest Lecturer Jim Beck)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n32\nF\nApr 25\n11\nImbalanced Classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n33\nM\nApr 28\n12\nWork Day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 10 Due\n\n\n34\nW\nApr 30\n12\nWork Day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n35\nF\nMay 2\n12\nWork Day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n36\nM\nMay 5\n13\nPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n37\nW\nMay 7\n13\nPCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n38\nF\nMay 9\n13\nProject Presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n39\nM\nMay 12\n14\nHack-a-thon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n40\nW\nMay 14\n14\nHack-a-thon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n41\nF\nMay 16\n14\nHack-a-thon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Presentations\nTH\nMay 22\n15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHack-a-thon CSV and HTML files due: 6:30pm",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#books-free-online",
    "href": "computing-r-resources.html#books-free-online",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#rstudio-primers",
    "href": "computing-r-resources.html#rstudio-primers",
    "title": "Resources for learning R",
    "section": "RStudio Primers",
    "text": "RStudio Primers\n\nInteractive LearnR Tutorial\nAnother set of tutorials\nR Primers for full list of tutorials. Note that we will be using a simplified version of ggplot2 in this course.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nRStudio Cheatsheets\nR Fun workshops and videos by Duke Center for Data and Visualization Sciences.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "prepare/prep-05.html",
    "href": "prepare/prep-05.html",
    "title": "Preparation for Data Splitting and KNN",
    "section": "",
    "text": "Read Chapter 3.5-4.1 from ISLR2\nRead Chapters 9 of TMWR"
  },
  {
    "objectID": "prepare/prep-05.html#assigned-readings-videos",
    "href": "prepare/prep-05.html#assigned-readings-videos",
    "title": "Preparation for Data Splitting and KNN",
    "section": "",
    "text": "Read Chapter 3.5-4.1 from ISLR2\nRead Chapters 9 of TMWR"
  },
  {
    "objectID": "prepare/prep-04.html",
    "href": "prepare/prep-04.html",
    "title": "Preparation for Multiple Linear Regression & Data Splitting",
    "section": "",
    "text": "Read Chapter 3.2, 3.3, and (optionally) 3.4 from ISLR2\nIf you you don’t have experience with the tidyverse please read Chapter 2 of TMWR\nRead Chapters 3-6 of TMWR"
  },
  {
    "objectID": "prepare/prep-04.html#assigned-readings-videos",
    "href": "prepare/prep-04.html#assigned-readings-videos",
    "title": "Preparation for Multiple Linear Regression & Data Splitting",
    "section": "",
    "text": "Read Chapter 3.2, 3.3, and (optionally) 3.4 from ISLR2\nIf you you don’t have experience with the tidyverse please read Chapter 2 of TMWR\nRead Chapters 3-6 of TMWR"
  },
  {
    "objectID": "prepare/prep-30.html",
    "href": "prepare/prep-30.html",
    "title": "Preparation for Imbalanced Classes",
    "section": "",
    "text": "SMOTE Paper\nInteresting Stackexchange Post"
  },
  {
    "objectID": "prepare/prep-30.html#assigned-readings-videos",
    "href": "prepare/prep-30.html#assigned-readings-videos",
    "title": "Preparation for Imbalanced Classes",
    "section": "",
    "text": "SMOTE Paper\nInteresting Stackexchange Post"
  },
  {
    "objectID": "prepare/prep-23.html",
    "href": "prepare/prep-23.html",
    "title": "Preparation for Brian’s Visit",
    "section": "",
    "text": "Load and review admissions data\nCome with several questions"
  },
  {
    "objectID": "prepare/prep-23.html#assigned-readings-videos",
    "href": "prepare/prep-23.html#assigned-readings-videos",
    "title": "Preparation for Brian’s Visit",
    "section": "",
    "text": "Load and review admissions data\nCome with several questions"
  },
  {
    "objectID": "prepare/prep-12.html",
    "href": "prepare/prep-12.html",
    "title": "Preparation for Pre-processing, Missing Data, and Cross Validation",
    "section": "",
    "text": "Read Chapters 11 of TMWR"
  },
  {
    "objectID": "prepare/prep-12.html#assigned-readings-videos",
    "href": "prepare/prep-12.html#assigned-readings-videos",
    "title": "Preparation for Pre-processing, Missing Data, and Cross Validation",
    "section": "",
    "text": "Read Chapters 11 of TMWR"
  },
  {
    "objectID": "prepare/prep-27.html",
    "href": "prepare/prep-27.html",
    "title": "Preparation for Classification",
    "section": "",
    "text": "Re-read 9.4 of TMWF"
  },
  {
    "objectID": "prepare/prep-27.html#assigned-readings-videos",
    "href": "prepare/prep-27.html#assigned-readings-videos",
    "title": "Preparation for Classification",
    "section": "",
    "text": "Re-read 9.4 of TMWF"
  },
  {
    "objectID": "prepare/prep-09.html",
    "href": "prepare/prep-09.html",
    "title": "Preparation for ROC and AUC",
    "section": "",
    "text": "Read Chapters 9 of TMWR"
  },
  {
    "objectID": "prepare/prep-09.html#assigned-readings-videos",
    "href": "prepare/prep-09.html#assigned-readings-videos",
    "title": "Preparation for ROC and AUC",
    "section": "",
    "text": "Read Chapters 9 of TMWR"
  },
  {
    "objectID": "prepare/prep-16.html",
    "href": "prepare/prep-16.html",
    "title": "Preparation for Model Tuning",
    "section": "",
    "text": "Read Chapters 12 of TMWR"
  },
  {
    "objectID": "prepare/prep-16.html#assigned-readings-videos",
    "href": "prepare/prep-16.html#assigned-readings-videos",
    "title": "Preparation for Model Tuning",
    "section": "",
    "text": "Read Chapters 12 of TMWR"
  },
  {
    "objectID": "prepare/prep-19.html",
    "href": "prepare/prep-19.html",
    "title": "Preparation for Decision Trees",
    "section": "",
    "text": "Read 8.1 of ISLR"
  },
  {
    "objectID": "prepare/prep-19.html#assigned-readings-videos",
    "href": "prepare/prep-19.html#assigned-readings-videos",
    "title": "Preparation for Decision Trees",
    "section": "",
    "text": "Read 8.1 of ISLR"
  },
  {
    "objectID": "prepare/prep-20.html",
    "href": "prepare/prep-20.html",
    "title": "Preparation for Decision Trees",
    "section": "",
    "text": "Read 8.1 of ISLR"
  },
  {
    "objectID": "prepare/prep-20.html#assigned-readings-videos",
    "href": "prepare/prep-20.html#assigned-readings-videos",
    "title": "Preparation for Decision Trees",
    "section": "",
    "text": "Read 8.1 of ISLR"
  },
  {
    "objectID": "prepare/prep-18.html",
    "href": "prepare/prep-18.html",
    "title": "Preparation for Model Tuning",
    "section": "",
    "text": "Read Chapters 13 and 14 of TMWR"
  },
  {
    "objectID": "prepare/prep-18.html#assigned-readings-videos",
    "href": "prepare/prep-18.html#assigned-readings-videos",
    "title": "Preparation for Model Tuning",
    "section": "",
    "text": "Read Chapters 13 and 14 of TMWR"
  },
  {
    "objectID": "prepare/prep-03.html",
    "href": "prepare/prep-03.html",
    "title": "Preparation for Linear Regression & Gradient Descent",
    "section": "",
    "text": "Read Chapter 3.1 from ISLR2\nIf you haven’t taken Multivariable Calculus or don’t remember what a partial derivative or gradient is watch this\nRead this article on gradient descent"
  },
  {
    "objectID": "prepare/prep-03.html#assigned-readings-videos",
    "href": "prepare/prep-03.html#assigned-readings-videos",
    "title": "Preparation for Linear Regression & Gradient Descent",
    "section": "",
    "text": "Read Chapter 3.1 from ISLR2\nIf you haven’t taken Multivariable Calculus or don’t remember what a partial derivative or gradient is watch this\nRead this article on gradient descent"
  },
  {
    "objectID": "prepare/prep-13.html",
    "href": "prepare/prep-13.html",
    "title": "Preparation for Pre-processing, Missing Data, and Cross Validation",
    "section": "",
    "text": "Read Chapters 11 of TMWR"
  },
  {
    "objectID": "prepare/prep-13.html#assigned-readings-videos",
    "href": "prepare/prep-13.html#assigned-readings-videos",
    "title": "Preparation for Pre-processing, Missing Data, and Cross Validation",
    "section": "",
    "text": "Read Chapters 11 of TMWR"
  },
  {
    "objectID": "prepare/prep-15.html",
    "href": "prepare/prep-15.html",
    "title": "Preparation for Regularization and Feature Selection",
    "section": "",
    "text": "Chapters 6.1-6.2 of ISLR2"
  },
  {
    "objectID": "prepare/prep-15.html#assigned-readings-videos",
    "href": "prepare/prep-15.html#assigned-readings-videos",
    "title": "Preparation for Regularization and Feature Selection",
    "section": "",
    "text": "Chapters 6.1-6.2 of ISLR2"
  },
  {
    "objectID": "prepare/prep-10.html",
    "href": "prepare/prep-10.html",
    "title": "Preparation for Cross-Validation",
    "section": "",
    "text": "Read Chapter 5.1 from ISLR2\nRead Chapters 10 of TMWR"
  },
  {
    "objectID": "prepare/prep-10.html#assigned-readings-videos",
    "href": "prepare/prep-10.html#assigned-readings-videos",
    "title": "Preparation for Cross-Validation",
    "section": "",
    "text": "Read Chapter 5.1 from ISLR2\nRead Chapters 10 of TMWR"
  },
  {
    "objectID": "slides/32-PCA.html#remainder-of-semester",
    "href": "slides/32-PCA.html#remainder-of-semester",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Remainder of Semester",
    "text": "Remainder of Semester\n\nProject (Due Friday)\nHack-a-thon (Next week + Final Exam Time)\nJob Application 2 (Due Next Friday)\nTwo Technical Interviews…\n\n\n\nTOO MUCH!!!"
  },
  {
    "objectID": "slides/32-PCA.html#lessons-learned-by-dr.-f",
    "href": "slides/32-PCA.html#lessons-learned-by-dr.-f",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Lesson’s Learned by Dr. F",
    "text": "Lesson’s Learned by Dr. F\n\nLearning objectives coming in:\n\nMachine Learning Theory\nProfessional Development\nCommunication!!!\n\n\n\n\nDo less to do more… don’t need multiple assignments for each of these\nPrioritize an assessment before Spring Break\nBuild in better time-management and revision checkpoints"
  },
  {
    "objectID": "slides/32-PCA.html#next-time",
    "href": "slides/32-PCA.html#next-time",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Next Time",
    "text": "Next Time\n\nFirst half of semester focused on ML pipeline and professional development\n\nAssess with written exam before Spring Break\nResume and Cover Letter due before Spring Break\n\nSecond half of semester focus on different models and communication\n\nBehavioral Interview shortly after Spring Break\nSample Analysis due last week of class with draft due earlier and opportunities to revise\nTechnical interview during final’s week\n\nNot sure about Hack-a-thon and Project\n\nWant to do both but feels like that is too much"
  },
  {
    "objectID": "slides/32-PCA.html#proposal-for-rest-of-semester",
    "href": "slides/32-PCA.html#proposal-for-rest-of-semester",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Proposal For Rest of Semester",
    "text": "Proposal For Rest of Semester\n\nIf you like original syllabus, feel free to stick with it (no penalty)\nProject - still due Friday\n\nOne-pager\nPresentation\n\nJob Application 2: Hack-a-thon report\nHack-a-thon - still next week\n\nPresentation\nPredictions and report due during final exam period\n\nHack-a-thon report counts as second Job Application\nTechnical interviews:\n\n45 minute technical interview during final exam week will replace your first technical interview grade if you do better… even if you don’t do the first interview\nThink of first interview as “practice”"
  },
  {
    "objectID": "slides/32-PCA.html#new-grade-structure-proposal",
    "href": "slides/32-PCA.html#new-grade-structure-proposal",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "New Grade Structure (Proposal)",
    "text": "New Grade Structure (Proposal)\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n10%\n\n\nJob Application 1\n15%\n\n\nJob Interview 1\n15%\n\n\nJob Interview 2\n20%\n\n\nHack-a-thon + Report\n25%\n\n\nProject\n15%"
  },
  {
    "objectID": "slides/32-PCA.html#computational-set-up",
    "href": "slides/32-PCA.html#computational-set-up",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/32-PCA.html#data-mnist",
    "href": "slides/32-PCA.html#data-mnist",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Data: mnist",
    "text": "Data: mnist\n\nMNIST Database: Modified National Institute of Standards and Technology Database\nLarge database of handwritten digits\n\n60,000 training images\n10,000 test images\n\nEach image:\n\n28x28 black and white pixels\n\\(28\\times 28\\times 1 = 784\\)"
  },
  {
    "objectID": "slides/32-PCA.html#loading-data",
    "href": "slides/32-PCA.html#loading-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Loading data",
    "text": "Loading data\n\nlibrary(dslabs)\nmnist &lt;- read_mnist()\nmnist_train &lt;- mnist$train$images\nmnist_train |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n18\n18\n18\n126\n136\n175\n26\n166\n255\n247\n127\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n30\n36\n94\n154\n170\n253\n253\n253\n253\n253\n225\n172\n253\n242\n195\n64\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n49\n238\n253\n253\n253\n253\n253\n253\n253\n253\n251\n93\n82\n82\n56\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n219\n253\n253\n253\n253\n253\n198\n182\n247\n241\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n80\n156\n107\n253\n253\n205\n11\n0\n43\n154\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n1\n154\n253\n90\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n139\n253\n190\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n190\n253\n70\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n35\n241\n225\n160\n108\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n81\n240\n253\n253\n119\n25\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n45\n186\n253\n253\n150\n27\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n93\n252\n253\n187\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n249\n64\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n46\n130\n183\n253\n253\n207\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n39\n148\n229\n253\n253\n253\n250\n182\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n114\n221\n253\n253\n253\n253\n201\n78\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n23\n66\n213\n253\n253\n253\n253\n198\n81\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n171\n219\n253\n253\n253\n253\n195\n80\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n55\n172\n226\n253\n253\n253\n253\n244\n133\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n136\n253\n253\n253\n212\n135\n132\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n51\n159\n253\n159\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n238\n252\n252\n252\n237\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n54\n227\n253\n252\n239\n233\n252\n57\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n60\n224\n252\n253\n252\n202\n84\n252\n253\n122\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n163\n252\n252\n252\n253\n252\n252\n96\n189\n253\n167\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n51\n238\n253\n253\n190\n114\n253\n228\n47\n79\n255\n168\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n238\n252\n252\n179\n12\n75\n121\n21\n0\n0\n253\n243\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n38\n165\n253\n233\n208\n84\n0\n0\n0\n0\n0\n0\n253\n252\n165\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n7\n178\n252\n240\n71\n19\n28\n0\n0\n0\n0\n0\n0\n253\n252\n195\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n57\n252\n252\n63\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n252\n195\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n198\n253\n190\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n255\n253\n196\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n76\n246\n252\n112\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n252\n148\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n230\n25\n0\n0\n0\n0\n0\n0\n0\n0\n7\n135\n253\n186\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n223\n0\n0\n0\n0\n0\n0\n0\n0\n7\n131\n252\n225\n71\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n145\n0\n0\n0\n0\n0\n0\n0\n48\n165\n252\n173\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n86\n253\n225\n0\n0\n0\n0\n0\n0\n114\n238\n253\n162\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n249\n146\n48\n29\n85\n178\n225\n253\n223\n167\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n252\n252\n229\n215\n252\n252\n252\n196\n130\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n28\n199\n252\n252\n253\n252\n252\n233\n145\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n25\n128\n252\n253\n252\n141\n37\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n67\n232\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n62\n81\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n120\n180\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n126\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n153\n210\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n220\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n27\n254\n162\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n222\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n183\n254\n125\n0\n0\n0\n0\n0\n0\n0\n0\n0\n46\n245\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n198\n254\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n120\n254\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n23\n231\n254\n29\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n254\n120\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n163\n254\n216\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n254\n67\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n86\n178\n248\n254\n91\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n254\n85\n0\n0\n0\n47\n49\n116\n144\n150\n241\n243\n234\n179\n241\n252\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n150\n253\n237\n207\n207\n207\n253\n254\n250\n240\n198\n143\n91\n28\n5\n233\n250\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n119\n177\n177\n177\n177\n177\n98\n56\n0\n0\n0\n0\n0\n102\n254\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n137\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n255\n94\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n96\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n255\n153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n254\n153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n124\n253\n255\n63\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n244\n251\n253\n62\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n127\n251\n251\n253\n62\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n68\n236\n251\n211\n31\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n60\n228\n251\n251\n94\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n155\n253\n253\n189\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n20\n253\n251\n235\n66\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n32\n205\n253\n251\n126\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n104\n251\n253\n184\n15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n80\n240\n251\n193\n23\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n32\n253\n253\n253\n159\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n151\n251\n251\n251\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n221\n251\n251\n172\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n234\n251\n251\n196\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n251\n251\n89\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n255\n253\n253\n31\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n228\n253\n247\n140\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n64\n251\n253\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n64\n251\n253\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n193\n253\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n55\n148\n210\n253\n253\n113\n87\n148\n55\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n87\n232\n252\n253\n189\n210\n252\n252\n253\n168\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n57\n242\n252\n190\n65\n5\n12\n182\n252\n253\n116\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n252\n252\n183\n14\n0\n0\n92\n252\n252\n225\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n132\n253\n252\n146\n14\n0\n0\n0\n215\n252\n252\n79\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n126\n253\n247\n176\n9\n0\n0\n8\n78\n245\n253\n129\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n232\n252\n176\n0\n0\n0\n36\n201\n252\n252\n169\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22\n252\n252\n30\n22\n119\n197\n241\n253\n252\n251\n77\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n231\n252\n253\n252\n252\n252\n226\n227\n252\n231\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n55\n235\n253\n217\n138\n42\n24\n192\n252\n143\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n62\n255\n253\n109\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n71\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n71\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n106\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n45\n255\n253\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n218\n252\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n252\n189\n42\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n184\n252\n170\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n147\n252\n42\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n25\n100\n122\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n33\n151\n208\n252\n252\n252\n146\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n40\n152\n244\n252\n253\n224\n211\n252\n232\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n15\n152\n239\n252\n252\n252\n216\n31\n37\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n252\n252\n252\n252\n217\n29\n0\n37\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n181\n252\n252\n220\n167\n30\n0\n0\n77\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n26\n128\n58\n22\n0\n0\n0\n0\n100\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n157\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n110\n121\n122\n121\n202\n252\n194\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n53\n179\n253\n253\n255\n253\n253\n228\n35\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n54\n227\n252\n243\n228\n170\n242\n252\n252\n231\n117\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n78\n252\n252\n125\n59\n0\n18\n208\n252\n252\n252\n252\n87\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n135\n252\n252\n180\n16\n0\n21\n203\n253\n247\n129\n173\n252\n252\n184\n66\n49\n49\n0\n0\n0\n0\n0\n0\n0\n0\n3\n136\n252\n241\n106\n17\n0\n53\n200\n252\n216\n65\n0\n14\n72\n163\n241\n252\n252\n223\n0\n0\n0\n0\n0\n0\n0\n0\n105\n252\n242\n88\n18\n73\n170\n244\n252\n126\n29\n0\n0\n0\n0\n0\n89\n180\n180\n37\n0\n0\n0\n0\n0\n0\n0\n0\n231\n252\n245\n205\n216\n252\n252\n252\n124\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n207\n252\n252\n252\n252\n178\n116\n36\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n93\n143\n121\n23\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "slides/32-PCA.html#digits",
    "href": "slides/32-PCA.html#digits",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[1,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/32-PCA.html#digits-1",
    "href": "slides/32-PCA.html#digits-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[2,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/32-PCA.html#digits-2",
    "href": "slides/32-PCA.html#digits-2",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[3,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/32-PCA.html#digits-3",
    "href": "slides/32-PCA.html#digits-3",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[4,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/32-PCA.html#what-if-we-want-to-visualize-our-data",
    "href": "slides/32-PCA.html#what-if-we-want-to-visualize-our-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "What if we want to visualize our data?",
    "text": "What if we want to visualize our data?\n\n\nCode\nmnist_train &lt;- as_tibble(mnist_train)\nggplot(mnist_train, aes(x = V300, y = V301, \n                        color = as_factor(mnist$train$labels))) + \n  geom_point()"
  },
  {
    "objectID": "slides/32-PCA.html#unsupervised-learning-dimensionality-reduction",
    "href": "slides/32-PCA.html#unsupervised-learning-dimensionality-reduction",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Unsupervised Learning & Dimensionality Reduction",
    "text": "Unsupervised Learning & Dimensionality Reduction\n\nUnsupervised Learning: ML for unlabeled data (i.e. no response variables)\n\nGoal: Uncover patterns/structure within data\nTasks:\n\nClustering: finding sub-groups within our data\nDimensionality Reduction: reducing the number of columns in our data set… why?"
  },
  {
    "objectID": "slides/32-PCA.html#dimensionality-reduction",
    "href": "slides/32-PCA.html#dimensionality-reduction",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nGoal phrasing 1: Reduce the number of columns, while losing as little information as possible\nGoal phrasing 2: Extract lower-dimensional structure from our data\nAnalogy: file compression"
  },
  {
    "objectID": "slides/32-PCA.html#which-one-is-compressed",
    "href": "slides/32-PCA.html#which-one-is-compressed",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Which One is Compressed?",
    "text": "Which One is Compressed?"
  },
  {
    "objectID": "slides/32-PCA.html#which-one-is-compressed-1",
    "href": "slides/32-PCA.html#which-one-is-compressed-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Which One is Compressed?",
    "text": "Which One is Compressed?\n\n\n\n\n\n\n\n\n\n1,330 KB\n\n\n\n\n\n\n\n396 KB"
  },
  {
    "objectID": "slides/32-PCA.html#idea",
    "href": "slides/32-PCA.html#idea",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Idea",
    "text": "Idea\n\nWe managed to:\n\nReduce file size by 70%\nNote lose much information\nExtract underlying structure (a duck)"
  },
  {
    "objectID": "slides/32-PCA.html#thinking-about-structure-in-tabular-data",
    "href": "slides/32-PCA.html#thinking-about-structure-in-tabular-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Thinking about structure in tabular data",
    "text": "Thinking about structure in tabular data"
  },
  {
    "objectID": "slides/32-PCA.html#underlying-structure",
    "href": "slides/32-PCA.html#underlying-structure",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Underlying Structure",
    "text": "Underlying Structure\n\n\nWhat dimension is our data in?\nWhat is the underlying structure here?\nWhat is the dimension of a plane?"
  },
  {
    "objectID": "slides/32-PCA.html#visualizing-plane",
    "href": "slides/32-PCA.html#visualizing-plane",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing Plane",
    "text": "Visualizing Plane"
  },
  {
    "objectID": "slides/32-PCA.html#thinking-about-structure-in-tabular-data-1",
    "href": "slides/32-PCA.html#thinking-about-structure-in-tabular-data-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Thinking about structure in tabular data",
    "text": "Thinking about structure in tabular data"
  },
  {
    "objectID": "slides/32-PCA.html#underlying-structure-1",
    "href": "slides/32-PCA.html#underlying-structure-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Underlying Structure",
    "text": "Underlying Structure\n\n\nWhat dimension is our data in?\nWhat is the underlying structure here?\nWhat is the dimension of a line?"
  },
  {
    "objectID": "slides/32-PCA.html#visualizing-2d",
    "href": "slides/32-PCA.html#visualizing-2d",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing 2D",
    "text": "Visualizing 2D"
  },
  {
    "objectID": "slides/32-PCA.html#visualizing-1d",
    "href": "slides/32-PCA.html#visualizing-1d",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing 1D",
    "text": "Visualizing 1D"
  },
  {
    "objectID": "slides/32-PCA.html#visualizing-1d-data",
    "href": "slides/32-PCA.html#visualizing-1d-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing 1D Data",
    "text": "Visualizing 1D Data"
  },
  {
    "objectID": "slides/32-PCA.html#thinking-about-structure-in-tabular-data-2",
    "href": "slides/32-PCA.html#thinking-about-structure-in-tabular-data-2",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Thinking about structure in tabular data",
    "text": "Thinking about structure in tabular data"
  },
  {
    "objectID": "slides/32-PCA.html#underlying-structure-2",
    "href": "slides/32-PCA.html#underlying-structure-2",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Underlying Structure",
    "text": "Underlying Structure\n\n\nWhat dimension is our data in?\nWhat is the underlying structure here?\nWhat is the dimension of a plane?"
  },
  {
    "objectID": "slides/32-PCA.html#visualizing-plane-1",
    "href": "slides/32-PCA.html#visualizing-plane-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing Plane",
    "text": "Visualizing Plane"
  },
  {
    "objectID": "slides/32-PCA.html#discussion",
    "href": "slides/32-PCA.html#discussion",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Discussion",
    "text": "Discussion\n\nWhat’s the difference between the first two scenario’s and the third scenario?\n\nHow much have we reduced the dimension?\nHow much information have we lost?"
  },
  {
    "objectID": "slides/32-PCA.html#vectors-and-projections",
    "href": "slides/32-PCA.html#vectors-and-projections",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Vector’s and Projections",
    "text": "Vector’s and Projections"
  },
  {
    "objectID": "slides/32-PCA.html#basis-vectors-and-new-coordinates",
    "href": "slides/32-PCA.html#basis-vectors-and-new-coordinates",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Basis Vectors and New Coordinates",
    "text": "Basis Vectors and New Coordinates\n\nPlane above: \\(z = x + y\\)\nNew Directions:\n\nNew Direction 1: \\(\\vec{d}_1 = \\langle 1, 1, 2\\rangle\\)\nNew Direction 2: \\(\\vec{d}_1 = \\langle 1, -1, 0\\rangle\\)\n\nNew data:\n\nNew \\(x\\): \\(1\\times x_{old} + 1\\times y_{old} + 2\\times z_{old}\\)\nNew \\(y\\): \\(1\\times x_{old} - 1\\times y_{old} + 0\\times z_{old}\\)\n\nNote: Not quite correct, need to re-normalize"
  },
  {
    "objectID": "slides/32-PCA.html#new-data",
    "href": "slides/32-PCA.html#new-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "New Data",
    "text": "New Data\n\nnew_data &lt;- data |&gt; \n  mutate(new_x = x + y + 2*z,\n         new_y = x - y,\n         new_x = new_x/6, #re-normalizing\n         new_y = new_y/2)\n\nnew_data |&gt; head() |&gt; kable()\n\n\n\n\nx\ny\nz\nnew_x\nnew_y\n\n\n\n\n-0.5604756\n-0.9957987\n-1.5562744\n-0.7781372\n0.2176615\n\n\n-0.2301775\n-1.0399550\n-1.2701325\n-0.6350663\n0.4048888\n\n\n1.5587083\n-0.0179802\n1.5407281\n0.7703640\n0.7883443\n\n\n0.0705084\n-0.1321751\n-0.0616667\n-0.0308334\n0.1013418\n\n\n0.1292877\n-2.5493428\n-2.4200550\n-1.2100275\n1.3393153\n\n\n1.7150650\n1.0405735\n2.7556384\n1.3778192\n0.3372458"
  },
  {
    "objectID": "slides/32-PCA.html#whats-actually-happening",
    "href": "slides/32-PCA.html#whats-actually-happening",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "What’s actually happening",
    "text": "What’s actually happening\n\nWe are projecting each observation onto our new directions \\(\\vec{d}_1\\) and \\(\\vec{d}_2\\)\nVisualization"
  },
  {
    "objectID": "slides/32-PCA.html#projecting-our-data",
    "href": "slides/32-PCA.html#projecting-our-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Projecting our data",
    "text": "Projecting our data"
  },
  {
    "objectID": "slides/32-PCA.html#plotting-these",
    "href": "slides/32-PCA.html#plotting-these",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Plotting these",
    "text": "Plotting these\n\nnew_data |&gt; \n  ggplot(aes(x = new_x, y = new_y)) +\n           geom_point()"
  },
  {
    "objectID": "slides/32-PCA.html#pca-vocabulary",
    "href": "slides/32-PCA.html#pca-vocabulary",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "PCA Vocabulary",
    "text": "PCA Vocabulary\n\nPrincipal Component (PC1): direction in \\(p\\)-dimensional space (e.g. \\(\\langle 1, 1, 2\\rangle\\))\nScores: our new variables (e.g. \\((-0.56\\times 1 + -0.996\\times 1 + -1.56\\times 2)/6 = -0.778\\))\nLoadings: For direction above’\n\nLoading on \\(x\\) is 1\nLoading on \\(y\\) is 1\nLoading on \\(z\\) is 2"
  },
  {
    "objectID": "slides/32-PCA.html#recall-variance",
    "href": "slides/32-PCA.html#recall-variance",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Recall: Variance",
    "text": "Recall: Variance\n\n\nWhat is variance?\nIntuitively: what does variance measure?\nVariance: \\(\\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar{x})^2\\)\n\nAverage of the squared distance from zero of each observation"
  },
  {
    "objectID": "slides/32-PCA.html#idea-behind-pca",
    "href": "slides/32-PCA.html#idea-behind-pca",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Idea behind PCA",
    "text": "Idea behind PCA\n\nSelect first PC so variance of scores is the maximum\nIteratively:\n\nSelect next PC so variance of scores is maximize AND new PC is orthogonal to all other PCs\n\n\n\n\nWhat does orthogonal mean?"
  },
  {
    "objectID": "slides/32-PCA.html#easy-example",
    "href": "slides/32-PCA.html#easy-example",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Easy Example",
    "text": "Easy Example\n\nExercise: What should the first and second PCs be?"
  },
  {
    "objectID": "slides/32-PCA.html#how-much-variance-is-explained-by-each-of-the-pcs",
    "href": "slides/32-PCA.html#how-much-variance-is-explained-by-each-of-the-pcs",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "How much variance is explained by each of the PC’s?",
    "text": "How much variance is explained by each of the PC’s?\n\nvar_exp &lt;- easy_ex |&gt; \n  mutate(PC1 = x,\n         PC2 = y) |&gt; \n  summarize(var1 = var(PC1), \n            var2 = var(PC2))\n\nvar_exp |&gt; kable()\n\n\n\n\nvar1\nvar2\n\n\n\n\n0.9834589\n0.0637151"
  },
  {
    "objectID": "slides/32-PCA.html#what-proportion-of-variance-is-explained-by-each-of-the-pcs",
    "href": "slides/32-PCA.html#what-proportion-of-variance-is-explained-by-each-of-the-pcs",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "What proportion of variance is explained by each of the PC’s?",
    "text": "What proportion of variance is explained by each of the PC’s?\n\nvar_exp |&gt; \n  pivot_longer(everything()) |&gt; \n  mutate(proportion = value/sum(value)) |&gt; \n  kable()\n\n\n\n\nname\nvalue\nproportion\n\n\n\n\nvar1\n0.9834589\n0.9391552\n\n\nvar2\n0.0637151\n0.0608448\n\n\n\n\n\n\n93% of our variance (information) is contained in our first PC"
  },
  {
    "objectID": "slides/32-PCA.html#harder-example",
    "href": "slides/32-PCA.html#harder-example",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Harder Example",
    "text": "Harder Example\n\nExercise: What should the first and second PCs be?"
  },
  {
    "objectID": "slides/32-PCA.html#how-much-variance-is-explained-by-each-of-the-pcs-1",
    "href": "slides/32-PCA.html#how-much-variance-is-explained-by-each-of-the-pcs-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "How much variance is explained by each of the PC’s?",
    "text": "How much variance is explained by each of the PC’s?\n\nvar_exp &lt;- harder_ex |&gt; \n  mutate(PC1 = (x + y)/2,\n         PC2 = (x-y)/2) |&gt; \n  summarize(var1 = var(PC1), \n            var2 = var(PC2))\n\nvar_exp |&gt; kable()\n\n\n\n\nvar1\nvar2\n\n\n\n\n1.021035\n0.0159288"
  },
  {
    "objectID": "slides/32-PCA.html#next-time-1",
    "href": "slides/32-PCA.html#next-time-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Next Time",
    "text": "Next Time\n\nUsing R to apply this to bigger data sets\nMore on interpreting PCA\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/34-Surveys.html#surveys",
    "href": "slides/34-Surveys.html#surveys",
    "title": "MATH 427: Fill out your surveys",
    "section": "Surveys",
    "text": "Surveys\n\nPlease fill out this post-survey about the apps.\nPlease fill out your course evaluation survey.\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/10-roc-auc.html#computational-set-up",
    "href": "slides/10-roc-auc.html#computational-set-up",
    "title": "MATH 427: ROC and AUC",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\nlibrary(ggforce) # sina plots\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/10-roc-auc.html#default-dataset",
    "href": "slides/10-roc-auc.html#default-dataset",
    "title": "MATH 427: ROC and AUC",
    "section": "Default Dataset",
    "text": "Default Dataset\n\n\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\nhead(Default) |&gt; kable()  # print first six observations\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n\n\nNo\nYes\n817.1804\n12106.135\n\n\nNo\nNo\n1073.5492\n31767.139\n\n\nNo\nNo\n529.2506\n35704.494\n\n\nNo\nNo\n785.6559\n38463.496\n\n\nNo\nYes\n919.5885\n7491.559\n\n\n\n\n\n\nResponse Variable: default\n\nDefault |&gt; \n  tabyl(default) |&gt;  # class frequencies\n  kable()           # Make it look nice\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n9667\n0.9667\n\n\nYes\n333\n0.0333"
  },
  {
    "objectID": "slides/10-roc-auc.html#split-the-data",
    "href": "slides/10-roc-auc.html#split-the-data",
    "title": "MATH 427: ROC and AUC",
    "section": "Split the data",
    "text": "Split the data\n\nset.seed(427)\n\ndefault_split &lt;- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n\n&lt;Training/Testing/Total&gt;\n&lt;6000/4000/10000&gt;\n\ndefault_train &lt;- training(default_split)\ndefault_test &lt;- testing(default_split)"
  },
  {
    "objectID": "slides/10-roc-auc.html#k-nearest-neighbors-classifier-build-model",
    "href": "slides/10-roc-auc.html#k-nearest-neighbors-classifier-build-model",
    "title": "MATH 427: ROC and AUC",
    "section": "K-Nearest Neighbors Classifier: Build Model",
    "text": "K-Nearest Neighbors Classifier: Build Model\n\nResponse (\\(Y\\)): default\nPredictor (\\(X\\)): balance\n\n\nknnfit &lt;- nearest_neighbor(neighbors = 10) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;  \n  fit(default ~ balance, data = default_train)   # fit 10-nn model"
  },
  {
    "objectID": "slides/10-roc-auc.html#k-nearest-neighbors-classifier-predictions",
    "href": "slides/10-roc-auc.html#k-nearest-neighbors-classifier-predictions",
    "title": "MATH 427: ROC and AUC",
    "section": "K-Nearest Neighbors Classifier: Predictions",
    "text": "K-Nearest Neighbors Classifier: Predictions\n\nClass labelsProbabilities\n\n\n\npredict(knnfit, new_data = default_test, type = \"class\") |&gt; head() |&gt; kable()   # obtain predictions as classes\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\n\n\n\n\n\n\nPredicts class w/ maximum probability\n\n\npredict(knnfit, new_data = default_test, type = \"prob\") |&gt; head() |&gt; kable() # obtain predictions as probabilities\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0"
  },
  {
    "objectID": "slides/10-roc-auc.html#fitting-a-logistic-regression",
    "href": "slides/10-roc-auc.html#fitting-a-logistic-regression",
    "title": "MATH 427: ROC and AUC",
    "section": "Fitting a logistic regression",
    "text": "Fitting a logistic regression\nFitting a logistic regression model with default as the response and balance as the predictor:\n\nlogregfit &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |&gt; kable()  # obtain results\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6926385\n0.4659035\n-22.95033\n0\n\n\nbalance\n0.0055327\n0.0002841\n19.47329\n0"
  },
  {
    "objectID": "slides/10-roc-auc.html#making-predictions-in-r",
    "href": "slides/10-roc-auc.html#making-predictions-in-r",
    "title": "MATH 427: ROC and AUC",
    "section": "Making predictions in R",
    "text": "Making predictions in R\n\nClass LabelsLog-OddsProbabilities\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |&gt; kable()   # obtain class predictions\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\n\n\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |&gt; kable()   # obtain log-odds predictions\n\n\n\n\nx\n\n\n\n\n-6.819727\n\n\n\n\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |&gt; kable()  # obtain probability predictions\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.9989092\n0.0010908"
  },
  {
    "objectID": "slides/10-roc-auc.html#binary-classifiers",
    "href": "slides/10-roc-auc.html#binary-classifiers",
    "title": "MATH 427: ROC and AUC",
    "section": "Binary Classifiers",
    "text": "Binary Classifiers\n\nStart with binary classification scenarios\nWith binary classification, designate one category as “Success/Positive” and the other as “Failure/Negative”\n\nIf relevant to your problem: “Positive” should be the thing you’re trying to predict/care more about\nNote: “Positive” \\(\\neq\\) “Good”\nFor default: “Yes” is Positive\n\nSome metrics weight “Positives” more and viceversa"
  },
  {
    "objectID": "slides/10-roc-auc.html#last-time",
    "href": "slides/10-roc-auc.html#last-time",
    "title": "MATH 427: ROC and AUC",
    "section": "Last Time",
    "text": "Last Time\n\nConfusion Matrix\nMetrics based on confusion matrix\n\nAccuracy\nRecall/Sensitivity\nPrecision/PPV\nSpecificity\nNPV\nMCC\nF-Measure\n\nToday: ROC and AUC"
  },
  {
    "objectID": "slides/10-roc-auc.html#using-a-threshold",
    "href": "slides/10-roc-auc.html#using-a-threshold",
    "title": "MATH 427: ROC and AUC",
    "section": "Using a threshold",
    "text": "Using a threshold\n\nStep 1: Predict probabilities for all observations\n\n\ndefault_test_wprobs &lt;- default_test |&gt;\n  mutate(\n    knn_probs = predict(knnfit, new_data = default_test, type = \"prob\") |&gt; pull(.pred_Yes),\n    logistic_probs = predict(logregfit, new_data = default_test, type = \"prob\") |&gt; pull(.pred_Yes)\n  )\n\ndefault_test_wprobs |&gt; head() |&gt; kable()   # obtain probability predictions\n\n\n\n\ndefault\nstudent\nbalance\nincome\nknn_probs\nlogistic_probs\n\n\n\n\nNo\nNo\n729.5265\n44361.63\n0\n0.0012842\n\n\nNo\nYes\n808.6675\n17600.45\n0\n0.0019883\n\n\nNo\nYes\n1220.5838\n13268.56\n0\n0.0190870\n\n\nNo\nNo\n237.0451\n28251.70\n0\n0.0000843\n\n\nNo\nNo\n606.7423\n44994.56\n0\n0.0006514\n\n\nNo\nNo\n286.2326\n45042.41\n0\n0.0001107"
  },
  {
    "objectID": "slides/10-roc-auc.html#using-a-threshold-1",
    "href": "slides/10-roc-auc.html#using-a-threshold-1",
    "title": "MATH 427: ROC and AUC",
    "section": "Using a threshold",
    "text": "Using a threshold\n\nStep 1: Predict probabilities for all observations\nStep 2: Set a threshold to obtain class labels (0.5 below)\n\n\nthreshold &lt;- 0.5   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs &gt; threshold, \"Yes\", \"No\"))\n  )\n\ndefault_test_wprobs |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\nknn_probs\nlogistic_probs\nknn_preds\nlogistic_preds\n\n\n\n\nNo\nNo\n729.5265\n44361.63\n0\n0.0012842\nNo\nNo\n\n\nNo\nYes\n808.6675\n17600.45\n0\n0.0019883\nNo\nNo\n\n\nNo\nYes\n1220.5838\n13268.56\n0\n0.0190870\nNo\nNo\n\n\nNo\nNo\n237.0451\n28251.70\n0\n0.0000843\nNo\nNo\n\n\nNo\nNo\n606.7423\n44994.56\n0\n0.0006514\nNo\nNo\n\n\nNo\nNo\n286.2326\n45042.41\n0\n0.0001107\nNo\nNo"
  },
  {
    "objectID": "slides/10-roc-auc.html#using-a-threshold-2",
    "href": "slides/10-roc-auc.html#using-a-threshold-2",
    "title": "MATH 427: ROC and AUC",
    "section": "Using a threshold",
    "text": "Using a threshold\n\nStep 1: Predict probabilities for all observations\nStep 2: Set a threshold to obtain class labels (0.5 below)\n\n\nthreshold &lt;- 0.5   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs &gt; threshold, \"Yes\", \"No\")))\n\ndefault_test_wprobs |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\nknn_probs\nlogistic_probs\nknn_preds\nlogistic_preds\n\n\n\n\nNo\nNo\n729.5265\n44361.63\n0\n0.0012842\nNo\nNo\n\n\nNo\nYes\n808.6675\n17600.45\n0\n0.0019883\nNo\nNo\n\n\nNo\nYes\n1220.5838\n13268.56\n0\n0.0190870\nNo\nNo\n\n\nNo\nNo\n237.0451\n28251.70\n0\n0.0000843\nNo\nNo\n\n\nNo\nNo\n606.7423\n44994.56\n0\n0.0006514\nNo\nNo\n\n\nNo\nNo\n286.2326\n45042.41\n0\n0.0001107\nNo\nNo"
  },
  {
    "objectID": "slides/10-roc-auc.html#performance",
    "href": "slides/10-roc-auc.html#performance",
    "title": "MATH 427: ROC and AUC",
    "section": "Performance",
    "text": "Performance\n\nroc_metrics &lt;- metric_set(accuracy, sensitivity, specificity)\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\") |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9717500\n\n\nsensitivity\nbinary\n0.3565891\n\n\nspecificity\nbinary\n0.9922501"
  },
  {
    "objectID": "slides/10-roc-auc.html#low-threshold",
    "href": "slides/10-roc-auc.html#low-threshold",
    "title": "MATH 427: ROC and AUC",
    "section": "Low Threshold",
    "text": "Low Threshold\n\nthreshold &lt;- 0.1   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\")))\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\")  |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9060000\n\n\nsensitivity\nbinary\n0.7364341\n\n\nspecificity\nbinary\n0.9116507"
  },
  {
    "objectID": "slides/10-roc-auc.html#high-threshold",
    "href": "slides/10-roc-auc.html#high-threshold",
    "title": "MATH 427: ROC and AUC",
    "section": "High Threshold",
    "text": "High Threshold\n\nthreshold &lt;- 0.9   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\"))\n  )\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\") |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9685000\n\n\nsensitivity\nbinary\n0.0310078\n\n\nspecificity\nbinary\n0.9997417"
  },
  {
    "objectID": "slides/10-roc-auc.html#question",
    "href": "slides/10-roc-auc.html#question",
    "title": "MATH 427: ROC and AUC",
    "section": "Question",
    "text": "Question\n\nIf I want to improve Recall/Sensitivity should I increase or decrease my threshold?\nIf I want to improve my Precision/PPV should I increase or decrease my threshold?"
  },
  {
    "objectID": "slides/10-roc-auc.html#roc-curve-and-auc",
    "href": "slides/10-roc-auc.html#roc-curve-and-auc",
    "title": "MATH 427: ROC and AUC",
    "section": "ROC Curve and AUC",
    "text": "ROC Curve and AUC\n\nROC (Receiver Operating Characteristics) curve: popular graphic for comparing different classifiers across all possible thresholds\n\nPlots the (1-Specificity) along the x-axis and the Sensitivity (true positive rate) along the y-axis\n\nAUC: area under the AUC curve\n\nIdeal ROC curve will hug the top left corner\n\nIdea: How well is my classifier separating positives from negatives"
  },
  {
    "objectID": "slides/10-roc-auc.html#roc-curve-1",
    "href": "slides/10-roc-auc.html#roc-curve-1",
    "title": "MATH 427: ROC and AUC",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\n.threshold\nspecificity\nsensitivity\n\n\n\n\n-Inf\n0.0000000\n1.0000000\n\n\n0.0000\n0.0000000\n1.0000000\n\n\n0.0145\n0.8796177\n0.8217054\n\n\n0.0415\n0.8858176\n0.8139535\n\n\n0.0560\n0.8971842\n0.7829457\n\n\n0.0655\n0.8979592\n0.7751938"
  },
  {
    "objectID": "slides/10-roc-auc.html#roc-curve-plot",
    "href": "slides/10-roc-auc.html#roc-curve-plot",
    "title": "MATH 427: ROC and AUC",
    "section": "ROC Curve: Plot",
    "text": "ROC Curve: Plot\n\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/10-roc-auc.html#auc",
    "href": "slides/10-roc-auc.html#auc",
    "title": "MATH 427: ROC and AUC",
    "section": "AUC",
    "text": "AUC\n\nAUC: Area under the curve (ROC Curve that is)\nMeasures how good your model is at separating categories\nOnly for binary classification"
  },
  {
    "objectID": "slides/10-roc-auc.html#auc-in-r",
    "href": "slides/10-roc-auc.html#auc-in-r",
    "title": "MATH 427: ROC and AUC",
    "section": "AUC in R",
    "text": "AUC in R\n\nroc_auc(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |&gt;\n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.8757397"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-1",
    "href": "slides/10-roc-auc.html#pathological-example-1",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 1",
    "text": "Pathological Example 1"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-1-1",
    "href": "slides/10-roc-auc.html#pathological-example-1-1",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 1",
    "text": "Pathological Example 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{100}{100} = 1\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{100}{100} = 1\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-1-2",
    "href": "slides/10-roc-auc.html#pathological-example-1-2",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 1",
    "text": "Pathological Example 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{100}{100} = 1\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{55}{100} = 0.55\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-1-3",
    "href": "slides/10-roc-auc.html#pathological-example-1-3",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 1",
    "text": "Pathological Example 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{100}{100} = 1\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{0}{100} = 0\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-1-4",
    "href": "slides/10-roc-auc.html#pathological-example-1-4",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 1",
    "text": "Pathological Example 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{56}{100} = 0.56\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{0}{100} = 0\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-1-5",
    "href": "slides/10-roc-auc.html#pathological-example-1-5",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 1",
    "text": "Pathological Example 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{0}{100} = 0\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{0}{100} = 0\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-2",
    "href": "slides/10-roc-auc.html#pathological-example-2",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 2",
    "text": "Pathological Example 2"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-3",
    "href": "slides/10-roc-auc.html#pathological-example-3",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 3",
    "text": "Pathological Example 3"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-3-1",
    "href": "slides/10-roc-auc.html#pathological-example-3-1",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 3",
    "text": "Pathological Example 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{100}{100} = 1\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{100}{100} = 1\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-3-2",
    "href": "slides/10-roc-auc.html#pathological-example-3-2",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 3",
    "text": "Pathological Example 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{100}{100} = 1\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{62}{100} = 0.62\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-3-3",
    "href": "slides/10-roc-auc.html#pathological-example-3-3",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 3",
    "text": "Pathological Example 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{99}{100} = 0.99\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{58}{100} = 0.58\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-3-4",
    "href": "slides/10-roc-auc.html#pathological-example-3-4",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 3",
    "text": "Pathological Example 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{75}{100} = 0.75\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{29}{100} = 0.29\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-3-5",
    "href": "slides/10-roc-auc.html#pathological-example-3-5",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 3",
    "text": "Pathological Example 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Greens Above Line}}{\\text{All Greens}} = \\frac{37}{100} = 0.37\\] \\[1-\\text{Specificity} = 1-\\frac{TN}{TN + FP} = \\frac{FP}{TN + FP} = \\frac{\\text{Reds Above Line}}{\\text{All Reds}} = \\frac{0}{100} = 0\\]"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-4",
    "href": "slides/10-roc-auc.html#pathological-example-4",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 4",
    "text": "Pathological Example 4"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-5",
    "href": "slides/10-roc-auc.html#pathological-example-5",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 5",
    "text": "Pathological Example 5"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-6",
    "href": "slides/10-roc-auc.html#pathological-example-6",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 6",
    "text": "Pathological Example 6"
  },
  {
    "objectID": "slides/10-roc-auc.html#pathological-example-7",
    "href": "slides/10-roc-auc.html#pathological-example-7",
    "title": "MATH 427: ROC and AUC",
    "section": "Pathological Example 7",
    "text": "Pathological Example 7"
  },
  {
    "objectID": "slides/10-roc-auc.html#auc-questions",
    "href": "slides/10-roc-auc.html#auc-questions",
    "title": "MATH 427: ROC and AUC",
    "section": "AUC Questions",
    "text": "AUC Questions\n\nWhat should be the minimum AUC?\nWhat should be that maximum possible AUC?"
  },
  {
    "objectID": "slides/10-roc-auc.html#review-logistic-regression-vs-knn",
    "href": "slides/10-roc-auc.html#review-logistic-regression-vs-knn",
    "title": "MATH 427: ROC and AUC",
    "section": "Review: Logistic Regression vs KNN",
    "text": "Review: Logistic Regression vs KNN\n\nLogistic regression \\(\\implies\\) parametric , KNN \\(\\implies\\) non-parametric.\nLogistic regression \\(\\implies\\) only for classification problems (\\(Y\\) categorical), KNN \\(\\implies\\) both regression and classification.\nLogistic regression is (more) interpretable, KNN is not.\nLogistic regression allows qualitative predictors. Euclidean distance with KNN does not allow for qualitative predictors.\nPrediction: KNN can be pretty good for small \\(p\\), that is, \\(p \\le 4\\) and large \\(n\\). Performance of KNN deteriorates as \\(p\\) increases - curse of dimensionality."
  },
  {
    "objectID": "slides/10-roc-auc.html#classification-metrics-app",
    "href": "slides/10-roc-auc.html#classification-metrics-app",
    "title": "MATH 427: ROC and AUC",
    "section": "Classification Metrics App",
    "text": "Classification Metrics App\n\nDr. F will split you into four groups\nOn one of your computers connect to a tv and open this app\nDo the following based on your group number:\n\n1: Choose plane on the first screen\n2: Choose circle on the first screen\n3: Choose parabola on the first screen\n4: Choose sine curve on the first screen\n\nWe will generate data from this population… do you think KNN or logistic regression will yield a better classifier? Why?"
  },
  {
    "objectID": "slides/10-roc-auc.html#classification-metrics-app-1",
    "href": "slides/10-roc-auc.html#classification-metrics-app-1",
    "title": "MATH 427: ROC and AUC",
    "section": "Classification Metrics App",
    "text": "Classification Metrics App\n\nOn the second tab generate a small test and training set\nOn the third tab fit a KNN model with 5 neighbors and then a logistic regression model\nWhich model do you think will perform better based on the plots you see?\nChoose the better model, click fit, and click on the fourth tab"
  },
  {
    "objectID": "slides/10-roc-auc.html#questions",
    "href": "slides/10-roc-auc.html#questions",
    "title": "MATH 427: ROC and AUC",
    "section": "Questions",
    "text": "Questions\nUsing the app, try and answer the following questions:\n\nWhich of the metrics are most and least impacted by:\n\nSample size\nImbalanced data (i.e. proportion positive near 0 or 1)\nHigh noise\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#concept-check",
    "href": "slides/03-SLR-Gradient_Desc.html#concept-check",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Concept Check",
    "text": "Concept Check\nWhat do we call these:\n\n\\(\\mathbf{X}\\)\n\\(Y\\)\n\nWhat is our goal in supervised learning?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#concept-check-1",
    "href": "slides/03-SLR-Gradient_Desc.html#concept-check-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Concept Check",
    "text": "Concept Check\n\nFeatures: \\(\\mathbf{X}\\)\nTarget: \\(Y\\)\nGoal: Predict \\(Y\\) using \\(\\mathbf{X}\\)\n\n\n\nWhat do we call this process if \\(Y\\) is numerical? What about categorical?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#concept-check-2",
    "href": "slides/03-SLR-Gradient_Desc.html#concept-check-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat do we call this process if \\(Y\\) is numerical? What about categorical?\n\nNumerical: regression\nCategorical: classification\n\n\n\n\nWhat is the difference between a training and a test set?\nHow do we evaluate the performance of a regression model?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\n\nLabeled training data \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\)\n\ni.e. \\(n\\) training observations\n\nFit/train a model from training data\n\n\\(\\hat{y}=\\hat{f}(x)\\), regression\n\\(\\hat{y}=\\hat{C}(x)\\), classification\n\n\nObtain estimates \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)\\) (or, \\(\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)\\)) of training data\nCompute error:\n\nRegression \\[\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2\\]\nClassification \\[\n\\begin{aligned}\n\\text{Training Error Rate}\n&=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]\\\\\n&= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance-1",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\nIn general, not interested in performance on training data\nWant: performance on unseen test data… why?\nFresh test data: \\((x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \\ldots, (x_m^{test},y_m^{test})\\).\nCompute test error:\n\nRegression \\[\\text{Test MSE}=\\text{Average}_{Test} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\left(y_i^{test}-\\hat{f}(x_i^{test})\\right)^2\\]\nClassification \\[\\text{Test Error Rate}=\\text{Average}_{Test} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\ I\\left(y_i^{test} \\ne \\hat{C}(x_i^{test})\\right)\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Bias-Variance Trade-off",
    "text": "Supervised Learning: Bias-Variance Trade-off\n\nModel fit on training data \\(\\hat{f}(x)\\)\n“True” relationship: \\(Y=f(x)+\\epsilon\\)\n\\((x_0^{test}, y_0^{test})\\): test observation\nBias-Variance Trade-Off (Theoretical) \\[\\underbrace{E\\left(y_0^{test}-\\hat{f}(x_0^{test})\\right)^2}_{total \\ error}=\\underbrace{Var\\left(\\hat{f}(x_0^{test})\\right)}_{source \\ 1} + \\underbrace{\\left[Bias\\left(\\hat{f}(x_0^{test})\\right)\\right]^2}_{source \\ 2}+\\underbrace{Var(\\epsilon)}_{source \\ 3}\\] where \\(Bias\\left(\\hat{f}(x_0)\\right)=E\\left(\\hat{f}(x_0)\\right)-f(x_0)\\)\n\n\n\nQuestion: Where is \\(\\hat{y}_0^{test}\\)?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off-1",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Bias-Variance Trade-off",
    "text": "Supervised Learning: Bias-Variance Trade-off\n\nReducible Error:\n\nSource 1: how \\(\\hat{f}(x)\\) varies among different randomly selected possible training data (Variance)\nSource 2: how \\(\\hat{f}(x)\\) (when predicting the test data) differs from its target \\(f(x)\\) (Bias)\n\nIrreducible Error:\n\nSource 3: how \\(y\\) differs from “true” \\(f(x)\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFor now: focus on regression problems (ideas extend to classification)\nConsider: three different examples of simulated “toy” datasets and three types of models (\\(\\hat{f}_i(.)\\))\n\nLinear Regression orange\nSmoothing Spline 1 blue\nMore flexible Smoothing Spline 2 green\n\n“True” (simulated) function \\(f(.)\\) black\nTraining Error\nTest Error"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-1",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-2",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-3",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-3",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-4",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-4",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#a-familiar-supervised-learning-model",
    "href": "slides/03-SLR-Gradient_Desc.html#a-familiar-supervised-learning-model",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "A Familiar Supervised Learning Model",
    "text": "A Familiar Supervised Learning Model\n\nAssume relationship between \\(\\mathbf{X}\\) and \\(Y\\) is: \\[Y=f(\\mathbf{X}) + \\epsilon\\] where \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero.\nObjective: To approximate/estimate \\(f(\\mathbf{X})\\)\nLinear Regression: assume that \\(f(\\mathbf{X})\\) is a linear function of \\(\\mathbf{X}\\)\n\nFor \\(p=1\\): \\(f(\\mathbf{X}) = \\beta_0 + \\beta_1 X_1\\)\nFor \\(p &gt; 1\\): \\(f(\\mathbf{X}) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#linear-regression-1",
    "href": "slides/03-SLR-Gradient_Desc.html#linear-regression-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Linear Regression",
    "text": "Linear Regression\nSuppose the CEO of a restaurant franchise is considering opening new outlets in different cities. They would like to expand their business to cities that give them higher profits with the assumption that highly populated cities will probably yield higher profits.\nThey have data on the population (in 100,000) and profit (in $1,000) at 97 cities where they currently have outlets.\n\noutlets &lt;- readRDS(\"../data/outlets.rds\")   # load dataset\nhead(outlets) |&gt; kable() # first six observations of the dataset\n\n\n\n\npopulation\nprofit\n\n\n\n\n6.1101\n17.5920\n\n\n5.5277\n9.1302\n\n\n8.5186\n13.6620\n\n\n7.0032\n11.8540\n\n\n5.8598\n6.8233\n\n\n8.3829\n11.8860"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#linear-regression-2",
    "href": "slides/03-SLR-Gradient_Desc.html#linear-regression-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nObjective: choose “best” \\(\\beta_0\\) and \\(\\beta_1\\) if we assume \\[\\text{profit} = \\beta_0 + \\beta_1\\times\\text{population}\\]\nOnce this is done, we can:\n\npredict the profit for a new city with a given population\nunderstand the relationship between population and profit better"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#outlet-eda",
    "href": "slides/03-SLR-Gradient_Desc.html#outlet-eda",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Outlet EDA",
    "text": "Outlet EDA\n\noutlets |&gt; ggpairs() # ggpairs is from GGAlley package"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#linear-regression-estimating-parameters",
    "href": "slides/03-SLR-Gradient_Desc.html#linear-regression-estimating-parameters",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Linear Regression: Estimating Parameters",
    "text": "Linear Regression: Estimating Parameters\n\n\nSuppose we have \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\nObserved response: \\(y_i\\) for \\(i=1,\\ldots,n\\)\nPredicted response: \\(\\hat{y}_i\\) for \\(i=1, \\ldots, n\\)\nResidual: \\(e_i = \\hat{y}_i - y_i\\) for \\(i=1, \\ldots, n\\)\nMean Squared Error (MSE): \\(MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}\\) also known as the loss/cost function\nGOAL: Find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes \\(MSE\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nHow do we minimize the \\(MSE\\)?\n\nCan be done “analytically” but most ML models can’t be fit that way\n\nToday: popular optimization algorithm called gradient descent.\nNOTE: Gradient Descent is not a machine learning model/algorithm. It is an optimization technique that helps to fit machine learning models."
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-1",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nThink of the MSE as a function of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\n\n\\(\\text{MSE} = \\frac{\\sum (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1)^2}{n}\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-2",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nUpdates to the parameter: \\[\n\\begin{aligned}\n\\text{new value of parameters} &= \\text{old value of parameters}\\\\\n&\\qquad- \\text{step size} \\times \\text{gradient of function w.r.t. parameters}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-3",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-3",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-4",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-4",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\n\n\n\n\n\n\n\n\nStep size too small\n\n\n\n\n\n\n\nStep size too too big"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nObjective: We want to find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes\n\\[\\begin{aligned}\nMSE &= \\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}\\\\\n&= \\dfrac{(\\hat{y}_1 - y_1)^2 +  (\\hat{y}_2 - y_2)^2 + \\ldots + (\\hat{y}_n - y_n)^2}{n}\n\\end{aligned}\\]\n\\[MSE = \\dfrac{(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_1 - y_1)^2 +  (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_2 - y_2)^2 + \\ldots + (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_n - y_n)^2}{n}\\]\n\\[MSE = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i)^2 = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (e_i)^2\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-1",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nTo compute gradient, need partial derivatives of \\(MSE\\) with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\\[\\text{gradient of MSE with respect to} \\ \\hat{\\beta}_0 = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i) = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\\]\n\\[\\text{gradient of MSE with respect to} \\ \\hat{\\beta}_1 = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i) = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{y}_i - y_i)\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-2",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nGradient descent update:\n\nFor \\(\\hat{\\beta}_0\\): \\[\n\\begin{aligned}\n\\hat{\\beta}_0 \\ \\text{(new)}\n&= \\hat{\\beta}_0 \\ \\text{(old)}- \\bigg(\\text{step size} \\times \\text{derivative w.r.t} \\ \\hat{\\beta}_0\\bigg)\\\\\n&= \\hat{\\beta}_0 \\ \\text{(old)}- \\bigg(\\text{step size} \\times \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\\bigg)\n\\end{aligned}\n\\]\nFor \\(\\hat{\\beta}_1\\): \\[\n\\begin{aligned}\n\\hat{\\beta}_1  \\ \\text{(new)} &= \\hat{\\beta}_1  \\ \\text{(old)} - \\bigg(\\text{step size} \\times \\text{derivative w.r.t} \\ \\hat{\\beta}_1 \\bigg)\\\\\n&= \\hat{\\beta}_1  \\ \\text{(old)} - \\bigg(\\text{step size} \\times \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{y}_i - y_i)\\bigg)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/00-welcome.html#meet-prof.-friedlander",
    "href": "slides/00-welcome.html#meet-prof.-friedlander",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Meet Prof. Friedlander!",
    "text": "Meet Prof. Friedlander!\n\n\nEducation and career journey\n\nGrew up outside New York City\nBS in Math & Statistics from Rice University (Houston, TX)\nBusiness Analyst at Capital One (Plano, TX)\nMS and PhD in Statistics & Operations Research from UNC-Chapel Hill\nPostdoc in Population Genetics at University of Chicago\nAssistant Professor of Math at St. Norbert College (Green Bay, WI)\n\nWork focuses on statistics education, queuing theory, and population genetics\nBig sports fan: NY Knicks, Giants, Rangers, Yankees, UNC Tarheels\nDad of three cute dogs: Allie, Miriam, Tony"
  },
  {
    "objectID": "slides/00-welcome.html#meet-prof.-friedlander-1",
    "href": "slides/00-welcome.html#meet-prof.-friedlander-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Meet Prof. Friedlander!",
    "text": "Meet Prof. Friedlander!"
  },
  {
    "objectID": "slides/00-welcome.html#tell-me-about-yourself-github",
    "href": "slides/00-welcome.html#tell-me-about-yourself-github",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Tell me about yourself + GitHub",
    "text": "Tell me about yourself + GitHub\nCreate a GitHub account. You may want to use this to show-off work to future employers so I recommend using something professional (like your name) as your user name.\nSend me an email with answers to the following questions:\n\nWhat is the GitHub username you just created?\nWhat would you like me to call you?\nWhy are you taking this class?\nWhat are your career goals?\nIs there anything else you would like me to know about you? E.g. athlete, preferred pronouns, accommodations, etc…\nPlease recommend at least one and up to infinity songs for the class playlist."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq",
    "href": "slides/00-welcome.html#course-faq",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Familiarity with concepts from statistical inference, linear regression, and logistic regression. A solid grounding in R, including the tidyverse and ggplot."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-1",
    "href": "slides/00-welcome.html#course-faq-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R for analysis, Quarto for writing up results, and GitHub for version control and collaboration"
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-2",
    "href": "slides/00-welcome.html#course-faq-2",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Will we learn the mathematical theory?\nA - Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics occasionally."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-3",
    "href": "slides/00-welcome.html#course-faq-3",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What distinguishes this from a 300-level course?\nA - I expect a high level of independence from you. You should not be relying on me to teach you every small detail from this course. For example, if you tell you about an R function, I expect that you will be able to figure out how to use it yourself."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-4",
    "href": "slides/00-welcome.html#course-faq-4",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Is there anything else I should know?\nA - Machine learning is a RAPIDLY evolving field. If you want to be successful in this field going forward, you will need to be able to learn things for yourself and SELF-ASSESS whether you know them. There are portions of this course that I have intentionally designed to not give you enough information to solve on your own."
  },
  {
    "objectID": "slides/00-welcome.html#course-learning-objectives",
    "href": "slides/00-welcome.html#course-learning-objectives",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\ntackle predictive modeling problems arising from real data.\nuse R to fit and evaluate machine learning models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate results results through writing and oral presentations."
  },
  {
    "objectID": "slides/00-welcome.html#course-toolkit",
    "href": "slides/00-welcome.html#course-toolkit",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website\n\nCentral hub for the course!\nTour of the website\n\nCanvas\n\nGradebook\nAnnouncements\n\nGitHub\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/00-welcome.html#computing-toolkit",
    "href": "slides/00-welcome.html#computing-toolkit",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through your personal computer (preferred) or the RStudio Server (email me ASAP if you are doing this)\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in MAT 427 course classroom"
  },
  {
    "objectID": "slides/00-welcome.html#prepare-participate-practice-perform",
    "href": "slides/00-welcome.html#prepare-participate-practice-perform",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Prepare, Participate, Practice, Perform",
    "text": "Prepare, Participate, Practice, Perform\n\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with team-based homework graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nTwo Job Applications/Portfolios (individual)\nTwo Job Interviews (individual)\nOne Hack-a-thon/Presentation (individual-ish)\nOne Project & Presentation (team)"
  },
  {
    "objectID": "slides/00-welcome.html#grading",
    "href": "slides/00-welcome.html#grading",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n10%\n\n\nJob Application 1\n15%\n\n\nJob Application 2\n15%\n\n\nJob Interview 1\n15%\n\n\nJob Interview 2\n15%\n\n\nHack-a-thon & Presentation\n15%\n\n\nFinal Project\n15%\n\n\n\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/00-welcome.html#support",
    "href": "slides/00-welcome.html#support",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Support",
    "text": "Support\n\nAttend office hours\n\nProf. Friedlander office hours\n\nDedicated homework help session\nUse email for questions regarding personal matters and/or grades\nSee the Course Support page for more details"
  },
  {
    "objectID": "slides/00-welcome.html#late-homework",
    "href": "slides/00-welcome.html#late-homework",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Late Homework",
    "text": "Late Homework\n\nOne week late (no grade penalty)\nAfter I start grading (no feedback)\nWhy should I care about feedback?\n\nIt’s how you learn… duh\nYou will be repurposing your homeworks for your job applications"
  },
  {
    "objectID": "slides/00-welcome.html#school-sponsored-events",
    "href": "slides/00-welcome.html#school-sponsored-events",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "School-Sponsored Events",
    "text": "School-Sponsored Events\n\nExcused absences for event? Email me at least a week in advance\nSick or injured? Email me as soon as it is safe to do so.\n\nDon’t get me sick…\n\nFailure to adhere to this policy gets you a 35% point reduction"
  },
  {
    "objectID": "slides/00-welcome.html#academic-integrity",
    "href": "slides/00-welcome.html#academic-integrity",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code."
  },
  {
    "objectID": "slides/00-welcome.html#collaboration-sharing-code",
    "href": "slides/00-welcome.html#collaboration-sharing-code",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nI have policies!"
  },
  {
    "objectID": "slides/00-welcome.html#use-of-artificial-intelligence-ai",
    "href": "slides/00-welcome.html#use-of-artificial-intelligence-ai",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nYou should treat AI tools, such as ChatGPT, the same as other online resources.\nThere are two guiding principles that govern how you can use AI in this course:1\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎"
  },
  {
    "objectID": "slides/00-welcome.html#use-of-artificial-intelligence-ai-1",
    "href": "slides/00-welcome.html#use-of-artificial-intelligence-ai-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nUnderstand everything you write down\nTell me where you got it from\nDon’t lie about it\n\n\n\n\n\n\n\nImportant\n\n\nIn general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content. Any code or content from your homework is eligible for inclusion during your job interview."
  },
  {
    "objectID": "slides/00-welcome.html#in-class-agreements",
    "href": "slides/00-welcome.html#in-class-agreements",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "In class agreements",
    "text": "In class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g. “you may turn in your homework late due to a sporting event”), you MUST send me a follow-up message. If you don’t, I will almost certainly forget, and our agreement will be considered null and void."
  },
  {
    "objectID": "slides/00-welcome.html#five-tips-for-success",
    "href": "slides/00-welcome.html#five-tips-for-success",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions, come to office hours and help session.\nDo the homework; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/00-welcome.html#emails-for-help",
    "href": "slides/00-welcome.html#emails-for-help",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Emails for help",
    "text": "Emails for help\nIf you email me about an error please include a screenshot of the error and the code causing the error."
  },
  {
    "objectID": "slides/05-knn.html#computational-setup",
    "href": "slides/05-knn.html#computational-setup",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Computational Setup",
    "text": "Computational Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(readODS)\nlibrary(modeldata) # contains ames dataset\n\ntidymodels_prefer()\n\nmlr_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")"
  },
  {
    "objectID": "slides/05-knn.html#comparing-models-data-splitting",
    "href": "slides/05-knn.html#comparing-models-data-splitting",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Comparing Models: Data Splitting",
    "text": "Comparing Models: Data Splitting\n\nSplit ames data set into two parts\n\nTraining set: randomly selected proportion \\(p\\) (typically 50-90%) of data used for fitting model\nTest set: randomly selected proportion \\(1-p\\) of data used for estimating prediction error\n\nIf comparing A LOT of models, split into three parts to prevent information leakage\n\nTraining set: randomly selected proportion \\(p\\) (typically 50-90%) of data used for fitting model\nValidation set: randomly selected proportion \\(q\\) (typically 20-30%) of data used to choosing tuning parameters\nTest set: randomly selected proportion \\(1-p-q\\) of data used for estimating prediction error\n\nIdea: use data your model hasn’t seen to get more accurate estimate of error and prevent overfitting"
  },
  {
    "objectID": "slides/05-knn.html#comparing-models-data-splitting-with-tidymodels",
    "href": "slides/05-knn.html#comparing-models-data-splitting-with-tidymodels",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Comparing Models: Data Splitting with tidymodels",
    "text": "Comparing Models: Data Splitting with tidymodels\n\nset.seed(427) # Why?\n\names_split &lt;- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2049/881/2930&gt;\n\names_train &lt;- training(ames_split) # get training data\names_test &lt;- testing(ames_split) # get test data\n\n\nstrata not necessary but good practice\n\nstrata will use stratified sampling on the variable you specify (very little downside)"
  },
  {
    "objectID": "slides/05-knn.html#linear-regression-comparing-models",
    "href": "slides/05-knn.html#linear-regression-comparing-models",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Linear Regression: Comparing Models",
    "text": "Linear Regression: Comparing Models\n\nLet’s create three models with Sale_Price as the response:\n\nfit1: a linear regression model with Bedroom_AbvGr as the only predictor\nfit2: a linear regression model with Gr_Liv_Area as the only predictor\nfit3 (similar to model in previous slides): a multiple regression model with Gr_Liv_Area and Bedroom_AbvGr as predictors\nfit4: super flexible model which fits a 10th degree polynomial to Gr_Liv_Area and a 2nd degree polynomial to Bedroom_AbvGr\n\n\n\nfit1 &lt;- mlr_model |&gt; fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set\nfit2 &lt;- mlr_model |&gt; fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)\nfit3 &lt;- mlr_model |&gt; fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)\nfit4 &lt;- mlr_model |&gt; fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)"
  },
  {
    "objectID": "slides/05-knn.html#computing-mse",
    "href": "slides/05-knn.html#computing-mse",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Computing MSE",
    "text": "Computing MSE\n\n# Fit 1\nfit1_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)\nfit1_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)\n\n# Fit 2\nfit2_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)\nfit2_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit3_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)\nfit3_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit4_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)\nfit4_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)"
  },
  {
    "objectID": "slides/05-knn.html#question",
    "href": "slides/05-knn.html#question",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Question",
    "text": "Question\nWithout looking at the numbers\n\nDo we know which of the following is the smallest: fit1_train_mse, fit2_train_mse, fit3_train_mse, fit4_train_mse? Yes, fit4_train_mse\nDo we know which of the following is the smallest: fit1_test_mse, fit2_test_mse, fit3_test_mse, fit4_test_mse? No"
  },
  {
    "objectID": "slides/05-knn.html#choosing-a-model",
    "href": "slides/05-knn.html#choosing-a-model",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Choosing a Model",
    "text": "Choosing a Model\n\n\n\n# Training Errors\nc(fit1_train_mse, fit2_train_mse, \n  fit3_train_mse, fit4_train_mse)\n\n[1] 6213135279 3188099910 2781293767 2472424544\n\nwhich.min(c(fit1_train_mse, fit2_train_mse, \n            fit3_train_mse, fit4_train_mse))\n\n[1] 4\n\n# test Errors\nc(fit1_test_mse, fit2_test_mse, \n  fit3_test_mse, fit4_test_mse)\n\n[1] 6.329031e+09 3.203895e+09 2.732389e+09 2.726084e+12\n\nwhich.min(c(fit1_test_mse, fit2_test_mse, \n            fit3_test_mse, fit4_test_mse))\n\n[1] 3\n\n\n\n\nfit4 has the lowest training MSE (to be expected)\nfit3 has the lowest test MSE\n\nWe would choose fit3\n\nAnything else interesting we see?"
  },
  {
    "objectID": "slides/05-knn.html#regression-conditional-averaging",
    "href": "slides/05-knn.html#regression-conditional-averaging",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Regression: Conditional Averaging",
    "text": "Regression: Conditional Averaging\nRestaurant Outlets Profit dataset\n\nWhat is a good value of \\(\\hat{f}(x)\\) (expected profit), say at \\(x=6\\)?\nA possible choice is the average of the observed responses at \\(x=6\\). But we may not observe responses for certain \\(x\\) values."
  },
  {
    "objectID": "slides/05-knn.html#k-nearest-neighbors-knn-regression",
    "href": "slides/05-knn.html#k-nearest-neighbors-knn-regression",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "K-Nearest Neighbors (KNN) Regression",
    "text": "K-Nearest Neighbors (KNN) Regression\n\nNon-parametric approach\nFormally: Given a value for \\(K\\) and a test data point \\(x_0\\), \\[\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) \\] where \\(\\mathcal{N}_0\\) is the set of the \\(K\\) training observations closest to \\(x_0\\).\nInformally, average together the \\(K\\) “closest” observations in your training set\n“Closeness”: usually use the Euclidean metric to measure distance\nEuclidean distance between \\(\\mathbf{X}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})\\) and \\(\\mathbf{x}_j=(x_{j1}, x_{j2}, \\ldots, x_{jp})\\): \\[||\\mathbf{x}_i-\\mathbf{x}_j||_2 = \\sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \\ldots + (x_{ip}-x_{jp    })^2}\\]"
  },
  {
    "objectID": "slides/05-knn.html#knn-regression-single-predictor-fit",
    "href": "slides/05-knn.html#knn-regression-single-predictor-fit",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "KNN Regression (single predictor): Fit",
    "text": "KNN Regression (single predictor): Fit\n\n\n\\(K=1\\)\n\nknnfit1 &lt;- nearest_neighbor(neighbors = 1) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\") |&gt; \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit1, new_data = tibble(population = 6)) |&gt; kable()  # 1-nn prediction\n\n\n\n\n.pred\n\n\n\n\n0.92695\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(K=5\\)\n\nknnfit5 &lt;- nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\") |&gt; \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit5, new_data = tibble(population = 6)) |&gt; kable()  # 1-nn prediction\n\n\n\n\n.pred\n\n\n\n\n4.113736"
  },
  {
    "objectID": "slides/05-knn.html#regression-methods-comparison",
    "href": "slides/05-knn.html#regression-methods-comparison",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Regression Methods: Comparison",
    "text": "Regression Methods: Comparison"
  },
  {
    "objectID": "slides/05-knn.html#question-1",
    "href": "slides/05-knn.html#question-1",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Question!!!",
    "text": "Question!!!\nAs \\(K\\) in KNN regression increases:\n\nthe flexibility of the fit (decreases /increases)\nthe bias of the fit (decreases/increases )\nthe variance of the fit (decreases/increases)"
  },
  {
    "objectID": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors",
    "href": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "K-Nearest Neighbors Regression (multiple predictors)",
    "text": "K-Nearest Neighbors Regression (multiple predictors)\n\nLet’s look at the ames data\n\n\names |&gt;\n  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |&gt;\n  head() |&gt; \n  kable()\n\n\n\n\nSale_Price\nGr_Liv_Area\nBedroom_AbvGr\n\n\n\n\n215000\n1656\n3\n\n\n105000\n896\n2\n\n\n172000\n1329\n3\n\n\n244000\n2110\n3\n\n\n189900\n1629\n3\n\n\n195500\n1604\n3\n\n\n\n\n\n\n\n\nShould 1 square foot count the same as 1 bedroom?\nNeed to center and scale (freq. just say scale)\n\nsubtract mean from each predictor\ndivide by standard deviation of each predictor\ncompares apples-to-apples"
  },
  {
    "objectID": "slides/05-knn.html#scaling-in-r",
    "href": "slides/05-knn.html#scaling-in-r",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Scaling in R",
    "text": "Scaling in R\n\n# scale predictors\names_scaled &lt;- tibble(size_scaled = scale(ames$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),\n                                  price = ames$Sale_Price)\n\nhead(ames_scaled) |&gt; kable()  # first six observations\n\n\n\n\nsize_scaled\nnum_bedrooms_scaled\nprice\n\n\n\n\n0.3092123\n0.1760642\n215000\n\n\n-1.1942232\n-1.0320576\n105000\n\n\n-0.3376606\n0.1760642\n172000\n\n\n1.2073172\n0.1760642\n244000\n\n\n0.2558008\n0.1760642\n189900\n\n\n0.2063456\n0.1760642\n195500"
  },
  {
    "objectID": "slides/05-knn.html#question-2",
    "href": "slides/05-knn.html#question-2",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Question…",
    "text": "Question…\n\n\nWhat about the training and test sets?\nNeed to scale BOTH sets based on the mean and standard deviation of the training set…\nDiscussion: Why?\nDiscussion: Why don’t I need to center and scale Sale_Price?"
  },
  {
    "objectID": "slides/05-knn.html#scaling-revisited",
    "href": "slides/05-knn.html#scaling-revisited",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Scaling Revisited",
    "text": "Scaling Revisited\n\names_train_scaled &lt;- tibble(size_scaled = scale(ames_train$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames_train$Bedroom_AbvGr),\n                                  price = ames_train$Sale_Price)\n\names_test_scaled &lt;- tibble(size_scaled = (ames_test$Gr_Liv_Area - mean(ames_train$Gr_Liv_Area)/sd(ames_train$Gr_Liv_Area)),\n                                  num_bedrooms_scaled = (ames_test$Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr),\n                                  price = ames_test$Sale_Price)\n\n\nNext time: using recipe’s in tidymodels to simplify this process"
  },
  {
    "objectID": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors-1",
    "href": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors-1",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "K-Nearest Neighbors Regression (multiple predictors)",
    "text": "K-Nearest Neighbors Regression (multiple predictors)\n\nknnfit10 &lt;- nearest_neighbor(neighbors = 10) |&gt;   # 10-nn regression\n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\") |&gt; \n  fit(price ~ size_scaled + num_bedrooms_scaled, data = ames_train_scaled)\n\n\nTest Point: Gr_Liv_area = 2000 square feet, and Bedroom_AbvGr = 3, then\n\n\n# obtain 10-nn prediction\n\npredict(knnfit10, new_data = tibble(size_scaled = (2000 - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area),\n                                     num_bedrooms_scaled = (3 - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr)))\n\n# A tibble: 1 × 1\n   .pred\n   &lt;dbl&gt;\n1 256380\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#announcements",
    "href": "slides/21-decision-trees-3.html#announcements",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Announcements",
    "text": "Announcements\n\nJob Application Discussion\nJob Interviews\nAdmissions Data Project"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#job-application-discussion",
    "href": "slides/21-decision-trees-3.html#job-application-discussion",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Job Application Discussion",
    "text": "Job Application Discussion\n\n\nWe missed the mark a bit.\nResumes and CVs were good. Sample analysis were… not.\nBiggest issue: professionalism and editing.\nIf the majority of your analysis was code, you probably got a bad grade.\nNext time:\n\nExplain everything that you are doing and justify it!\nProofread your document!\nMake it look nice!"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#job-application-discussion-1",
    "href": "slides/21-decision-trees-3.html#job-application-discussion-1",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Job Application Discussion",
    "text": "Job Application Discussion\n\n\nSome common themes:\n\nY’all love the word “hone”\nY’all love the phrase “actionable insights”… so do I… but still\nI think some of you are overselling yourselves\nIn cover letter, use fewer “buzz-words” and include more substance"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#job-interviews",
    "href": "slides/21-decision-trees-3.html#job-interviews",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Job Interviews",
    "text": "Job Interviews\n\nLink to Resources\nTwo Rounds\n\nFirst: Screening interview with Dani (Schedule by Wednesday)\n\nDue next Friday, April 11th\n\nSecond: Technical interview with me (Due last day of class)\n\nNote that we’ll be doing a lot of stuff between now and then so try to get this done sooner rather than later"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#project",
    "href": "slides/21-decision-trees-3.html#project",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Project",
    "text": "Project\n\nProject Instructions\nBrian Bava visiting on Friday to discuss\nDO ASAP: Sign data agreement\nDo by Wednesday: Load the data and review instructions. We will have a discussion about cleaning the data on Wednesday. You are all expected to contribute.\nDo by Friday: Explore the data. Your team is expected to bring at least three substantial questions for Brian."
  },
  {
    "objectID": "slides/21-decision-trees-3.html#computational-set-up",
    "href": "slides/21-decision-trees-3.html#computational-set-up",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#last-time",
    "href": "slides/21-decision-trees-3.html#last-time",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Last Time",
    "text": "Last Time\n\nRegression Trees in R\nWarm-up question: What is cost complexity tuning?"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#data-dcbikeshare",
    "href": "slides/21-decision-trees-3.html#data-dcbikeshare",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Data: dcbikeshare",
    "text": "Data: dcbikeshare\nBike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. As of May 2018, there are about over 1600 bike-sharing programs around the world, providing more than 18 million bicycles for public use. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. Documentation\n\nglimpse(dcbikeshare)\n\nRows: 731\nColumns: 16\n$ instant    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dteday     &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05…\n$ season     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ yr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mnth       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ holiday    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ weekday    &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,…\n$ workingday &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,…\n$ weathersit &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ temp       &lt;dbl&gt; 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.20…\n$ atemp      &lt;dbl&gt; 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.23…\n$ hum        &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261,…\n$ windspeed  &lt;dbl&gt; 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.08…\n$ casual     &lt;dbl&gt; 331, 131, 120, 108, 82, 88, 148, 68, 54, 41, 43, 25, 38, 54…\n$ registered &lt;dbl&gt; 654, 670, 1229, 1454, 1518, 1518, 1362, 891, 768, 1280, 122…\n$ cnt        &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 126…"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#cleaning-the-data",
    "href": "slides/21-decision-trees-3.html#cleaning-the-data",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\n\ndcbikeshare_clean &lt;- dcbikeshare |&gt; \n  select(-instant, -dteday, -casual, -registered, -yr) |&gt; \n  mutate(\n    season = as_factor(case_when(\n      season == 1 ~ \"winter\",\n      season == 2 ~ \"spring\",\n      season == 3 ~ \"summer\",\n      season == 4 ~ \"fall\"\n    )),\n    mnth = as_factor(mnth),\n    weekday = as_factor(weekday),\n    weathersit = as_factor(weathersit)\n  )"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#split-the-data",
    "href": "slides/21-decision-trees-3.html#split-the-data",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Split the Data",
    "text": "Split the Data\n\nset.seed(427)\n\nbike_split &lt;- initial_split(dcbikeshare_clean, prop = 0.7, strata = cnt)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#recipe",
    "href": "slides/21-decision-trees-3.html#recipe",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Recipe",
    "text": "Recipe\n\nbike_recipe &lt;- recipe(cnt ~ ., data = bike_train) |&gt;   # set up recipe\n  step_integer(season, mnth, weekday) |&gt;   # numeric conversion of levels of the predictors\n  step_dummy(all_nominal(), one_hot = TRUE)  # one-hot/dummy encode nominal categorical predictors"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#define-model-workflow",
    "href": "slides/21-decision-trees-3.html#define-model-workflow",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Define Model Workflow",
    "text": "Define Model Workflow\n\ndec_tree_lowcc &lt;- decision_tree(cost_complexity = 10^(-4)) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\ndec_tree_highcc &lt;- decision_tree(cost_complexity = 0.1) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#visualize",
    "href": "slides/21-decision-trees-3.html#visualize",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize\n\nlibrary(rpart.plot)\nworkflow() |&gt; \n  add_recipe(bike_recipe) |&gt; \n  add_model(dec_tree_lowcc) |&gt; \n  fit(bike_train) |&gt;\n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#visualize-1",
    "href": "slides/21-decision-trees-3.html#visualize-1",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#visualize-2",
    "href": "slides/21-decision-trees-3.html#visualize-2",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize\n\nlibrary(rpart.plot)\nworkflow() |&gt; \n  add_recipe(bike_recipe) |&gt; \n  add_model(dec_tree_highcc) |&gt; \n  fit(bike_train) |&gt;\n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#visualize-3",
    "href": "slides/21-decision-trees-3.html#visualize-3",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#defin-model-workflow-with-tuning",
    "href": "slides/21-decision-trees-3.html#defin-model-workflow-with-tuning",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Defin Model Workflow with Tuning",
    "text": "Defin Model Workflow with Tuning\n\ndec_tree &lt;- decision_tree(cost_complexity = tune()) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\ndt_wf &lt;- workflow() |&gt; \n  add_recipe(bike_recipe) |&gt; \n  add_model(dec_tree)"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#define-folds-and-tuning-grid",
    "href": "slides/21-decision-trees-3.html#define-folds-and-tuning-grid",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Define Folds and Tuning Grid",
    "text": "Define Folds and Tuning Grid\n\nbike_folds &lt;- vfold_cv(bike_train, v = 5, repeats = 10)\n\ncp_grid &lt;- grid_regular(cost_complexity(range = c(-4, -1)), # I had to play around with these \n                             levels = 20)"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#tuning-cp",
    "href": "slides/21-decision-trees-3.html#tuning-cp",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Tuning CP",
    "text": "Tuning CP\n\ntuning_cp_results &lt;- tune_grid(\n  dt_wf,\n  resamples= bike_folds,\n  grid = cp_grid\n)"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#plot-results",
    "href": "slides/21-decision-trees-3.html#plot-results",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Plot Results",
    "text": "Plot Results\n\nautoplot(tuning_cp_results)"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#select-best-trees",
    "href": "slides/21-decision-trees-3.html#select-best-trees",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Select Best Trees",
    "text": "Select Best Trees\n\n\n\nbest_tree &lt;- select_best(tuning_cp_results)\nbest_tree |&gt; kable()\n\n\n\n\ncost_complexity\n.config\n\n\n\n\n0.0054556\nPreprocessor1_Model12\n\n\n\n\n\n\n\nose_tree &lt;- select_by_one_std_err(tuning_cp_results, desc(cost_complexity))\nose_tree |&gt; kable()\n\n\n\n\ncost_complexity\n.config\n\n\n\n\n0.0078476\nPreprocessor1_Model13"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#fit-best-tree",
    "href": "slides/21-decision-trees-3.html#fit-best-tree",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Fit Best Tree",
    "text": "Fit Best Tree\n\nbest_tree &lt;- select_best(tuning_cp_results)\nbest_wf &lt;- finalize_workflow(dt_wf, best_tree)\nbest_model &lt;- best_wf |&gt; fit(bike_train)\nbest_model |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#fit-ose-tree",
    "href": "slides/21-decision-trees-3.html#fit-ose-tree",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Fit OSE Tree",
    "text": "Fit OSE Tree\n\nose_tree &lt;- select_best(tuning_cp_results)\nose_wf &lt;- finalize_workflow(dt_wf, ose_tree)\nose_model &lt;- ose_wf |&gt; fit(bike_train)\nose_model |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#questions",
    "href": "slides/21-decision-trees-3.html#questions",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Questions",
    "text": "Questions\n\nWhy are both models the same but have different RMSE estimates from CV?\nWhat’s the difference between encoding mnth as an ordinal variable vs. a one-hot encoding?"
  },
  {
    "objectID": "slides/21-decision-trees-3.html#decision-trees",
    "href": "slides/21-decision-trees-3.html#decision-trees",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nAdvantages\n\nEasy to explain and interpret\nClosely mirror human decision-making\nCan be displayed graphically, and are easily interpreted by non-experts\nDoes not require standardization of predictors\nCan handle missing data directly\nCan easily capture non-linear patterns\n\nDisadvantages\n\nDo not have same level of prediction accuracy\nNot very robust\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/17-regularization.html#computational-set-up",
    "href": "slides/17-regularization.html#computational-set-up",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(fivethirtyeight) # for candy rankings data\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/17-regularization.html#data-candy",
    "href": "slides/17-regularization.html#data-candy",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data for this lecture comes from the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy match-ups (e.g. Reese’s vs. Skittles). Click here to check out some of the match ups.\nThe data set contains 12 characteristics and win percentage from 85 candies in the experiment."
  },
  {
    "objectID": "slides/17-regularization.html#data-candy-1",
    "href": "slides/17-regularization.html#data-candy-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data: Candy",
    "text": "Data: Candy\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "slides/17-regularization.html#data-cleaning",
    "href": "slides/17-regularization.html#data-cleaning",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\ncandy_rankings_clean &lt;- candy_rankings |&gt; \n  select(-competitorname) |&gt; \n  mutate(sugarpercent = sugarpercent*100, # convert proportions into percentages\n         pricepercent = pricepercent*100, # convert proportions into percentages\n         across(where(is.logical), ~ factor(.x, levels = c(\"FALSE\", \"TRUE\")))) # convert logicals into factors"
  },
  {
    "objectID": "slides/17-regularization.html#data-cleaning-1",
    "href": "slides/17-regularization.html#data-cleaning-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nglimpse(candy_rankings_clean)\n\nRows: 85\nColumns: 12\n$ chocolate        &lt;fct&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;fct&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;fct&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 73.2, 60.4, 1.1, 1.1, 90.6, 46.5, 60.4, 31.3, 90.6, 6…\n$ pricepercent     &lt;dbl&gt; 86.0, 51.1, 11.6, 51.1, 51.1, 76.7, 76.7, 51.1, 32.5,…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "slides/17-regularization.html#data-splitting",
    "href": "slides/17-regularization.html#data-splitting",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data Splitting",
    "text": "Data Splitting\n\ncandy_split &lt;- initial_split(candy_rankings_clean, strata = winpercent)\ncandy_train &lt;- training(candy_split)\ncandy_test &lt;- testing(candy_split)"
  },
  {
    "objectID": "slides/17-regularization.html#question",
    "href": "slides/17-regularization.html#question",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Question",
    "text": "Question\n\nWhat criteria do we use to fit a linear regression model? Write down an expression with \\(\\hat{\\beta_i}\\)’s, \\(x_{ij}\\)’s, and \\(y_j\\)’s in it."
  },
  {
    "objectID": "slides/17-regularization.html#ols",
    "href": "slides/17-regularization.html#ols",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "OLS",
    "text": "OLS\n\nOrdinary Least Squares Regression:\n\n\\[\n\\begin{aligned}\n\\hat{\\beta} =\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}SSE(\\hat{\\beta}) &= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{y}_j)^2\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2\n\\end{aligned}\n\\]\n\n\\(\\hat{\\beta} = (\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p)\\) is the vector of all my coefficients\n\\(\\operatorname{argmin}\\) is a function (operator) that returns the arguments that minimize the quantity it’s being applied to"
  },
  {
    "objectID": "slides/17-regularization.html#ridge-regression",
    "href": "slides/17-regularization.html#ridge-regression",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\\[\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_2^2\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\n\\end{aligned}\n\\]\n\n\\(\\lambda\\) is called a “tuning parameter” and is not estimated from the data (more on this later)\nIdea: penalize large coefficients\nIf we penalize large coefficients, what’s going to happen to to our estimated coefficients? THEY SHRINK!\n\\(\\|\\cdot\\|_2\\) is called the \\(L_2\\)-norm"
  },
  {
    "objectID": "slides/17-regularization.html#but-but-but-why",
    "href": "slides/17-regularization.html#but-but-but-why",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "But… but… but why?!?",
    "text": "But… but… but why?!?\n\n\nRecall the Bias-Variance Trade-Off\nOur reducible error can partitioned into:\n\nBias: how much \\(\\hat{f}\\) misses \\(f\\) by on average\nVariance: how much \\(\\hat{f}\\) moves around from sample to sample\n\nRidge: increase bias a little bit in exchange for large decrease in variance\nAs we increase \\(\\lambda\\) do we increase or decrease the penalty for large coefficients?\nAs we increase \\(\\lambda\\) do we increase or decrease the flexibility of our model?"
  },
  {
    "objectID": "slides/17-regularization.html#lasso",
    "href": "slides/17-regularization.html#lasso",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "LASSO",
    "text": "LASSO\n\\[\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_1\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\n\\end{aligned}\n\\]\n\nLASSO: Least Absolute Shrinkage and Selection Operator\n\\(\\lambda\\) is called a “tuning parameter” and is not estimated from the data (more on this later)\nIdea: penalize large coefficients\nIf we penalize large coefficients, what’s going to happen to to our estimated coefficients THEY SHRINK!\n\\(\\|\\cdot\\|_1\\) is called the \\(L_1\\)-norm"
  },
  {
    "objectID": "slides/17-regularization.html#question-1",
    "href": "slides/17-regularization.html#question-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Question",
    "text": "Question\n\nWhat should happen to our coefficients as we increase \\(\\lambda\\)?"
  },
  {
    "objectID": "slides/17-regularization.html#ridge-vs.-lasso",
    "href": "slides/17-regularization.html#ridge-vs.-lasso",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Ridge vs. LASSO",
    "text": "Ridge vs. LASSO\n\n\nRidge has a closed-form solution… how might we calculate it?\nRidge has some nice linear algebra properties that makes it EXTREMELY FAST to fit\nLASSO has no closed-form solution… why?\nLASSO coefficients estimated numerically… how?\n\nGradient descent works (kind of) but something called coordinate descent is typically better\n\nMOST IMPORTANT PROPERTY OF LASSO: it induces sparsity while Ridge does not"
  },
  {
    "objectID": "slides/17-regularization.html#sparsity-in-applied-math",
    "href": "slides/17-regularization.html#sparsity-in-applied-math",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Sparsity in Applied Math",
    "text": "Sparsity in Applied Math\n\nsparse typically means “most things are zero”\nExample: sparse matrices are matrices where most entries are zero\n\nfor large matrices this can provide HUGE performance gains\n\nLASSO induces sparsity by setting most of the parameter estimates to zero\n\nthis means it fits the model and does feature selection SIMULTANEOUSLY\n\nLet’s do some board work to see why this is…"
  },
  {
    "objectID": "slides/17-regularization.html#lasso-and-ridge-in-r",
    "href": "slides/17-regularization.html#lasso-and-ridge-in-r",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "LASSO and Ridge in R",
    "text": "LASSO and Ridge in R\n\nols &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\nridge_0 &lt;- linear_reg(mixture = 0, penalty = 0) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_1 &lt;- linear_reg(mixture = 0, penalty = 1) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_10 &lt;- linear_reg(mixture = 0, penalty = 10) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_0 &lt;- linear_reg(mixture = 1, penalty = 0) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_1 &lt;- linear_reg(mixture = 1, penalty = 1) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_10 &lt;- linear_reg(mixture = 1, penalty = 10) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/17-regularization.html#create-recipe",
    "href": "slides/17-regularization.html#create-recipe",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Create Recipe",
    "text": "Create Recipe\n\nlm_preproc &lt;- recipe(winpercent ~ . , data = candy_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/17-regularization.html#question-2",
    "href": "slides/17-regularization.html#question-2",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Question",
    "text": "Question\n\nWhy do we need to normalize our data?"
  },
  {
    "objectID": "slides/17-regularization.html#create-workflows",
    "href": "slides/17-regularization.html#create-workflows",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Create workflows",
    "text": "Create workflows\n\ngeneric_wf &lt;- workflow() |&gt; add_recipe(lm_preproc)\nols_wf &lt;- generic_wf |&gt; add_model(ols)\nridge0_wf &lt;- generic_wf |&gt; add_model(ridge_0)\nridge1_wf &lt;- generic_wf |&gt; add_model(ridge_1)\nridge10_wf &lt;- generic_wf |&gt; add_model(ridge_10)\nlasso0_wf &lt;- generic_wf |&gt; add_model(lasso_0)\nlasso1_wf &lt;- generic_wf |&gt; add_model(lasso_1)\nlasso10_wf &lt;- generic_wf |&gt; add_model(lasso_10)"
  },
  {
    "objectID": "slides/17-regularization.html#fit-models",
    "href": "slides/17-regularization.html#fit-models",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Fit Models",
    "text": "Fit Models\n\nols_fit &lt;- ols_wf |&gt; fit(candy_train)\nridge0_fit &lt;- ridge0_wf |&gt; fit(candy_train)\nridge1_fit &lt;- ridge1_wf |&gt; fit(candy_train)\nridge10_fit &lt;- ridge10_wf |&gt; fit(candy_train)\nlasso0_fit &lt;- lasso0_wf |&gt; fit(candy_train)\nlasso1_fit &lt;- lasso1_wf |&gt; fit(candy_train)\nlasso10_fit &lt;- lasso10_wf |&gt; fit(candy_train)"
  },
  {
    "objectID": "slides/17-regularization.html#collect-coefficients",
    "href": "slides/17-regularization.html#collect-coefficients",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Collect coefficients",
    "text": "Collect coefficients\n\nall_coefs &lt;- bind_cols(model = \"ols\", tidy(ols_fit)) |&gt; \n  bind_rows(bind_cols(model = \"ridge0\", tidy(ridge0_fit))) |&gt; \n  bind_rows(bind_cols(model = \"ridge1\", tidy(ridge1_fit))) |&gt; \n  bind_rows(bind_cols(model = \"ridge10\", tidy(ridge10_fit))) |&gt; \n  bind_rows(bind_cols(model = \"lasso0\", tidy(lasso0_fit))) |&gt; \n  bind_rows(bind_cols(model = \"lasso1\", tidy(lasso1_fit))) |&gt; \n  bind_rows(bind_cols(model = \"lasso10\", tidy(lasso10_fit)))"
  },
  {
    "objectID": "slides/17-regularization.html#question-3",
    "href": "slides/17-regularization.html#question-3",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Question",
    "text": "Question\n\n\nWhat should we expect from ols, ridge0, and lasso0?\nThey should be the same!"
  },
  {
    "objectID": "slides/17-regularization.html#visualize",
    "href": "slides/17-regularization.html#visualize",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Visualize",
    "text": "Visualize\n\nall_coefs |&gt; \n  filter(model %in% c(\"ols\", \"ridge0\", \"lasso0\")) |&gt; \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")"
  },
  {
    "objectID": "slides/17-regularization.html#uh-oh",
    "href": "slides/17-regularization.html#uh-oh",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Uh-Oh!",
    "text": "Uh-Oh!\n\n\nThey’re not the same\nAny idea why?\nAlgorithm used to fit model\n\nlm estimates coefficients analytically\nglmnet estimates coefficients numerically using an algorithm named “coordinate-descent”\n\nMoral: you must understand theory AND application"
  },
  {
    "objectID": "slides/17-regularization.html#visualizing-coefficients-ridge",
    "href": "slides/17-regularization.html#visualizing-coefficients-ridge",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Visualizing Coefficients: Ridge",
    "text": "Visualizing Coefficients: Ridge\n\nall_coefs |&gt; \n  filter(model %in% c(\"ols\", \"ridge1\", \"ridge10\")) |&gt; \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")"
  },
  {
    "objectID": "slides/17-regularization.html#visualizing-coefficients-lasso",
    "href": "slides/17-regularization.html#visualizing-coefficients-lasso",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Visualizing Coefficients: LASSO",
    "text": "Visualizing Coefficients: LASSO\n\nall_coefs |&gt; \n  filter(model %in% c(\"ols\", \"lasso1\", \"lasso10\")) |&gt; \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")"
  },
  {
    "objectID": "slides/17-regularization.html#ridge-coefficients-vs.-penalty-lambda",
    "href": "slides/17-regularization.html#ridge-coefficients-vs.-penalty-lambda",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Ridge Coefficients vs. Penalty ( \\(\\lambda\\) )",
    "text": "Ridge Coefficients vs. Penalty ( \\(\\lambda\\) )\n\nridge0_fit |&gt; extract_fit_engine() |&gt; autoplot()"
  },
  {
    "objectID": "slides/17-regularization.html#lasso-coefficients-vs.-penalty-lambda",
    "href": "slides/17-regularization.html#lasso-coefficients-vs.-penalty-lambda",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "LASSO Coefficients vs. Penalty ( \\(\\lambda\\) )",
    "text": "LASSO Coefficients vs. Penalty ( \\(\\lambda\\) )\n\nlasso0_fit |&gt; extract_fit_engine() |&gt; autoplot()"
  },
  {
    "objectID": "slides/17-regularization.html#why-should-i-learn-math-for-data-science",
    "href": "slides/17-regularization.html#why-should-i-learn-math-for-data-science",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Why should I learn math for data science?",
    "text": "Why should I learn math for data science?\n\nToday:\n\nOrdinary Least Squares: Linear algebra, Calc 3\nLASSO: Calc 3, Geometry, Numerical Analysis\nRidge: Linear Algebra, Calc 3\nBias-Variance Trade-Off: Probability"
  },
  {
    "objectID": "slides/17-regularization.html#question-4",
    "href": "slides/17-regularization.html#question-4",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Question",
    "text": "Question\n\nHow do you think we should choose \\(\\lambda\\)?\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#computational-set-up",
    "href": "slides/12-preproc-missing-cv.html#computational-set-up",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\nlibrary(readODS)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#repeated-k-fold-cross-validation-cv",
    "href": "slides/12-preproc-missing-cv.html#repeated-k-fold-cross-validation-cv",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Repeated K-Fold Cross-Validation (CV)",
    "text": "Repeated K-Fold Cross-Validation (CV)\n\n\nPartition your data into \\(K\\) randomly selected non-overlapping “folds”\n\nFolds don’t overlap and every training observation is in one fold\nEach fold contained \\(1/K\\) of the training data\n\nLooping through the folds \\(k = 1, \\ldots, K\\):\n\nTreat fold \\(k\\) as the assessment set\nTreat all folds except for \\(k\\) as the analysis set\nFit model to analysis set (use whole modeling workflow)\nCompute error metrics on assessment set\n\nAfter loop, you will have \\(K\\) copies of each error metrics\nAverage them together to get performance estimate\nCan also look at distribution of performance metrics\nRepeat this process from the beginning selected \\(K\\) different randomly chosen folds\nIf we want more accurate estimate you can perform repeated CV with different randomly chosen folds"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#data-ames-housing-prices",
    "href": "slides/12-preproc-missing-cv.html#data-ames-housing-prices",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Data: Ames Housing Prices",
    "text": "Data: Ames Housing Prices\nA data set from De Cock (2011) has 82 fields were recorded for 2,930 properties in Ames IA. This version is copies from the AmesHousing package but does not include a few quality columns that appear to be outcomes rather than predictors.\nGoal: Predict Sale_Price.\n\names |&gt; glimpse()\n\nRows: 2,930\nColumns: 74\n$ MS_SubClass        &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_1946…\n$ MS_Zoning          &lt;fct&gt; Residential_Low_Density, Residential_High_Density, …\n$ Lot_Frontage       &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,…\n$ Lot_Area           &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005…\n$ Street             &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav…\n$ Alley              &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access, …\n$ Lot_Shape          &lt;fct&gt; Slightly_Irregular, Regular, Slightly_Irregular, Re…\n$ Land_Contour       &lt;fct&gt; Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L…\n$ Utilities          &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All…\n$ Lot_Config         &lt;fct&gt; Corner, Inside, Corner, Corner, Inside, Inside, Ins…\n$ Land_Slope         &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G…\n$ Neighborhood       &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gil…\n$ Condition_1        &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No…\n$ Condition_2        &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor…\n$ Bldg_Type          &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn…\n$ House_Style        &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Sto…\n$ Overall_Cond       &lt;fct&gt; Average, Above_Average, Above_Average, Average, Ave…\n$ Year_Built         &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199…\n$ Year_Remod_Add     &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199…\n$ Roof_Style         &lt;fct&gt; Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G…\n$ Roof_Matl          &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompSh…\n$ Exterior_1st       &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Exterior_2nd       &lt;fct&gt; Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Mas_Vnr_Type       &lt;fct&gt; Stone, None, BrkFace, None, None, BrkFace, None, No…\n$ Mas_Vnr_Area       &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6…\n$ Exter_Cond         &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica…\n$ Foundation         &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc…\n$ Bsmt_Cond          &lt;fct&gt; Good, Typical, Typical, Typical, Typical, Typical, …\n$ Bsmt_Exposure      &lt;fct&gt; Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,…\n$ BsmtFin_Type_1     &lt;fct&gt; BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U…\n$ BsmtFin_SF_1       &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, …\n$ BsmtFin_Type_2     &lt;fct&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U…\n$ BsmtFin_SF_2       &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0…\n$ Bsmt_Unf_SF        &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,…\n$ Total_Bsmt_SF      &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, …\n$ Heating            &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas…\n$ Heating_QC         &lt;fct&gt; Fair, Typical, Typical, Excellent, Good, Excellent,…\n$ Central_Air        &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …\n$ Electrical         &lt;fct&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB…\n$ First_Flr_SF       &lt;int&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, …\n$ Second_Flr_SF      &lt;int&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,…\n$ Gr_Liv_Area        &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616…\n$ Bsmt_Full_Bath     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, …\n$ Bsmt_Half_Bath     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Full_Bath          &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, …\n$ Half_Bath          &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ Bedroom_AbvGr      &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, …\n$ Kitchen_AbvGr      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ TotRms_AbvGrd      &lt;int&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,…\n$ Functional         &lt;fct&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T…\n$ Fireplaces         &lt;int&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, …\n$ Garage_Type        &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att…\n$ Garage_Finish      &lt;fct&gt; Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F…\n$ Garage_Cars        &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, …\n$ Garage_Area        &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4…\n$ Garage_Cond        &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica…\n$ Paved_Drive        &lt;fct&gt; Partial_Pavement, Paved, Paved, Paved, Paved, Paved…\n$ Wood_Deck_SF       &lt;int&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48…\n$ Open_Porch_SF      &lt;int&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0…\n$ Enclosed_Porch     &lt;int&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Three_season_porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Screen_Porch       &lt;int&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, …\n$ Pool_Area          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Pool_QC            &lt;fct&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo…\n$ Fence              &lt;fct&gt; No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini…\n$ Misc_Feature       &lt;fct&gt; None, None, Gar2, None, None, None, None, None, Non…\n$ Misc_Val           &lt;int&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, …\n$ Mo_Sold            &lt;int&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, …\n$ Year_Sold          &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201…\n$ Sale_Type          &lt;fct&gt; WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W…\n$ Sale_Condition     &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Nor…\n$ Sale_Price         &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213…\n$ Longitude          &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638…\n$ Latitude           &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4…"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#initial-data-split",
    "href": "slides/12-preproc-missing-cv.html#initial-data-split",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Initial Data Split",
    "text": "Initial Data Split\n\nset.seed(427)\n\ndata_split &lt;- initial_split(ames, strata = \"Sale_Price\")\names_train &lt;- training(data_split)\names_test  &lt;- testing(data_split)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-folds",
    "href": "slides/12-preproc-missing-cv.html#define-folds",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Folds",
    "text": "Define Folds\n\names_folds &lt;- vfold_cv(ames_train, v = 10, repeats = 10)\names_folds\n\n#  10-fold cross-validation repeated 10 times \n# A tibble: 100 × 3\n   splits             id       id2   \n   &lt;list&gt;             &lt;chr&gt;    &lt;chr&gt; \n 1 &lt;split [1977/220]&gt; Repeat01 Fold01\n 2 &lt;split [1977/220]&gt; Repeat01 Fold02\n 3 &lt;split [1977/220]&gt; Repeat01 Fold03\n 4 &lt;split [1977/220]&gt; Repeat01 Fold04\n 5 &lt;split [1977/220]&gt; Repeat01 Fold05\n 6 &lt;split [1977/220]&gt; Repeat01 Fold06\n 7 &lt;split [1977/220]&gt; Repeat01 Fold07\n 8 &lt;split [1978/219]&gt; Repeat01 Fold08\n 9 &lt;split [1978/219]&gt; Repeat01 Fold09\n10 &lt;split [1978/219]&gt; Repeat01 Fold10\n# ℹ 90 more rows"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-models",
    "href": "slides/12-preproc-missing-cv.html#define-models",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Model(s)",
    "text": "Define Model(s)\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine('lm')\n\nknn5_model &lt;- nearest_neighbor(neighbors = 5) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")\n\nknn10_model &lt;- nearest_neighbor(neighbors = 10) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-preprocessing-linear-regression",
    "href": "slides/12-preproc-missing-cv.html#define-preprocessing-linear-regression",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Preprocessing: Linear regression",
    "text": "Define Preprocessing: Linear regression\n\nlm_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; # Convert categorical data into dummy variables\n  step_zv(all_predictors()) |&gt; # remove zero-variance predictors (i.e. predictors with one value)\n  step_corr(all_predictors(), threshold = 0.5) |&gt; # remove highly correlated predictors\n  step_lincomb(all_predictors()) # remove variables that have exact linear combinations"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-preprocessing-knn",
    "href": "slides/12-preproc-missing-cv.html#define-preprocessing-knn",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Preprocessing: KNN",
    "text": "Define Preprocessing: KNN\n\nknn_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; # only uses ames_train for data types\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt; # Convert categorical data into dummy variables\n  step_zv(all_predictors()) |&gt; # remove zero-variance predictors (i.e. predictors with one value)\n  step_normalize(all_predictors())"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-workflows",
    "href": "slides/12-preproc-missing-cv.html#define-workflows",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Workflows",
    "text": "Define Workflows\n\nlm_wf &lt;- workflow() |&gt; add_model(lm_model) |&gt; add_recipe(lm_preproc)\nknn5_wf &lt;- workflow() |&gt; add_model(knn5_model) |&gt; add_recipe(knn_preproc)\nknn10_wf &lt;- workflow() |&gt; add_model(knn10_model) |&gt; add_recipe(knn_preproc)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-metrics",
    "href": "slides/12-preproc-missing-cv.html#define-metrics",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Metrics",
    "text": "Define Metrics\n\names_metrics &lt;- metric_set(rmse, rsq)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#fit-and-assess-models",
    "href": "slides/12-preproc-missing-cv.html#fit-and-assess-models",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Fit and Assess Models",
    "text": "Fit and Assess Models\n\nlm_results &lt;- lm_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn5_results &lt;- knn5_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn10_results &lt;- knn10_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#collecting-metrics",
    "href": "slides/12-preproc-missing-cv.html#collecting-metrics",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Collecting Metrics",
    "text": "Collecting Metrics\n\ncollect_metrics(lm_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.308697e+04\n100\n519.8344225\nPreprocessor1_Model1\n\n\nrsq\nstandard\n8.293514e-01\n100\n0.0044723\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn5_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n4.029708e+04\n100\n438.177121\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.480995e-01\n100\n0.004546\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn10_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.827391e+04\n100\n431.911500\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.774301e-01\n100\n0.003866\nPreprocessor1_Model1"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#final-model",
    "href": "slides/12-preproc-missing-cv.html#final-model",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Final Model",
    "text": "Final Model\n\nAfter choosing best model/workflow, fit on full training set and assess on test set\n\n\nfinal_fit &lt;- lm_wf |&gt; fit(data = ames_train)\nfinal_fit_perf &lt;- final_fit |&gt; \n  predict(new_data = ames_test) |&gt; \n  bind_cols(ames_test) |&gt; \n  ames_metrics(truth = Sale_Price, estimate = .pred)\n\nfinal_fit_perf |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n3.980410e+04\n\n\nrsq\nstandard\n7.609099e-01"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#extracting-cv-metrics",
    "href": "slides/12-preproc-missing-cv.html#extracting-cv-metrics",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Extracting CV Metrics",
    "text": "Extracting CV Metrics\n\ncollect_metrics averages over all 100 models\nset summarize = FALSE to get all the individual errors\n\n\ncollect_metrics(lm_results, summarize = FALSE) |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\nid\nid2\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nRepeat01\nFold01\nrmse\nstandard\n3.879626e+04\nPreprocessor1_Model1\n\n\nRepeat01\nFold01\nrsq\nstandard\n8.281876e-01\nPreprocessor1_Model1\n\n\nRepeat01\nFold02\nrmse\nstandard\n2.811550e+04\nPreprocessor1_Model1\n\n\nRepeat01\nFold02\nrsq\nstandard\n8.581329e-01\nPreprocessor1_Model1\n\n\nRepeat01\nFold03\nrmse\nstandard\n2.930500e+04\nPreprocessor1_Model1\n\n\nRepeat01\nFold03\nrsq\nstandard\n8.533908e-01\nPreprocessor1_Model1"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#combining-cv-metrics",
    "href": "slides/12-preproc-missing-cv.html#combining-cv-metrics",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Combining CV Metrics",
    "text": "Combining CV Metrics\n\nall_metrics &lt;- bind_cols(method = \"lm\", collect_metrics(lm_results, summarize = FALSE)) |&gt;\n  bind_rows(bind_cols(method = \"knn5\", collect_metrics(knn5_results, summarize = FALSE))) |&gt;\n  bind_rows(bind_cols(method = \"knn10\", collect_metrics(knn10_results, summarize = FALSE)))"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#visualizing-cv-metrics",
    "href": "slides/12-preproc-missing-cv.html#visualizing-cv-metrics",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Visualizing CV Metrics",
    "text": "Visualizing CV Metrics\n\n\n\nall_metrics |&gt; \n  filter(.metric == \"rmse\") |&gt; \n  ggplot(aes(x = method, y = .estimate)) +\n  geom_boxplot() +\n  geom_hline(yintercept=final_fit_perf |&gt; filter(.metric == \"rmse\") |&gt; pull(.estimate), color = \"red\")\n\n\n\n\n\n\n\n\n\n\nall_metrics |&gt; \n  filter(.metric == \"rsq\") |&gt; \n  ggplot(aes(x = method, y = .estimate)) +\n  geom_boxplot() +\n  geom_hline(yintercept=final_fit_perf |&gt; filter(.metric == \"rsq\") |&gt; pull(.estimate), color = \"red\")"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#data-different-ames-housing-prices",
    "href": "slides/12-preproc-missing-cv.html#data-different-ames-housing-prices",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Data: Different Ames Housing Prices",
    "text": "Data: Different Ames Housing Prices\nGoal: Predict Sale_Price.\n\names &lt;- read_rds(\"../data/AmesHousing.rds\")\names |&gt; glimpse()\n\nRows: 881\nColumns: 20\n$ Sale_Price    &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, …\n$ Gr_Liv_Area   &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, …\n$ Garage_Type   &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, …\n$ Garage_Cars   &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,…\n$ Garage_Area   &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7…\n$ Street        &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa…\n$ Utilities     &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, …\n$ Pool_Area     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Neighborhood  &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest…\n$ Screen_Porch  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Overall_Qual  &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average…\n$ Lot_Area      &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101…\n$ Lot_Frontage  &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6…\n$ MS_SubClass   &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_…\n$ Misc_Val      &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,…\n$ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7…\n$ First_Flr_SF  &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15…\n$ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11…\n$ Year_Built    &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20…"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#today",
    "href": "slides/12-preproc-missing-cv.html#today",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Today",
    "text": "Today\nWell cover some common pre-processing tasks:\n\nDealing with zero-variance (zv) and/or near-zero variance (nzv) variables\nImputing missing entries\nLabel encoding ordinal categorical variables\nStandardizing (centering and scaling) numeric predictors\nLumping predictors\nOne-hot/dummy encoding categorical predictor"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#pre-split-cleaning",
    "href": "slides/12-preproc-missing-cv.html#pre-split-cleaning",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Pre-Split Cleaning",
    "text": "Pre-Split Cleaning\n\nBefore you split your data: make sure data is in correct format\nThis may mean different things for different data sets\nCommon examples:\n\nFixing names of columns\nEnsure all variable types are correct\nEnsure all factor levels are correct and in order (if applicable)\nRemove any variables that are not important (or harmful) to your analysis\nEnsure missing values are coded as such (i.e. as NA instead of 0 or -1 or “missing”)\nFilling in missing values where you know what the answer should be (i.e. if a missing value really means 0 instead of missing)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#example-factor-levels-in-wrong-order",
    "href": "slides/12-preproc-missing-cv.html#example-factor-levels-in-wrong-order",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Example: Factor Levels in Wrong Order",
    "text": "Example: Factor Levels in Wrong Order\n\names |&gt; pull(Overall_Qual) |&gt; levels()\n\n [1] \"Above_Average\"  \"Average\"        \"Below_Average\"  \"Excellent\"     \n [5] \"Fair\"           \"Good\"           \"Poor\"           \"Very_Excellent\"\n [9] \"Very_Good\"      \"Very_Poor\""
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#re-factoring",
    "href": "slides/12-preproc-missing-cv.html#re-factoring",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Re-Factoring",
    "text": "Re-Factoring\n\names &lt;- ames |&gt; \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")))\names |&gt; pull(Overall_Qual) |&gt; levels()\n\n [1] \"Very_Poor\"      \"Poor\"           \"Fair\"           \"Below_Average\" \n [5] \"Average\"        \"Above_Average\"  \"Good\"           \"Very_Good\"     \n [9] \"Excellent\"      \"Very_Excellent\""
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables",
    "href": "slides/12-preproc-missing-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables",
    "text": "Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables\nHeuristic for detecting near-zero variance features is:\n\nThe fraction of unique values over the sample size is low (say ≤ 10%).\nThe ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say ≥ 20%).\n\n\nlibrary(caret)\nnearZeroVar(ames, saveMetrics = TRUE) |&gt; kable()\n\n\n\n\n\nfreqRatio\npercentUnique\nzeroVar\nnzv\n\n\n\n\nSale_Price\n1.000000\n55.7321226\nFALSE\nFALSE\n\n\nGr_Liv_Area\n1.333333\n62.9965948\nFALSE\nFALSE\n\n\nGarage_Type\n2.196581\n0.6810443\nFALSE\nFALSE\n\n\nGarage_Cars\n1.970213\n0.5675369\nFALSE\nFALSE\n\n\nGarage_Area\n2.250000\n38.0249716\nFALSE\nFALSE\n\n\nStreet\n219.250000\n0.2270148\nFALSE\nTRUE\n\n\nUtilities\n880.000000\n0.2270148\nFALSE\nTRUE\n\n\nPool_Area\n876.000000\n0.6810443\nFALSE\nTRUE\n\n\nNeighborhood\n1.476744\n2.9511918\nFALSE\nFALSE\n\n\nScreen_Porch\n199.750000\n6.6969353\nFALSE\nTRUE\n\n\nOverall_Qual\n1.119816\n1.1350738\nFALSE\nFALSE\n\n\nLot_Area\n1.071429\n79.7956867\nFALSE\nFALSE\n\n\nLot_Frontage\n1.617021\n11.5777526\nFALSE\nFALSE\n\n\nMS_SubClass\n1.959064\n1.7026107\nFALSE\nFALSE\n\n\nMisc_Val\n141.833333\n1.9296254\nFALSE\nTRUE\n\n\nOpen_Porch_SF\n23.176471\n19.2962543\nFALSE\nFALSE\n\n\nTotRms_AbvGrd\n1.311225\n1.2485812\nFALSE\nFALSE\n\n\nFirst_Flr_SF\n1.777778\n63.7911464\nFALSE\nFALSE\n\n\nSecond_Flr_SF\n64.250000\n31.3280363\nFALSE\nFALSE\n\n\nYear_Built\n1.175000\n11.9182747\nFALSE\nFALSE"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#recipe-near-zero-variance",
    "href": "slides/12-preproc-missing-cv.html#recipe-near-zero-variance",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Recipe: Near-Zero Variance",
    "text": "Recipe: Near-Zero Variance\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) # remove zero or near-zero variable predictors"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#missing-data",
    "href": "slides/12-preproc-missing-cv.html#missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Missing Data",
    "text": "Missing Data\n\nMany times, you can’t just drop missing data\nEven if you can, dropping missing values can generate biased data/models\nSometimes missing data gives you more information\nTypes of missing data:\n\nMissing completely at random (MCAR): there is no pattern to your missing values\nMissing at random (MAR): missing values are dependent on other values in the data set\nMissing not at random (MNAR): missing values are dependent on the value that is missing\n\nStructured missingness (SM): when the missingness of certain values are depends on one another, regardless of whether the missing values are MCAR, MAR, or MNAR"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#mcar-examples",
    "href": "slides/12-preproc-missing-cv.html#mcar-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "MCAR: Examples",
    "text": "MCAR: Examples\n\nSensor data: occasionally sensors break so you’re missing data randomly\nSurvey data: sometimes people just randomly skip questions\nSurvey data: customers are randomly given 5 questions from a bank of 100 questions"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#mar-examples",
    "href": "slides/12-preproc-missing-cv.html#mar-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "MAR: Examples",
    "text": "MAR: Examples\n\nMen are less likely to respond to surveys about depression\nMedical study: patients who miss follow-up appointments are more likely to be young\nSurvey responses: ESL respondents may be more likely to skip certain questions that are difficult to interpret (only MAR if you know they are ESL)\nMeasure of student performance: students who score lower are more likely to skip questions"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#mnar-examples",
    "href": "slides/12-preproc-missing-cv.html#mnar-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "MNAR: Examples",
    "text": "MNAR: Examples\n\nSurvey on income: respondent may be less likely to report their income if they are poor\nSurvey about political beliefs: respondent may be more likely to skip questions when their answer is perceived as undesirable\nCustomer satisfaction: only customers who feel strongly respond\nMedical study: patients refuse to report unhealthy habits"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#structurally-missing-examples",
    "href": "slides/12-preproc-missing-cv.html#structurally-missing-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Structurally Missing: Examples",
    "text": "Structurally Missing: Examples\n\nHealth survey: all questions related to pregnancy are left blank by males\nBank data set: combination of home, auto, and credit cards… not all customer have all three so have missing data in certain portions\nSurvey: many respondents by stop the survey early so all questions after a certain point are missing\nNetflix: customers may only watch similar movies and TV shows"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#remedies-for-missing-data",
    "href": "slides/12-preproc-missing-cv.html#remedies-for-missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Remedies for Missing Data",
    "text": "Remedies for Missing Data\n\nLot of complicated ways that you can read about\nCan drop column of too much of the data is missing\nImputing:\n\nstep_impute_median: used for numeric (especially discrete) variables\nstep_impute_mean: used for numeric variables\nstep_impute_knn: used for both numeric and categorical variables (computationally expensive)\nstep_impute_mode: used for nominal (having no order) categorical variable"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#exploring-missing-data",
    "href": "slides/12-preproc-missing-cv.html#exploring-missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Exploring Missing Data",
    "text": "Exploring Missing Data\n\names |&gt; \n  summarize(across(everything(), ~ sum(is.na(.)))) |&gt; \n  pivot_longer(everything()) |&gt; \n  filter(value &gt; 0) |&gt; \n  kable()\n\n\n\n\nname\nvalue\n\n\n\n\nGr_Liv_Area\n113\n\n\nGarage_Type\n54\n\n\nYear_Built\n41"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#missing-data-garage_type",
    "href": "slides/12-preproc-missing-cv.html#missing-data-garage_type",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Missing Data: Garage_Type",
    "text": "Missing Data: Garage_Type\n\nThe reason that Garage_Type is missing is because there is no basement\n\nSolution: replace NAs with No_Garage\nDo this before data splitting"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#fixing-garage_type",
    "href": "slides/12-preproc-missing-cv.html#fixing-garage_type",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Fixing Garage_Type",
    "text": "Fixing Garage_Type\n\names &lt;- ames |&gt; \n  mutate(Garage_Type = as_factor(if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type)))"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#missing-data-year_built",
    "href": "slides/12-preproc-missing-cv.html#missing-data-year_built",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Missing Data: Year_Built",
    "text": "Missing Data: Year_Built\n\nMCAR\n\nSolution 1: Impute with mean or median\nSolution 2: Impute with KNN… maybe we can infer what the values are based on other values in the data set?"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#missing-data-gr_liv_area",
    "href": "slides/12-preproc-missing-cv.html#missing-data-gr_liv_area",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Missing Data: Gr_Liv_Area",
    "text": "Missing Data: Gr_Liv_Area\n\nMCAR\n\nSolution 1: Impute with mean or median\nSolution 2: Impute with KNN… maybe we can infer what the values are based on other values in the data set?"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#recipe-missing-data",
    "href": "slides/12-preproc-missing-cv.html#recipe-missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Recipe: Missing Data",
    "text": "Recipe: Missing Data\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) # impute missing values in Overall_Qual and Year_Built\n\n\nNote: step_imput_knn uses the “Gower’s Distance” so don’t need to worry about normalizing"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#encoding-ordinal-features",
    "href": "slides/12-preproc-missing-cv.html#encoding-ordinal-features",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Encoding Ordinal Features",
    "text": "Encoding Ordinal Features\nTwo types of categorical features:\n\nOrdinal (order is important)\nNominal (order is not important)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#encoding-ordinal-features-1",
    "href": "slides/12-preproc-missing-cv.html#encoding-ordinal-features-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Encoding Ordinal Features",
    "text": "Encoding Ordinal Features\n\names |&gt; pull(Overall_Qual) |&gt; levels()\n\n [1] \"Very_Poor\"      \"Poor\"           \"Fair\"           \"Below_Average\" \n [5] \"Average\"        \"Above_Average\"  \"Good\"           \"Very_Good\"     \n [9] \"Excellent\"      \"Very_Excellent\"\n\n\n\nVery_Poor = 1, Poor = 2, Fair = 3, etc…"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#recipe-encoding-ordinal-features",
    "href": "slides/12-preproc-missing-cv.html#recipe-encoding-ordinal-features",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Recipe: Encoding Ordinal Features",
    "text": "Recipe: Encoding Ordinal Features\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) # convert Overall_Qual into ordinal encoding"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#lump-small-categories-together",
    "href": "slides/12-preproc-missing-cv.html#lump-small-categories-together",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Lump Small Categories Together",
    "text": "Lump Small Categories Together\n\names |&gt; count(Neighborhood) |&gt; kable()\n\n\n\n\nNeighborhood\nn\n\n\n\n\nNorth_Ames\n127\n\n\nCollege_Creek\n86\n\n\nOld_Town\n83\n\n\nEdwards\n49\n\n\nSomerset\n50\n\n\nNorthridge_Heights\n52\n\n\nGilbert\n47\n\n\nSawyer\n49\n\n\nNorthwest_Ames\n41\n\n\nSawyer_West\n31\n\n\nMitchell\n33\n\n\nBrookside\n33\n\n\nCrawford\n22\n\n\nIowa_DOT_and_Rail_Road\n28\n\n\nTimberland\n21\n\n\nNorthridge\n22\n\n\nStone_Brook\n17\n\n\nSouth_and_West_of_Iowa_State_University\n21\n\n\nClear_Creek\n16\n\n\nMeadow_Village\n14\n\n\nBriardale\n10\n\n\nBloomington_Heights\n10\n\n\nVeenker\n9\n\n\nNorthpark_Villa\n3\n\n\nBlueste\n3\n\n\nGreens\n4"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#lump-small-categories-together-1",
    "href": "slides/12-preproc-missing-cv.html#lump-small-categories-together-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Lump Small Categories Together",
    "text": "Lump Small Categories Together\n\names |&gt; mutate(Neighborhood = fct_lump_prop(Neighborhood, 0.05)) |&gt; \n  count(Neighborhood) |&gt;  kable()\n\n\n\n\nNeighborhood\nn\n\n\n\n\nNorth_Ames\n127\n\n\nCollege_Creek\n86\n\n\nOld_Town\n83\n\n\nEdwards\n49\n\n\nSomerset\n50\n\n\nNorthridge_Heights\n52\n\n\nGilbert\n47\n\n\nSawyer\n49\n\n\nOther\n338"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#recipe-lumping-small-factors-together",
    "href": "slides/12-preproc-missing-cv.html#recipe-lumping-small-factors-together",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Recipe: Lumping Small Factors Together",
    "text": "Recipe: Lumping Small Factors Together\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") # lump all categories with less than 1% representation into a category called Other for each variable"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#one-hotdummy-encoding-categorical-predictors",
    "href": "slides/12-preproc-missing-cv.html#one-hotdummy-encoding-categorical-predictors",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "One-hot/dummy encoding categorical predictors",
    "text": "One-hot/dummy encoding categorical predictors\n\nFigure 3.9: Machine Learning with R"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#recipe-dummy-variables",
    "href": "slides/12-preproc-missing-cv.html#recipe-dummy-variables",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Recipe: Dummy Variables",
    "text": "Recipe: Dummy Variables\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)  # in general use one_hot unless doing linear regression"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#receipe-center-and-scale",
    "href": "slides/12-preproc-missing-cv.html#receipe-center-and-scale",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Receipe: Center and scale",
    "text": "Receipe: Center and scale\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;  # in general use one_hot unless doing linear regression\n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#order-of-preprocessing-step",
    "href": "slides/12-preproc-missing-cv.html#order-of-preprocessing-step",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Order of Preprocessing Step",
    "text": "Order of Preprocessing Step\nQuestions to ask:\n\nShould this be done before or after data splitting?\nIf I do step_A first what is the impact on step_B? For example, do you want to encode categorical variables before or after normalizing?\nWhat data format is required by the model I’m fitting and how will my model react to these changes?\nIs this step part of my “model”? I.e. is this a decision I’m making based on the data or based on subject matter expertise?\nDo I have access to my test predictors?"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#questions",
    "href": "slides/12-preproc-missing-cv.html#questions",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Questions",
    "text": "Questions\n\nShould I lump before or after dummy coding?\nShould I dummy code before or after normalizing?\nShould I lump before my initial split?\nHow does ordinal encoding impact linear regression vs. KNN?"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#clean-data-set",
    "href": "slides/12-preproc-missing-cv.html#clean-data-set",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Clean Data Set",
    "text": "Clean Data Set\n\names &lt;- ames |&gt; \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")),\n         Garage_Type = if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type),\n         Garage_Type = as_factor(Garage_Type)\n         )"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#initial-data-split-1",
    "href": "slides/12-preproc-missing-cv.html#initial-data-split-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Initial Data Split",
    "text": "Initial Data Split\n\nset.seed(427)\n\ndata_split &lt;- initial_split(ames, strata = \"Sale_Price\")\names_train &lt;- training(data_split)\names_test  &lt;- testing(data_split)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-folds-1",
    "href": "slides/12-preproc-missing-cv.html#define-folds-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Folds",
    "text": "Define Folds\n\names_folds &lt;- vfold_cv(ames_train, v = 10, repeats = 10)\names_folds\n\n#  10-fold cross-validation repeated 10 times \n# A tibble: 100 × 3\n   splits           id       id2   \n   &lt;list&gt;           &lt;chr&gt;    &lt;chr&gt; \n 1 &lt;split [594/66]&gt; Repeat01 Fold01\n 2 &lt;split [594/66]&gt; Repeat01 Fold02\n 3 &lt;split [594/66]&gt; Repeat01 Fold03\n 4 &lt;split [594/66]&gt; Repeat01 Fold04\n 5 &lt;split [594/66]&gt; Repeat01 Fold05\n 6 &lt;split [594/66]&gt; Repeat01 Fold06\n 7 &lt;split [594/66]&gt; Repeat01 Fold07\n 8 &lt;split [594/66]&gt; Repeat01 Fold08\n 9 &lt;split [594/66]&gt; Repeat01 Fold09\n10 &lt;split [594/66]&gt; Repeat01 Fold10\n# ℹ 90 more rows"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-models-1",
    "href": "slides/12-preproc-missing-cv.html#define-models-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Model(s)",
    "text": "Define Model(s)\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine('lm')\n\nknn5_model &lt;- nearest_neighbor(neighbors = 5) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")\n\nknn10_model &lt;- nearest_neighbor(neighbors = 10) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-preprocessing-linear-regression-1",
    "href": "slides/12-preproc-missing-cv.html#define-preprocessing-linear-regression-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Preprocessing: Linear regression",
    "text": "Define Preprocessing: Linear regression\n\nlm_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt;  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |&gt; # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-preprocessing-knn-1",
    "href": "slides/12-preproc-missing-cv.html#define-preprocessing-knn-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Preprocessing: KNN",
    "text": "Define Preprocessing: KNN\n\nknn_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-workflows-1",
    "href": "slides/12-preproc-missing-cv.html#define-workflows-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Workflows",
    "text": "Define Workflows\n\nlm_wf &lt;- workflow() |&gt; add_model(lm_model) |&gt; add_recipe(lm_preproc)\nknn5_wf &lt;- workflow() |&gt; add_model(knn5_model) |&gt; add_recipe(knn_preproc)\nknn10_wf &lt;- workflow() |&gt; add_model(knn10_model) |&gt; add_recipe(knn_preproc)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#define-metrics-1",
    "href": "slides/12-preproc-missing-cv.html#define-metrics-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Define Metrics",
    "text": "Define Metrics\n\names_metrics &lt;- metric_set(rmse, rsq)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#fit-and-assess-models-1",
    "href": "slides/12-preproc-missing-cv.html#fit-and-assess-models-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Fit and Assess Models",
    "text": "Fit and Assess Models\n\nlm_results &lt;- lm_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn5_results &lt;- knn5_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn10_results &lt;- knn10_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#collecting-metrics-1",
    "href": "slides/12-preproc-missing-cv.html#collecting-metrics-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Collecting Metrics",
    "text": "Collecting Metrics\n\ncollect_metrics(lm_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.968417e+04\n100\n764.5524121\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.559368e-01\n100\n0.0088924\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn5_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.999696e+04\n100\n956.4824660\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.596873e-01\n100\n0.0062576\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn10_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.952087e+04\n100\n1008.0467220\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.681888e-01\n100\n0.0065365\nPreprocessor1_Model1"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#final-evaluate-final-model",
    "href": "slides/12-preproc-missing-cv.html#final-evaluate-final-model",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Final & Evaluate Final Model",
    "text": "Final & Evaluate Final Model\n\nAfter choosing best model/workflow, fit on full training set and assess on test set\n\n\nfinal_fit &lt;- knn10_wf |&gt; fit(data = ames_train)\nfinal_fit_perf &lt;- final_fit |&gt; \n  predict(new_data = ames_test) |&gt; \n  bind_cols(ames_test) |&gt; \n  ames_metrics(truth = Sale_Price, estimate = .pred)\n\nfinal_fit_perf |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n4.941178e+04\n\n\nrsq\nstandard\n7.181459e-01"
  },
  {
    "objectID": "slides/12-preproc-missing-cv.html#tips",
    "href": "slides/12-preproc-missing-cv.html#tips",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling",
    "section": "Tips",
    "text": "Tips\n\nCan try out different pre-processing to see if it improves your model!\nProcess can be intense for you computer, so might take a while\nNo 100% correct way to do it, although there are some 100% incorrect ways to do it\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/28-more-classification-2.html#computational-set-up",
    "href": "slides/28-more-classification-2.html#computational-set-up",
    "title": "MATH 427: More More on Classification",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/28-more-classification-2.html#data-voter-frequency",
    "href": "slides/28-more-classification-2.html#data-voter-frequency",
    "title": "MATH 427: More More on Classification",
    "section": "Data: Voter Frequency",
    "text": "Data: Voter Frequency\n\nInfo about data\nGoal: Identify individuals who are unlikely to vote to help organization target “get out the vote” effort.\n\n\nvoter_data &lt;- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nvoter_clean &lt;- voter_data |&gt;\n  select(-RespId, -weight, -Q1) |&gt;\n  mutate(\n    educ = factor(educ, levels = c(\"High school or less\", \"Some college\", \"College\")),\n    income_cat = factor(income_cat, levels = c(\"Less than $40k\", \"$40-75k \",\n                                               \"$75-125k\", \"$125k or more\")),\n    voter_category = factor(voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n  ) |&gt;\n  filter(Q22 != 5 | is.na(Q22)) |&gt;\n  mutate(Q22 = as_factor(Q22),\n         Q22 = if_else(is.na(Q22), \"Not Asked\", Q22),\n         across(Q28_1:Q28_8, ~if_else(.x == -1, 0, .x)),\n         across(Q28_1:Q28_8, ~ as_factor(.x)),\n         across(Q28_1:Q28_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n         across(Q29_1:Q29_10, ~if_else(.x == -1, 0, .x)),\n         across(Q29_1:Q29_10, ~ as_factor(.x)),\n         across(Q29_1:Q29_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n        Party_ID = as_factor(case_when(\n          Q31 == 1 ~ \"Strong Republican\",\n          Q31 == 2 ~ \"Republican\",\n          Q32 == 1  ~ \"Strong Democrat\",\n          Q32 == 2 ~ \"Democrat\",\n          Q33 == 1 ~ \"Lean Republican\",\n          Q33 == 2 ~ \"Lean Democrat\",\n          TRUE ~ \"Other\"\n        )),\n        Party_ID = factor(Party_ID, levels =c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n                                                \"Other\", \"Lean Democrat\", \"Democrat\", \"Strong Democrat\")),\n        across(!ppage, ~as_factor(if_else(.x == -1, NA, .x))))"
  },
  {
    "objectID": "slides/28-more-classification-2.html#split-data",
    "href": "slides/28-more-classification-2.html#split-data",
    "title": "MATH 427: More More on Classification",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\n\nvoter_splits &lt;- initial_split(voter_clean, prop = 0.7, strata = voter_category)\nvoter_train &lt;- training(voter_splits)\nvoter_test &lt;- testing(voter_splits)"
  },
  {
    "objectID": "slides/28-more-classification-2.html#problem-more-than-two-categories",
    "href": "slides/28-more-classification-2.html#problem-more-than-two-categories",
    "title": "MATH 427: More More on Classification",
    "section": "Problem: More than two categories",
    "text": "Problem: More than two categories\n\nvoter_train |&gt; \n  ggplot(aes(x = voter_category)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/28-more-classification-2.html#define-model-multinomial-regression",
    "href": "slides/28-more-classification-2.html#define-model-multinomial-regression",
    "title": "MATH 427: More More on Classification",
    "section": "Define Model: Multinomial Regression",
    "text": "Define Model: Multinomial Regression\n\nmn_reg_model &lt;- multinom_reg(mixture = 1, penalty = 0.005) |&gt; # I chose this penalty arbitrarily\n  set_engine(\"glmnet\", family = \"multinomial\") |&gt; \n  set_mode(\"classification\")"
  },
  {
    "objectID": "slides/28-more-classification-2.html#define-recipe",
    "href": "slides/28-more-classification-2.html#define-recipe",
    "title": "MATH 427: More More on Classification",
    "section": "Define Recipe",
    "text": "Define Recipe\n\nmr_recipe &lt;- recipe(voter_category ~ . , data = voter_train) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |&gt;\n  step_impute_median(all_numeric_predictors()) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/28-more-classification-2.html#define-workflow-and-fit",
    "href": "slides/28-more-classification-2.html#define-workflow-and-fit",
    "title": "MATH 427: More More on Classification",
    "section": "Define Workflow and Fit",
    "text": "Define Workflow and Fit\n\nmr_fit &lt;- workflow() |&gt;\n  add_model(mn_reg_model) |&gt;\n  add_recipe(mr_recipe) |&gt; \n  fit(voter_train)"
  },
  {
    "objectID": "slides/28-more-classification-2.html#look-at-predictions",
    "href": "slides/28-more-classification-2.html#look-at-predictions",
    "title": "MATH 427: More More on Classification",
    "section": "Look at Predictions",
    "text": "Look at Predictions\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; slice_sample(n=10) |&gt; select(1:4) |&gt; head() |&gt;  kable()\n\n\n\n\n.pred_class\n.pred_rarely/never\n.pred_sporadic\n.pred_always\n\n\n\n\nsporadic\n0.0317402\n0.5122327\n0.4560271\n\n\nalways\n0.0463060\n0.4464935\n0.5072005\n\n\nsporadic\n0.0553279\n0.6529393\n0.2917328\n\n\nalways\n0.0388506\n0.4351109\n0.5260385\n\n\nalways\n0.0361359\n0.4164662\n0.5473979\n\n\nalways\n0.0991499\n0.4310622\n0.4697879"
  },
  {
    "objectID": "slides/28-more-classification-2.html#confusion-matrix",
    "href": "slides/28-more-classification-2.html#confusion-matrix",
    "title": "MATH 427: More More on Classification",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  conf_mat(truth = voter_category, estimate = .pred_class) |&gt; autoplot(\"heatmap\")"
  },
  {
    "objectID": "slides/28-more-classification-2.html#last-time",
    "href": "slides/28-more-classification-2.html#last-time",
    "title": "MATH 427: More More on Classification",
    "section": "Last Time",
    "text": "Last Time\n\nNo “Positive” and “Negative” anymore\nMost of our metrics were based on having “Positive” vs. “Negative”\nSolution 1: 1-vs-all metrics"
  },
  {
    "objectID": "slides/28-more-classification-2.html#evaluating-multiclass-models",
    "href": "slides/28-more-classification-2.html#evaluating-multiclass-models",
    "title": "MATH 427: More More on Classification",
    "section": "Evaluating Multiclass Models",
    "text": "Evaluating Multiclass Models\n\nSolution 2: Average metrics across labels\n\nMacro-averaging average one-versus-all metrics\n\nRecall: \\(\\frac{0.66+0.72+0.41}{3} \\approx 0.60\\)\n\nMacro-weighted averaging same but weight by class size\n\nRecall: \\(\\frac{430\\times 0.66+773\\times 0.72+543\\times 0.41}{1746} \\approx 0.61\\)\n\nMicro-averaging compute contribution for each class, aggregates them, then computes a single metric\n\nRecall: \\(\\frac{285+559+221}{430+779+543} \\approx 0.61\\)"
  },
  {
    "objectID": "slides/28-more-classification-2.html#questions-from-last-time",
    "href": "slides/28-more-classification-2.html#questions-from-last-time",
    "title": "MATH 427: More More on Classification",
    "section": "Questions From Last Time",
    "text": "Questions From Last Time\n\nMultinomial Logistic regression:\n\nEach class \\(k\\): \\[\\log \\frac{\\text{Prob. class } k}{\\text{Prob. class } K} = \\beta_{0k} + \\beta_{1k}X_1 + \\cdots + \\beta_{pk}X_p\\]\n\nShuba: Micro-averaged recall is the same as accuracy… CORRECT!\n\nSame is true of precision AND \\(F_1\\)\nUseful if you have a “multi-label” classification problem (not covered in this class)"
  },
  {
    "objectID": "slides/28-more-classification-2.html#rabins-question",
    "href": "slides/28-more-classification-2.html#rabins-question",
    "title": "MATH 427: More More on Classification",
    "section": "Rabin’s Question",
    "text": "Rabin’s Question\n\n“What does a medium sized data set mean?”\nIdea behind ML: identify patterns in data and use them to make predictions\nAs data gets “bigger”:\n\nPatterns become clearer\nComputational complexity increases\n\nInformal definitions:\n\nSmall data: not enough data to fully represent patterns\nBig data: all the info is there but special approaches need to be taken to handle all the data"
  },
  {
    "objectID": "slides/28-more-classification-2.html#thinking-about-small-data",
    "href": "slides/28-more-classification-2.html#thinking-about-small-data",
    "title": "MATH 427: More More on Classification",
    "section": "Thinking about small data",
    "text": "Thinking about small data\n\nPatterns not fully represented in data \\(\\Rightarrow\\) restrict the set of possible patterns and give model less flexibility and freedom\n\nLogistic regression (probably with regularization)\nSupport vector machines (we haven’t talked about these yet)\nTo a lesser extent: Decision Trees\nNOT KNN"
  },
  {
    "objectID": "slides/28-more-classification-2.html#thinking-about-big-data",
    "href": "slides/28-more-classification-2.html#thinking-about-big-data",
    "title": "MATH 427: More More on Classification",
    "section": "Thinking about big data",
    "text": "Thinking about big data\n\nPatterns are definitely there but size introduces computational problems\n\nData set can’t fit in memory\n\nSolution 1: Use a high-memory HPC cluster node\nSolution 2: Modify algorithms to use parts of data instead of full data set (e.g. Stochastic gradient descent)\n\nAlgorithm scales with size of data and will take too long to run/fit\n\nSolution 1: Run in parallel if possible using HPC cluster\nSolution 2: Develop faster algorithms\n\nImplement in a compiled language like C\nDevelop (faster) approximate solution\n\n\n\nCurse of dimensionalality\n\nIf \\(p\\) is too-big \\(\\Rightarrow\\) too much space and things are too far apart \\(\\Rightarrow\\) similar impact to small data but without computational benefit\n\nNote: big \\(n\\) vs. big \\(p\\) can present different issues"
  },
  {
    "objectID": "slides/28-more-classification-2.html#medium-data",
    "href": "slides/28-more-classification-2.html#medium-data",
    "title": "MATH 427: More More on Classification",
    "section": "Medium Data",
    "text": "Medium Data\n\nData that’s big enough to have (most) of the information you need but not so big that it presents computational issues\nKNN sweet spot\n\nEnough data the “nearest-neighbors” are actually “near”\nNot so much data that it takes forever to make predictions"
  },
  {
    "objectID": "slides/28-more-classification-2.html#exploring-with-app",
    "href": "slides/28-more-classification-2.html#exploring-with-app",
    "title": "MATH 427: More More on Classification",
    "section": "Exploring with App",
    "text": "Exploring with App\n\nApp 1\nApp 2"
  },
  {
    "objectID": "slides/28-more-classification-2.html#macro-averaging-in-r",
    "href": "slides/28-more-classification-2.html#macro-averaging-in-r",
    "title": "MATH 427: More More on Classification",
    "section": "Macro-Averaging in R",
    "text": "Macro-Averaging in R\n\nvoter_metrics &lt;- metric_set(accuracy, precision, recall)\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  voter_metrics(truth = voter_category, estimate = .pred_class, estimator = \"macro\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.6099656\n\n\nprecision\nmacro\n0.6375886\n\n\nrecall\nmacro\n0.5976485"
  },
  {
    "objectID": "slides/28-more-classification-2.html#macro-weighted-averaging-in-r",
    "href": "slides/28-more-classification-2.html#macro-weighted-averaging-in-r",
    "title": "MATH 427: More More on Classification",
    "section": "Macro-Weighted Averaging in R",
    "text": "Macro-Weighted Averaging in R\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  voter_metrics(truth = voter_category, estimate = .pred_class, estimator = \"macro_weighted\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.6099656\n\n\nprecision\nmacro_weighted\n0.6176222\n\n\nrecall\nmacro_weighted\n0.6099656"
  },
  {
    "objectID": "slides/28-more-classification-2.html#micro-averaging-in-r",
    "href": "slides/28-more-classification-2.html#micro-averaging-in-r",
    "title": "MATH 427: More More on Classification",
    "section": "Micro-Averaging in R",
    "text": "Micro-Averaging in R\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  voter_metrics(truth = voter_category, estimate = .pred_class, estimator = \"micro\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.6099656\n\n\nprecision\nmicro\n0.6099656\n\n\nrecall\nmicro\n0.6099656"
  },
  {
    "objectID": "slides/28-more-classification-2.html#what-if-output-is-probability",
    "href": "slides/28-more-classification-2.html#what-if-output-is-probability",
    "title": "MATH 427: More More on Classification",
    "section": "What if output is probability?",
    "text": "What if output is probability?\n\nFor binary case we used ROC curve and AUC…\nSimilar ideas apply here:\n\nOne vs. all\nMacro Averaging\nNO MICRO AVERAGING!\nHand and Till extension of AUC"
  },
  {
    "objectID": "slides/28-more-classification-2.html#plotting-one-vs.-all",
    "href": "slides/28-more-classification-2.html#plotting-one-vs.-all",
    "title": "MATH 427: More More on Classification",
    "section": "Plotting one-vs.-all",
    "text": "Plotting one-vs.-all\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_curve(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always) |&gt; \n  autoplot()"
  },
  {
    "objectID": "slides/28-more-classification-2.html#macro-averaged-auc",
    "href": "slides/28-more-classification-2.html#macro-averaged-auc",
    "title": "MATH 427: More More on Classification",
    "section": "Macro Averaged AUC",
    "text": "Macro Averaged AUC\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always, \n          estimator = \"macro\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nmacro\n0.7802495"
  },
  {
    "objectID": "slides/28-more-classification-2.html#macro-weighted-averaged-auc",
    "href": "slides/28-more-classification-2.html#macro-weighted-averaged-auc",
    "title": "MATH 427: More More on Classification",
    "section": "Macro-Weighted Averaged AUC",
    "text": "Macro-Weighted Averaged AUC\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always, \n          estimator = \"macro_weighted\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nmacro_weighted\n0.7636223"
  },
  {
    "objectID": "slides/28-more-classification-2.html#hand-and-till-auc",
    "href": "slides/28-more-classification-2.html#hand-and-till-auc",
    "title": "MATH 427: More More on Classification",
    "section": "Hand and Till AUC",
    "text": "Hand and Till AUC\n\nIdea behind traditional AUC: “How well are my classes being separated?”\nHand and Till: Extend this idea to multiple-classes\nPaper\n\nBasic Idea: Do pairwise comparison of classes and average\n\n\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always) |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nhand_till\n0.7967478"
  },
  {
    "objectID": "slides/28-more-classification-2.html#discussion",
    "href": "slides/28-more-classification-2.html#discussion",
    "title": "MATH 427: More More on Classification",
    "section": "Discussion",
    "text": "Discussion\n\nWhy do you compute averages/means?\n\n\n\nHow would heavily imbalanced classes impact each type of averaging?\n\nWhich type(s) of averaging weight(s) each class equally regardless of balance?\nWhich type(s) of averaging favor(s) larger classes?\nDoes this imply that one is better than the others?"
  },
  {
    "objectID": "slides/28-more-classification-2.html#exploring-with-app-1",
    "href": "slides/28-more-classification-2.html#exploring-with-app-1",
    "title": "MATH 427: More More on Classification",
    "section": "Exploring with App",
    "text": "Exploring with App\n\nApp\n\nBreak into groups\nInvestigate how your performance metrics change between balanced data and unbalanced data\nAdditional Considerations:\n\nImpact of boundaries/models?\nImpact of sample size?\nImpact of noise level?\n\nPlease write down observations so we can discuss\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#pre-survey",
    "href": "slides/02-StatisticalLearning.html#pre-survey",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Pre-Survey",
    "text": "Pre-Survey\nTake 10 minute and fill our this survey.\nIf you chose to opt-out please read as much of this article as you an in 8 minutes and then fill out this survey."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#data-generating-process",
    "href": "slides/02-StatisticalLearning.html#data-generating-process",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Data Generating Process",
    "text": "Data Generating Process\nSuppose we have\n\nFeatures: \\(\\mathbf{X}\\)\nTarget: \\(Y\\)\nGoal: Predict \\(Y\\) using \\(\\mathbf{X}\\)\n\n\n\nData generating process: underlying, unseen and unknowable process that generates \\(Y\\) given \\(\\mathbf{X}\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#population",
    "href": "slides/02-StatisticalLearning.html#population",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Population",
    "text": "Population\nMore mathematically, the “true”/population model can be represented by\n\\[Y=f(\\mathbf{X}) + \\epsilon\\]\nwhere \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero.\n\nGOAL: Estimate \\(f\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#why-estimate-fmathbfx",
    "href": "slides/02-StatisticalLearning.html#why-estimate-fmathbfx",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Why Estimate \\(f(\\mathbf{X})\\)?",
    "text": "Why Estimate \\(f(\\mathbf{X})\\)?\nWe wish to know about \\(f(\\mathbf{X})\\) for two reasons:\n\nPrediction: make an educated guess for what \\(y\\) should be given a new \\(x_0\\): \\[\\hat{y}_0=\\hat{f}(x_0) \\ \\ \\ \\text{or} \\ \\ \\ \\hat{y}_0=\\hat{C}(x_0)\\]\nInference: Understand the relationship between \\(\\mathbf{X}\\) and \\(Y\\).\n\n\n\nAn ML algorithm that is developed mainly for predictive purposes is often termed as a Black Box algorithm."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction",
    "href": "slides/02-StatisticalLearning.html#prediction",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction",
    "text": "Prediction\nThere are two types of prediction problems:\n\nRegression (response \\(Y\\) is quantitative): Build a model \\(\\hat{Y} = \\hat{f}(\\mathbf{X})\\)\nClassification (response \\(Y\\) is qualitative/categorical): Build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\)\n\n\n\nNote: a “hat”, \\(\\hat{\\phantom{f}}\\), over an object represents an estimate of that object\n\nE.g. \\(\\hat{Y}\\) is an estimate of \\(Y\\) and \\(\\hat{f}\\) is an estimate of \\(f\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction-and-inference",
    "href": "slides/02-StatisticalLearning.html#prediction-and-inference",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction and Inference",
    "text": "Prediction and Inference\nIncome dataset\n\nWhy ML? (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction-and-inference-1",
    "href": "slides/02-StatisticalLearning.html#prediction-and-inference-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction and Inference",
    "text": "Prediction and Inference\nIncome dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML? (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#question",
    "href": "slides/02-StatisticalLearning.html#question",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\nBased on the previous two slides, which of the following statements are correct?\n\nQuestionsAnswers\n\n\n\nAs Years of Education increases, Income increases, keeping Seniority fixed.\nAs Years of Education increases, Income decreases, keeping Seniority fixed.\nAs Years of Education increases, Income increases.\nAs Seniority increases, Income increases, keeping Years of Education fixed.\nAs Seniority increases, Income decreases, keeping Years of Education fixed.\nAs Seniority increases, Income increases.\n\n\n\n\nAs Years of Education increases, Income increases, keeping Seniority fixed. TRUE\nAs Years of Education increases, Income decreases, keeping Seniority fixed. FALSE\nAs Years of Education increases, Income increases. TRUE\nAs Seniority increases, Income increases, keeping Years of Education fixed. TRUE\nAs Seniority increases, Income decreases, keeping Years of Education fixed. FALSE\nAs Seniority increases, Income increases. TRUE"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#discussion",
    "href": "slides/02-StatisticalLearning.html#discussion",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Discussion",
    "text": "Discussion\nWhat’s the difference between these two statements:\n\nAs Years of Education increases, Income increases, keeping Seniority fixed.\nAs Years of Education increases, Income increases."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#how-do-we-estimate-fmathbfx",
    "href": "slides/02-StatisticalLearning.html#how-do-we-estimate-fmathbfx",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "How Do We Estimate \\(f(\\mathbf{X})\\)?",
    "text": "How Do We Estimate \\(f(\\mathbf{X})\\)?\nBroadly speaking, we have two approaches.\n\nParametric methods\nNon-parametric methods"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods",
    "href": "slides/02-StatisticalLearning.html#parametric-methods",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\n\nAssume a functional form for \\(f(\\mathbf{X})\\)\n\nLinear Regression: \\(f(\\mathbf{X})=\\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\ldots + \\beta_p \\mathbf{x}_p\\)\nEstimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using labeled data\n\nChoosing \\(\\beta\\)’s that minimize some error metrics is called fitting the model\nThe data we use to fit the model is called our training data"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods-1",
    "href": "slides/02-StatisticalLearning.html#parametric-methods-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\n\nParametric model fit (from ISLR2)\n\nWhat are some potential parametric models that could result in this picture?\nNote: Right line is the true relationship"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods-2",
    "href": "slides/02-StatisticalLearning.html#parametric-methods-2",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\nIncome dataset\n\n\n\n\n\n\n\n\n\nTrue relationship\n\n\n\n\n\n\n\nParametric model\n\n\n\n\n\n\nFrom ISLR2\n\n\n\n\n\nWhat are some functions that could have resulted in the model on the right?\n\\(\\text{Income} \\approx \\beta_0 + \\beta_1\\times\\text{Years of Education} + \\beta_2\\times\\text{Seniority}\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#non-parametric-methods",
    "href": "slides/02-StatisticalLearning.html#non-parametric-methods",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Non-parametric Methods",
    "text": "Non-parametric Methods\n\nNon-parametric approach: no explicit assumptions about the functional form of \\(f(\\mathbf{X})\\)\nMuch more observations (compared to a parametric approach) required to fit non-parametric model\n\nIdea: parametric model restricts space of possible answers\n\n\nIncome dataset\n\n\n\n\n\n\n\n\n\nTrue relationship\n\n\n\n\n\n\n\nNon-parametric model fit\n\n\n\n\n\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-flexibility-of-models",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-flexibility-of-models",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Flexibility of Models",
    "text": "Supervised Learning: Flexibility of Models\n\nFlexibility: smoothness of functions\nMore theoretically: how many parameters are there to estimate?\n\n\n[More flexible \\(\\implies\\) More complex \\(\\implies\\) Less Smooth \\(\\implies\\) Less Restrictive \\(\\implies\\) Less Interpretable"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-some-trade-offs",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-some-trade-offs",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Some Trade-offs",
    "text": "Supervised Learning: Some Trade-offs\n\nPrediction Accuracy versus Interpretability\nGood Fit versus Over-fit or Under-fit\n\n\nTrade-off between flexibility and interpretability (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-selecting-a-model",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-selecting-a-model",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Selecting a Model",
    "text": "Supervised Learning: Selecting a Model\n\nWhy so many different ML techniques?\nThere is no free lunch in statistics: All methods have different pros and cons\n\nMust select correct model for each use-case\n\nRelevant questions in model selection:\n\nHow much observations \\(n\\) and variables \\(p\\)?\nWhat is the relative importance is prediction, interpretability, and inference?\nDo we expect relationship to be non-linear?\nRegression or classification?"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\nWhen we estimate \\(f(\\mathbf{X})\\) using \\(\\hat{f}(\\mathbf{X})\\), then,\n\n\\[\\underbrace{E\\left[Y-\\hat{Y}\\right]^2}_{Error}=E\\left[f(\\mathbf{X})+\\epsilon - \\hat{f}(\\mathbf{X})\\right]^2=\\underbrace{E\\left[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})\\right]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}\\]\n\n\\(E\\left[Y-\\hat{Y}\\right]^2\\): Expected (average) squared difference between predicted and actual (observed) response, Mean Squared Error (MSE)\nGoal: find an estimate of \\(f(\\mathbf{X})\\) to minimize the reducible error"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-1",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\n\nLabeled training data \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\)\n\ni.e. \\(n\\) training observations\n\nFit/train a model from training data\n\n\\(\\hat{y}=\\hat{f}(x)\\), regression\n\\(\\hat{y}=\\hat{C}(x)\\), classification\n\n\nObtain estimates \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)\\) (or, \\(\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)\\)) of training data\nCompute error:\n\nRegression \\[\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2\\]\nClassification \\[\n\\begin{aligned}\n\\text{Training Error Rate}\n&=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]\\\\\n&= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#recap",
    "href": "slides/02-StatisticalLearning.html#recap",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Recap",
    "text": "Recap\n\nRegression vs. Classification\nParametric vs. non-parametric models\nTraining v. test data\nAssessing regression models: Mean-Squared Error\nTrade-offs:\n\nFlexibility vs. interpretability\nBias vs. variance"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#computational-set-up",
    "href": "slides/15-wfsets-selection.html#computational-set-up",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\nlibrary(readODS)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#data-different-ames-housing-prices",
    "href": "slides/15-wfsets-selection.html#data-different-ames-housing-prices",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Data: Different Ames Housing Prices",
    "text": "Data: Different Ames Housing Prices\nGoal: Predict Sale_Price.\n\names &lt;- read_rds(\"../data/AmesHousing.rds\")\names |&gt; glimpse()\n\nRows: 881\nColumns: 20\n$ Sale_Price    &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, …\n$ Gr_Liv_Area   &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, …\n$ Garage_Type   &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, …\n$ Garage_Cars   &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,…\n$ Garage_Area   &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7…\n$ Street        &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa…\n$ Utilities     &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, …\n$ Pool_Area     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Neighborhood  &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest…\n$ Screen_Porch  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Overall_Qual  &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average…\n$ Lot_Area      &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101…\n$ Lot_Frontage  &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6…\n$ MS_SubClass   &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_…\n$ Misc_Val      &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,…\n$ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7…\n$ First_Flr_SF  &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15…\n$ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11…\n$ Year_Built    &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20…"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#clean-data-set",
    "href": "slides/15-wfsets-selection.html#clean-data-set",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Clean Data Set",
    "text": "Clean Data Set\n\names &lt;- ames |&gt; \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")),\n         Garage_Type = if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type),\n         Garage_Type = as_factor(Garage_Type)\n         )"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#initial-data-split",
    "href": "slides/15-wfsets-selection.html#initial-data-split",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Initial Data Split",
    "text": "Initial Data Split\n\ndata_split &lt;- initial_split(ames, strata = \"Sale_Price\")\names_train &lt;- training(data_split)\names_test  &lt;- testing(data_split)"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-folds",
    "href": "slides/15-wfsets-selection.html#define-folds",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Folds",
    "text": "Define Folds\n\names_folds &lt;- vfold_cv(ames_train, v = 10, repeats = 10)\names_folds\n\n#  10-fold cross-validation repeated 10 times \n# A tibble: 100 × 3\n   splits           id       id2   \n   &lt;list&gt;           &lt;chr&gt;    &lt;chr&gt; \n 1 &lt;split [594/66]&gt; Repeat01 Fold01\n 2 &lt;split [594/66]&gt; Repeat01 Fold02\n 3 &lt;split [594/66]&gt; Repeat01 Fold03\n 4 &lt;split [594/66]&gt; Repeat01 Fold04\n 5 &lt;split [594/66]&gt; Repeat01 Fold05\n 6 &lt;split [594/66]&gt; Repeat01 Fold06\n 7 &lt;split [594/66]&gt; Repeat01 Fold07\n 8 &lt;split [594/66]&gt; Repeat01 Fold08\n 9 &lt;split [594/66]&gt; Repeat01 Fold09\n10 &lt;split [594/66]&gt; Repeat01 Fold10\n# ℹ 90 more rows"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-models",
    "href": "slides/15-wfsets-selection.html#define-models",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Model(s)",
    "text": "Define Model(s)\n\nlm_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n\nknn5_model &lt;- nearest_neighbor(neighbors = 5) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")\n\nknn10_model &lt;- nearest_neighbor(neighbors = 10) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-preprocessing-linear-regression",
    "href": "slides/15-wfsets-selection.html#define-preprocessing-linear-regression",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Preprocessing: Linear regression",
    "text": "Define Preprocessing: Linear regression\n\nlm_knnimpute &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt;  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |&gt; # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-preprocessing-linear-regression-1",
    "href": "slides/15-wfsets-selection.html#define-preprocessing-linear-regression-1",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Preprocessing: Linear regression",
    "text": "Define Preprocessing: Linear regression\n\nlm_meanimpute &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_mean(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt;  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |&gt; # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-preprocessing-linear-regression-2",
    "href": "slides/15-wfsets-selection.html#define-preprocessing-linear-regression-2",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Preprocessing: Linear regression",
    "text": "Define Preprocessing: Linear regression\n\nlm_medianimpute &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_median(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt;  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |&gt; # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-preprocessing-knn",
    "href": "slides/15-wfsets-selection.html#define-preprocessing-knn",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Preprocessing: KNN",
    "text": "Define Preprocessing: KNN\n\nknn_preproc1 &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-preprocessing-knn-1",
    "href": "slides/15-wfsets-selection.html#define-preprocessing-knn-1",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Preprocessing: KNN",
    "text": "Define Preprocessing: KNN\n\nknn_preproc2 &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_mean(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-preprocessing-knn-2",
    "href": "slides/15-wfsets-selection.html#define-preprocessing-knn-2",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Preprocessing: KNN",
    "text": "Define Preprocessing: KNN\n\nknn_preproc3 &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_median(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#workflow-sets",
    "href": "slides/15-wfsets-selection.html#workflow-sets",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Workflow Sets",
    "text": "Workflow Sets\n\nInput lists of models and recipes\nIf cross = TRUE will try out all combinations"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#create-lists",
    "href": "slides/15-wfsets-selection.html#create-lists",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Create lists",
    "text": "Create lists\n\nknn_preprocessors &lt;- list(\n  knn_knn_impute = knn_preproc1,\n  knn_mean_impute = knn_preproc2,\n  knn_median_imput = knn_preproc3\n)\n\nknn_models &lt;- list(\n  knn5 = knn5_model,\n  knn10 = knn10_model\n)"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#create-lists-1",
    "href": "slides/15-wfsets-selection.html#create-lists-1",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Create lists",
    "text": "Create lists\n\nlm_preprocessors &lt;- list(\n  lm_knn_impute = lm_knnimpute,\n  lm_mean_impute = lm_meanimpute,\n  lm_median_imput = lm_medianimpute\n)\n\nlm_models &lt;- list(\n  lm_model = lm_model\n)"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-workflow-sets",
    "href": "slides/15-wfsets-selection.html#define-workflow-sets",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Workflow Sets",
    "text": "Define Workflow Sets\n\nknn_models &lt;- workflow_set(knn_preprocessors, knn_models, cross = TRUE)\nlm_models &lt;-  workflow_set(lm_preprocessors, lm_models, cross = TRUE)\nall_models &lt;- lm_models |&gt; \n  bind_rows(knn_models)\n  \nall_models\n\n# A workflow set/tibble: 9 × 4\n  wflow_id                 info             option    result    \n  &lt;chr&gt;                    &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 lm_knn_impute_lm_model   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 lm_mean_impute_lm_model  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 lm_median_imput_lm_model &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 knn_knn_impute_knn5      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 knn_knn_impute_knn10     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 knn_mean_impute_knn5     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n7 knn_mean_impute_knn10    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n8 knn_median_imput_knn5    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n9 knn_median_imput_knn10   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#define-metrics",
    "href": "slides/15-wfsets-selection.html#define-metrics",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Define Metrics",
    "text": "Define Metrics\n\names_metrics &lt;- metric_set(rmse, rsq)"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#fit-resamples",
    "href": "slides/15-wfsets-selection.html#fit-resamples",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Fit Resamples",
    "text": "Fit Resamples\n\nall_fits &lt;- all_models |&gt; \n  workflow_map(\"fit_resamples\",\n               resamples = ames_folds,\n               metrics = ames_metrics)"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#view-metrics",
    "href": "slides/15-wfsets-selection.html#view-metrics",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "View Metrics",
    "text": "View Metrics\n\ncollect_metrics(all_fits) |&gt; \n  filter(.metric == \"rmse\") |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\npreproc\nmodel\n.metric\n.estimator\nmean\nn\nstd_err\n\n\n\n\nlm_knn_impute_lm_model\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrmse\nstandard\n39754.78\n100\n724.5845\n\n\nlm_mean_impute_lm_model\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrmse\nstandard\n40339.76\n100\n790.8570\n\n\nlm_median_imput_lm_model\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrmse\nstandard\n40303.18\n100\n792.2404\n\n\nknn_knn_impute_knn5\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrmse\nstandard\n40865.74\n100\n1168.9516\n\n\nknn_knn_impute_knn10\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrmse\nstandard\n40585.89\n100\n1196.3147\n\n\nknn_mean_impute_knn5\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrmse\nstandard\n40973.29\n100\n1169.1127\n\n\nknn_mean_impute_knn10\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrmse\nstandard\n40749.35\n100\n1197.0999\n\n\nknn_median_imput_knn5\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrmse\nstandard\n40979.23\n100\n1166.2676\n\n\nknn_median_imput_knn10\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrmse\nstandard\n40747.77\n100\n1196.2514"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#view-metrics-1",
    "href": "slides/15-wfsets-selection.html#view-metrics-1",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "View Metrics",
    "text": "View Metrics\n\ncollect_metrics(all_fits) |&gt; \n  filter(.metric == \"rsq\") |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\npreproc\nmodel\n.metric\n.estimator\nmean\nn\nstd_err\n\n\n\n\nlm_knn_impute_lm_model\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrsq\nstandard\n0.7728989\n100\n0.0055434\n\n\nlm_mean_impute_lm_model\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrsq\nstandard\n0.7661074\n100\n0.0067038\n\n\nlm_median_imput_lm_model\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrsq\nstandard\n0.7667739\n100\n0.0066014\n\n\nknn_knn_impute_knn5\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrsq\nstandard\n0.7574393\n100\n0.0080339\n\n\nknn_knn_impute_knn10\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrsq\nstandard\n0.7662157\n100\n0.0070608\n\n\nknn_mean_impute_knn5\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrsq\nstandard\n0.7566579\n100\n0.0076762\n\n\nknn_mean_impute_knn10\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrsq\nstandard\n0.7651128\n100\n0.0068290\n\n\nknn_median_imput_knn5\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrsq\nstandard\n0.7568404\n100\n0.0076513\n\n\nknn_median_imput_knn10\nPreprocessor1_Model1\nrecipe\nnearest_neighbor\nrsq\nstandard\n0.7653082\n100\n0.0068162"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#plotting-results",
    "href": "slides/15-wfsets-selection.html#plotting-results",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Plotting Results",
    "text": "Plotting Results\n\nlibrary(ggrepel)\nautoplot(all_fits, metric = \"rmse\") +\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#plotting-results-1",
    "href": "slides/15-wfsets-selection.html#plotting-results-1",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Plotting Results",
    "text": "Plotting Results\n\nautoplot(all_fits, metric = \"rsq\") +\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#what-is-feature-selection",
    "href": "slides/15-wfsets-selection.html#what-is-feature-selection",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "What is feature selection?",
    "text": "What is feature selection?\n\nHow do we choose what variables to include in our model?\nUp to now… include all of them… probably not the best\nAdvantage of linear regression: interpretability\nIncluding every feature decreases interpretability\nReasons for feature selection:\n\nImprove model performance\nImprove model interpretability\n\nParsimony: simpler models are called more parsimonious\nOccam’s Razor: more parsimonious models are better than less parsimonious models, holding all else constant"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#types-of-feature-selection",
    "href": "slides/15-wfsets-selection.html#types-of-feature-selection",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Types of feature selection",
    "text": "Types of feature selection\n\nSubset selection: Forward/Backward/Best-Subset Selection\nShrinkage-based methods: LASSO and Ridge Regression\nDimension reduction: consider linear combinations of predictors"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#exercise",
    "href": "slides/15-wfsets-selection.html#exercise",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Exercise",
    "text": "Exercise\n\nWith your group, write out the steps for the following algorithms on the board\nGroup 1: Forward selection\nGroup 2: Backward elimination\nGroup 3: Step-wise selection\nGroup 4: Best-subset selection"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#subset-selection-in-r",
    "href": "slides/15-wfsets-selection.html#subset-selection-in-r",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Subset Selection in R",
    "text": "Subset Selection in R\n\ntidymodels does not have an implementation for any subset selection techniques\nregularization (shrinkage-based) methods almost always perform better\ncolino package provides tidymodels implementation\nOther options\n\ncaret package\nolsrr and blorr packages if you don’t care about cross-validation\nimplement yourself"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#feature-selection-in-r",
    "href": "slides/15-wfsets-selection.html#feature-selection-in-r",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Feature Selection in R",
    "text": "Feature Selection in R\n\nWhen creating your recipe, don’t need to always include all variables in your recipe:\n\n\nint_recipe &lt;- recipe(pred ~ var1 + var2 + var1*var2, data = training_data) |&gt; \n  step_x(...)"
  },
  {
    "objectID": "slides/15-wfsets-selection.html#re-using-recipe-but-changing-formula",
    "href": "slides/15-wfsets-selection.html#re-using-recipe-but-changing-formula",
    "title": "MATH 427: Workflow Sets and Feature Selection",
    "section": "Re-using Recipe but changing formula",
    "text": "Re-using Recipe but changing formula\n\nnoint_recipe2 &lt;- new_recipe |&gt; \n  remove_formula() |&gt; \n  add_formula(pred ~ var1 + var2)\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#announcements",
    "href": "slides/09-job-app-intro-roc.html#announcements",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Announcements",
    "text": "Announcements\nOn March 5th at 10am in JAAC, Kyle Mayer will be guest lecturing to talk about his role as a data analyst at Micron. Kyle has a Bachelors in Engineering and a Masters in Physics.\nBio: Kyle Mayer is a Senior Data Analytics Engineer at Micron Technology with over 15 years of experience. He currently supports the Global Quality organization with engineering and data science projects. Kyle specializes in modeling complex physical processes, delivering insights that drive business value. Leveraging his expertise in semiconductor manufacturing, Kyle has developed tools and processes that have prevented over $100M in lost revenue. His passion for applying artificial intelligence to solve complex problems and develop automated systems highlights his commitment to innovation. In his spare time, Kyle enjoys spending time with his kids and going on hikes."
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#job-application-1",
    "href": "slides/09-job-app-intro-roc.html#job-application-1",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Job Application 1",
    "text": "Job Application 1\n\nJob Ad\nDirections and Resources\nRubic (in progress)\nDue Next Friday: CV and Cover Letter\nDue Friday March 21st: whole application"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#computational-set-up",
    "href": "slides/09-job-app-intro-roc.html#computational-set-up",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\nlibrary(ggforce) # sina plots\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#default-dataset",
    "href": "slides/09-job-app-intro-roc.html#default-dataset",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Default Dataset",
    "text": "Default Dataset\n\n\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\nhead(Default) |&gt; kable()  # print first six observations\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n\n\nNo\nYes\n817.1804\n12106.135\n\n\nNo\nNo\n1073.5492\n31767.139\n\n\nNo\nNo\n529.2506\n35704.494\n\n\nNo\nNo\n785.6559\n38463.496\n\n\nNo\nYes\n919.5885\n7491.559\n\n\n\n\n\n\nResponse Variable: default\n\nDefault |&gt; \n  tabyl(default) |&gt;  # class frequencies\n  kable()           # Make it look nice\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n9667\n0.9667\n\n\nYes\n333\n0.0333"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#split-the-data",
    "href": "slides/09-job-app-intro-roc.html#split-the-data",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Split the data",
    "text": "Split the data\n\nset.seed(427)\n\ndefault_split &lt;- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n\n&lt;Training/Testing/Total&gt;\n&lt;6000/4000/10000&gt;\n\ndefault_train &lt;- training(default_split)\ndefault_test &lt;- testing(default_split)"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#k-nearest-neighbors-classifier-build-model",
    "href": "slides/09-job-app-intro-roc.html#k-nearest-neighbors-classifier-build-model",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "K-Nearest Neighbors Classifier: Build Model",
    "text": "K-Nearest Neighbors Classifier: Build Model\n\nResponse (\\(Y\\)): default\nPredictor (\\(X\\)): balance\n\n\nknnfit &lt;- nearest_neighbor(neighbors = 10) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;  \n  fit(default ~ balance, data = default_train)   # fit 10-nn model"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#k-nearest-neighbors-classifier-predictions",
    "href": "slides/09-job-app-intro-roc.html#k-nearest-neighbors-classifier-predictions",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "K-Nearest Neighbors Classifier: Predictions",
    "text": "K-Nearest Neighbors Classifier: Predictions\n\nClass labelsProbabilities\n\n\n\npredict(knnfit, new_data = default_test, type = \"class\") |&gt; head() |&gt; kable()   # obtain predictions as classes\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\n\n\n\n\n\n\nPredicts class w/ maximum probability\n\n\npredict(knnfit, new_data = default_test, type = \"prob\") |&gt; head() |&gt; kable() # obtain predictions as probabilities\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#fitting-a-logistic-regression",
    "href": "slides/09-job-app-intro-roc.html#fitting-a-logistic-regression",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Fitting a logistic regression",
    "text": "Fitting a logistic regression\nFitting a logistic regression model with default as the response and balance as the predictor:\n\nlogregfit &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |&gt; kable()  # obtain results\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6926385\n0.4659035\n-22.95033\n0\n\n\nbalance\n0.0055327\n0.0002841\n19.47329\n0"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#making-predictions-in-r",
    "href": "slides/09-job-app-intro-roc.html#making-predictions-in-r",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Making predictions in R",
    "text": "Making predictions in R\n\nClass LabelsLog-OddsProbabilities\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |&gt; kable()   # obtain class predictions\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\n\n\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |&gt; kable()   # obtain log-odds predictions\n\n\n\n\nx\n\n\n\n\n-6.819727\n\n\n\n\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |&gt; kable()  # obtain probability predictions\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.9989092\n0.0010908"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#binary-classifiers",
    "href": "slides/09-job-app-intro-roc.html#binary-classifiers",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Binary Classifiers",
    "text": "Binary Classifiers\n\nStart with binary classification scenarios\nWith binary classification, designate one category as “Success/Positive” and the other as “Failure/Negative”\n\nIf relevant to your problem: “Positive” should be the thing you’re trying to predict/care more about\nNote: “Positive” \\(\\neq\\) “Good”\nFor default: “Yes” is Positive\n\nSome metrics weight “Positives” more and viceversa"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#last-time",
    "href": "slides/09-job-app-intro-roc.html#last-time",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Last Time",
    "text": "Last Time\n\nConfusion Matrix\nMetrics based on confusion matrix\n\nAccuracy\nRecall/Sensitivity\nPrecision/PPV\nSpecificity\nNPV\nMCC\nF-Measure\n\nToday: ROC and AUC"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#using-a-threshold",
    "href": "slides/09-job-app-intro-roc.html#using-a-threshold",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Using a threshold",
    "text": "Using a threshold\n\nStep 1: Predict probabilities for all observations\n\n\ndefault_test_wprobs &lt;- default_test |&gt;\n  mutate(\n    knn_probs = predict(knnfit, new_data = default_test, type = \"prob\") |&gt; pull(.pred_Yes),\n    logistic_probs = predict(logregfit, new_data = default_test, type = \"prob\") |&gt; pull(.pred_Yes)\n  )\n\ndefault_test_wprobs |&gt; head() |&gt; kable()   # obtain probability predictions\n\n\n\n\ndefault\nstudent\nbalance\nincome\nknn_probs\nlogistic_probs\n\n\n\n\nNo\nNo\n729.5265\n44361.63\n0\n0.0012842\n\n\nNo\nYes\n808.6675\n17600.45\n0\n0.0019883\n\n\nNo\nYes\n1220.5838\n13268.56\n0\n0.0190870\n\n\nNo\nNo\n237.0451\n28251.70\n0\n0.0000843\n\n\nNo\nNo\n606.7423\n44994.56\n0\n0.0006514\n\n\nNo\nNo\n286.2326\n45042.41\n0\n0.0001107"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#using-a-threshold-1",
    "href": "slides/09-job-app-intro-roc.html#using-a-threshold-1",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Using a threshold",
    "text": "Using a threshold\n\nStep 1: Predict probabilities for all observations\nStep 2: Set a threshold to obtain class labels (0.5 below)\n\n\nthreshold &lt;- 0.5   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs &gt; threshold, \"Yes\", \"No\"))\n  )\n\ndefault_test_wprobs |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\nknn_probs\nlogistic_probs\nknn_preds\nlogistic_preds\n\n\n\n\nNo\nNo\n729.5265\n44361.63\n0\n0.0012842\nNo\nNo\n\n\nNo\nYes\n808.6675\n17600.45\n0\n0.0019883\nNo\nNo\n\n\nNo\nYes\n1220.5838\n13268.56\n0\n0.0190870\nNo\nNo\n\n\nNo\nNo\n237.0451\n28251.70\n0\n0.0000843\nNo\nNo\n\n\nNo\nNo\n606.7423\n44994.56\n0\n0.0006514\nNo\nNo\n\n\nNo\nNo\n286.2326\n45042.41\n0\n0.0001107\nNo\nNo"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#using-a-threshold-2",
    "href": "slides/09-job-app-intro-roc.html#using-a-threshold-2",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Using a threshold",
    "text": "Using a threshold\n\nStep 1: Predict probabilities for all observations\nStep 2: Set a threshold to obtain class labels (0.5 below)\n\n\nthreshold &lt;- 0.5   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs &gt; threshold, \"Yes\", \"No\")))\n\ndefault_test_wprobs |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\nknn_probs\nlogistic_probs\nknn_preds\nlogistic_preds\n\n\n\n\nNo\nNo\n729.5265\n44361.63\n0\n0.0012842\nNo\nNo\n\n\nNo\nYes\n808.6675\n17600.45\n0\n0.0019883\nNo\nNo\n\n\nNo\nYes\n1220.5838\n13268.56\n0\n0.0190870\nNo\nNo\n\n\nNo\nNo\n237.0451\n28251.70\n0\n0.0000843\nNo\nNo\n\n\nNo\nNo\n606.7423\n44994.56\n0\n0.0006514\nNo\nNo\n\n\nNo\nNo\n286.2326\n45042.41\n0\n0.0001107\nNo\nNo"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#performance",
    "href": "slides/09-job-app-intro-roc.html#performance",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Performance",
    "text": "Performance\n\nroc_metrics &lt;- metric_set(accuracy, sensitivity, specificity)\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\") |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9717500\n\n\nsensitivity\nbinary\n0.3565891\n\n\nspecificity\nbinary\n0.9922501"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#low-threshold",
    "href": "slides/09-job-app-intro-roc.html#low-threshold",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Low Threshold",
    "text": "Low Threshold\n\nthreshold &lt;- 0.1   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\")))\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\")  |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9060000\n\n\nsensitivity\nbinary\n0.7364341\n\n\nspecificity\nbinary\n0.9116507"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#high-threshold",
    "href": "slides/09-job-app-intro-roc.html#high-threshold",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "High Threshold",
    "text": "High Threshold\n\nthreshold &lt;- 0.9   # set threshold\ndefault_test_wprobs &lt;- default_test_wprobs |&gt;\n  mutate(knn_preds = as_factor(if_else(knn_probs &gt; threshold, \"Yes\", \"No\"))\n  )\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\") |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9685000\n\n\nsensitivity\nbinary\n0.0310078\n\n\nspecificity\nbinary\n0.9997417"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#question",
    "href": "slides/09-job-app-intro-roc.html#question",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Question",
    "text": "Question\n\nIf I want to improve Recall/Sensitivity should I increase or decrease my threshold?\nIf I want to improve my Precision/PPV should I increase or decrease my threshold?"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#roc-curve-and-auc",
    "href": "slides/09-job-app-intro-roc.html#roc-curve-and-auc",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "ROC Curve and AUC",
    "text": "ROC Curve and AUC\n\nROC (Receiver Operating Characteristics) curve: popular graphic for comparing different classifiers across all possible thresholds\n\nPlots the (1-Specificity) along the x-axis and the Sensitivity (true positive rate) along the y-axis\n\nAUC: area under the AUC curve\n\nIdeal ROC curve will hug the top left corner\n\nIdea: How well is my classifier separating positives from negatives"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#roc-curve-1",
    "href": "slides/09-job-app-intro-roc.html#roc-curve-1",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |&gt;\n  head() |&gt;\n  kable()\n\n\n\n\n.threshold\nspecificity\nsensitivity\n\n\n\n\n-Inf\n0.0000000\n1.0000000\n\n\n0.0000\n0.0000000\n1.0000000\n\n\n0.0145\n0.8796177\n0.8217054\n\n\n0.0415\n0.8858176\n0.8139535\n\n\n0.0560\n0.8971842\n0.7829457\n\n\n0.0655\n0.8979592\n0.7751938"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#roc-curve-plot",
    "href": "slides/09-job-app-intro-roc.html#roc-curve-plot",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "ROC Curve: Plot",
    "text": "ROC Curve: Plot\n\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#auc",
    "href": "slides/09-job-app-intro-roc.html#auc",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "AUC",
    "text": "AUC\n\nAUC: Area under the curve (ROC Curve that is)\nMeasures how good your model is at separating categories\nOnly for binary classification"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#auc-in-r",
    "href": "slides/09-job-app-intro-roc.html#auc-in-r",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "AUC in R",
    "text": "AUC in R\n\nroc_auc(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |&gt;\n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.8757397"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#pathological-example-1",
    "href": "slides/09-job-app-intro-roc.html#pathological-example-1",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Pathological Example 1",
    "text": "Pathological Example 1"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#pathological-example-2",
    "href": "slides/09-job-app-intro-roc.html#pathological-example-2",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Pathological Example 2",
    "text": "Pathological Example 2"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#pathological-example-3",
    "href": "slides/09-job-app-intro-roc.html#pathological-example-3",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Pathological Example 3",
    "text": "Pathological Example 3"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#pathological-example-4",
    "href": "slides/09-job-app-intro-roc.html#pathological-example-4",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Pathological Example 4",
    "text": "Pathological Example 4"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#pathological-example-5",
    "href": "slides/09-job-app-intro-roc.html#pathological-example-5",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Pathological Example 5",
    "text": "Pathological Example 5"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#pathological-example-6",
    "href": "slides/09-job-app-intro-roc.html#pathological-example-6",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Pathological Example 6",
    "text": "Pathological Example 6"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#pathological-example-7",
    "href": "slides/09-job-app-intro-roc.html#pathological-example-7",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "Pathological Example 7",
    "text": "Pathological Example 7"
  },
  {
    "objectID": "slides/09-job-app-intro-roc.html#auc-questions",
    "href": "slides/09-job-app-intro-roc.html#auc-questions",
    "title": "MATH 427: ROC Curve and AUC",
    "section": "AUC Questions",
    "text": "AUC Questions\n\nWhat should be the minimum AUC?\nWhat should be that maximum possible AUC?\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/01-big-picture.html#five-tips-for-success",
    "href": "slides/01-big-picture.html#five-tips-for-success",
    "title": "MATH 427: The Big Picture",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions, come to office hours and help session.\nDo the homework; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/01-big-picture.html#emails-for-help",
    "href": "slides/01-big-picture.html#emails-for-help",
    "title": "MATH 427: The Big Picture",
    "section": "Emails for help",
    "text": "Emails for help\nIf you email me about an error please include a screenshot of the error and the code causing the error."
  },
  {
    "objectID": "slides/01-big-picture.html#what-is-machine-learning",
    "href": "slides/01-big-picture.html#what-is-machine-learning",
    "title": "MATH 427: The Big Picture",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nMachine Learning is the study of tools/techniques for extracting information and making predictions from complex datasets\nThe name machine learning was coined in 1959 by Arthur Samuel\n\n“Field of study that gives computers the ability to learn without being explicitly programmed”"
  },
  {
    "objectID": "slides/01-big-picture.html#what-is-machine-learning-1",
    "href": "slides/01-big-picture.html#what-is-machine-learning-1",
    "title": "MATH 427: The Big Picture",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\nTom M. Mitchell (1998):\n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "slides/01-big-picture.html#what-is-machine-learning-2",
    "href": "slides/01-big-picture.html#what-is-machine-learning-2",
    "title": "MATH 427: The Big Picture",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nMNIST handwritten digits (from ISLR, James et al.)"
  },
  {
    "objectID": "slides/01-big-picture.html#question",
    "href": "slides/01-big-picture.html#question",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\nSuppose your email program watches which emails you do or do not mark as spam, and based on that learns how to better filter spam. According to Tom Mitchell’s definition, which of the following is the task T, experience E, and performance measure P in this setting?\n\nP The number (or fraction) of emails correctly classified as spam/ham (not spam)\nT Classifying emails as spam or ham\nE Watching you label emails as spam or ham\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "slides/01-big-picture.html#statistical-learning-vs-machine-learning-vs-data-science",
    "href": "slides/01-big-picture.html#statistical-learning-vs-machine-learning-vs-data-science",
    "title": "MATH 427: The Big Picture",
    "section": "Statistical Learning vs Machine Learning vs Data Science",
    "text": "Statistical Learning vs Machine Learning vs Data Science\n\nMachine learning arose as a sub-field of Artificial Intelligence which is a sub-fields of Computer Science\nStatistical learning arose as a sub-field of Statistics\nThere is much overlap, a great deal of “cross-fertilization”\n“Data Science” - Reflects the fact that both statistical and machine learning are about data\n“Machine Learning” or “Data Science” are “fancier” terms"
  },
  {
    "objectID": "slides/01-big-picture.html#statistics-vs-machine-learning",
    "href": "slides/01-big-picture.html#statistics-vs-machine-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Statistics vs Machine Learning",
    "text": "Statistics vs Machine Learning\n\nStatistics: more concerned with answering why and how things work, making inferences\nMachine/Statistical learning: more concerned with making predictions"
  },
  {
    "objectID": "slides/01-big-picture.html#terminologynotation",
    "href": "slides/01-big-picture.html#terminologynotation",
    "title": "MATH 427: The Big Picture",
    "section": "Terminology/Notation",
    "text": "Terminology/Notation\nAmes Housing dataset - Contains data on 881 houses in Ames, IA. We are interested in predicting sale price.\nThe first ten observations are shown below.\n\n\n\n\n\nSale_Price\nGr_Liv_Area\nGarage_Type\nGarage_Cars\nGarage_Area\nStreet\nUtilities\nPool_Area\nNeighborhood\n\n\n\n\n244000\n2110\nAttchd\n2\n522\nPave\nAllPub\n0\nNorth_Ames\n\n\n213500\n1338\nAttchd\n2\n582\nPave\nAllPub\n0\nStone_Brook\n\n\n185000\n1187\nAttchd\n2\n420\nPave\nAllPub\n0\nGilbert\n\n\n394432\n1856\nAttchd\n3\n834\nPave\nAllPub\n0\nStone_Brook\n\n\n190000\n1844\nAttchd\n2\n546\nPave\nAllPub\n0\nNorthwest_Ames\n\n\n149000\nNA\nAttchd\n2\n480\nPave\nAllPub\n0\nNorth_Ames\n\n\n149900\nNA\nAttchd\n2\n500\nPave\nAllPub\n0\nNorth_Ames\n\n\n127500\n1069\nAttchd\n2\n440\nPave\nAllPub\n0\nNorthpark_Villa\n\n\n395192\n1940\nAttchd\n3\n606\nPave\nAllPub\n0\nNorthridge_Heights\n\n\n290941\n1544\nAttchd\n3\n868\nPave\nAllPub\n0\nNorthridge_Heights"
  },
  {
    "objectID": "slides/01-big-picture.html#terminologynotation-1",
    "href": "slides/01-big-picture.html#terminologynotation-1",
    "title": "MATH 427: The Big Picture",
    "section": "Terminology/Notation",
    "text": "Terminology/Notation\nDefault dataset - Contains credit card default data on 10,000 individuals. We are interested in predicting whether somebody will default or not.\nTen observations are shown below.\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n939.0985\n45519.02\n\n\nNo\nYes\n397.5425\n22710.87\n\n\nYes\nNo\n1511.6110\n53506.94\n\n\nNo\nNo\n301.3194\n51539.95\n\n\nNo\nNo\n878.4461\n29561.78\n\n\nYes\nNo\n1673.4863\n49310.33\n\n\nNo\nNo\n310.1302\n37697.22\n\n\nNo\nNo\n1272.0539\n44895.59\n\n\nNo\nNo\n887.2014\n41641.45\n\n\nNo\nNo\n230.8689\n32798.78"
  },
  {
    "objectID": "slides/01-big-picture.html#terminologynotation-2",
    "href": "slides/01-big-picture.html#terminologynotation-2",
    "title": "MATH 427: The Big Picture",
    "section": "Terminology/Notation",
    "text": "Terminology/Notation\n\nResponse/Target/Outcome - variable we are interested in predicting, denoted as \\(Y\\)\nFeatures/Inputs/Predictors - variables used to predict the response, denoted as \\(X\\)\nFeature Matrix - all features taken together, denoted as \\(\\mathbf{X}\\)\nNumber of data points/observations denoted as \\(n\\)\nNumber of features/inputs/predictors denotes as \\(p\\)\nMissing entries in R are denoted as NA"
  },
  {
    "objectID": "slides/01-big-picture.html#question-1",
    "href": "slides/01-big-picture.html#question-1",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\nFor the Ames Housing and Default datasets:\n\nQuestionsAnswers\n\n\n\nWhat are the corresponding values of \\(n\\) and \\(p\\)?\nWhat will be the dimension of the corresponding response vector \\(Y\\)?\nWhat is the value of the 3rd feature for the 2nd observation?\n\n\n\n\nWhat are the corresponding values of \\(n\\) and \\(p\\)?\n\nAmes: \\(n = 881\\) and \\(p = 9\\)\nDefault: \\(n = 10000\\) and \\(p = 4\\)\n\nWhat will be the dimension of the corresponding response vector \\(Y\\)?\n\nAmes: \\(881\\times 1\\)\nDefault: \\(10000\\times 1\\)\n\nWhat is the value of the 3rd feature for the 2nd observation?\n\nAmes: Attchd\nDefault: 397.5425"
  },
  {
    "objectID": "slides/01-big-picture.html#question-2",
    "href": "slides/01-big-picture.html#question-2",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\nSuppose you have information about 867 cancer patients on their age, tumor size, clump thickness of the tumor, uniformity of cell size, and whether the tumor is malignant or benign. Based on these data, you are interested in building a model to predict the type of tumor (malignant or benign) for future cancer patients.\n\nWhat are the values of \\(n\\) and \\(p\\) in this dataset? \\(n = 867, p = 5\\)\nWhat are the inputs/features?"
  },
  {
    "objectID": "slides/01-big-picture.html#supervised-vs-unsupervised-learning",
    "href": "slides/01-big-picture.html#supervised-vs-unsupervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Supervised vs Unsupervised Learning",
    "text": "Supervised vs Unsupervised Learning\n\nMachine Learning Tasks (from Bunker and Fayez, 2017)"
  },
  {
    "objectID": "slides/01-big-picture.html#supervised-learning",
    "href": "slides/01-big-picture.html#supervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nWe have access to labeled data\nObjective: learn overall pattern of relationship between the inputs (\\(\\mathbf{X}\\)) and response (\\(Y\\)) in order to\n\nInvestigate the relationship between inputs and response\nPredict for potential unseen test cases\nAssess the quality of predictions"
  },
  {
    "objectID": "slides/01-big-picture.html#types-of-supervised-learning",
    "href": "slides/01-big-picture.html#types-of-supervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Types of Supervised Learning",
    "text": "Types of Supervised Learning\nSupervised Learning problems can be categorized into:\n\n\nRegression problems (response is quantitative, continuous)\nClassification problems (response is qualitative, categorical)\nWhat if I have discrete quantitative data (e.g. counts)?\n\nCan use either but one is probably better…\nHow many discrete values are there?\n\nEnough we can think of the response as continuous?\n\nWhat is our goal?\n\nHow important is being exactly correct?"
  },
  {
    "objectID": "slides/01-big-picture.html#unsupervised-learning",
    "href": "slides/01-big-picture.html#unsupervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nNo response/outcome variable, just \\(\\mathbf{X}\\)\nUnderstand structure within data\n\nfind similar groups of observations based on features (clustering)\nfind a smaller subset of features with the most variation (dimensionality reduction)\n\nNo gold-standard\nEasier to collect unlabeled data\nUseful pre-processing step for supervised learning"
  },
  {
    "objectID": "slides/01-big-picture.html#unsupervised-learning-1",
    "href": "slides/01-big-picture.html#unsupervised-learning-1",
    "title": "MATH 427: The Big Picture",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nUS Arrests dataset - Data on arrests for 50 US states.\nThe first ten observations are shown below.\n\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\nAlabama\n13.2\n236\n58\n21.2\n\n\nAlaska\n10.0\n263\n48\n44.5\n\n\nArizona\n8.1\n294\n80\n31.0\n\n\nArkansas\n8.8\n190\n50\n19.5\n\n\nCalifornia\n9.0\n276\n91\n40.6\n\n\nColorado\n7.9\n204\n78\n38.7\n\n\nConnecticut\n3.3\n110\n77\n11.1\n\n\nDelaware\n5.9\n238\n72\n15.8\n\n\nFlorida\n15.4\n335\n80\n31.9\n\n\nGeorgia\n17.4\n211\n60\n25.8"
  },
  {
    "objectID": "slides/01-big-picture.html#question-3",
    "href": "slides/01-big-picture.html#question-3",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\n\nFor each of the following, identify whether the problem belongs to the supervised or unsupervised learning paradigm\n\nExamine the statistics of two football teams, and predict which team will win tomorrow’s match (given historical data of teams’ wins/losses to learn from) supervised\nGiven genetic (DNA) data from a person, predict the probability of the person developing diabetes over the next 10 years supervised\nTake a collection of 1000 essays written on the US economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow “similar” or “related” unsupervised\nExamine data on the income and years of education of adults in a neighborhood and build a model to predict the income from years of education supervised"
  },
  {
    "objectID": "slides/01-big-picture.html#recap",
    "href": "slides/01-big-picture.html#recap",
    "title": "MATH 427: The Big Picture",
    "section": "Recap",
    "text": "Recap\n\nWhat is a Machine Learning algorithm?\n\nT: has task\nP: performance is measured\nE: improves with experience\n\nTerminology:\n\nFeatures\n\n\\(p\\)\n\nTarget\nObservations\n\n\\(n\\)\n\n\nSupervised vs. unsupervised learning\n\nSupervised: data is labeled\nUnsupervised: data is unlabeled\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/27-more-classification.html#computational-set-up",
    "href": "slides/27-more-classification.html#computational-set-up",
    "title": "MATH 427: More on Classification",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rpart.plot)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/27-more-classification.html#quick-review-of-classification",
    "href": "slides/27-more-classification.html#quick-review-of-classification",
    "title": "MATH 427: More on Classification",
    "section": "Quick Review of Classification",
    "text": "Quick Review of Classification\n\nWe’ve covered many different classification methods:\n\nLogistic Regression (potentially with regularization)\nKNN\nDecision Tress\nRandom Forests\nGradient Boosted Trees\n\nFor each of these methods, rank them as (high/medium/low) for each of the following criteria\n\nComputational complexity to fit\nComputational complexity to predict\nAbility to handle non-linearity\nInterpretability\nPrediction accuracy"
  },
  {
    "objectID": "slides/27-more-classification.html#quick-review-of-classification-1",
    "href": "slides/27-more-classification.html#quick-review-of-classification-1",
    "title": "MATH 427: More on Classification",
    "section": "Quick Review of Classification",
    "text": "Quick Review of Classification\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\nKNN\nDecision Trees\nRandom Forests\nGradient Boosted Trees\n\n\n\n\nComp. Fit\nLow\nLow\nLow\nMedium\nHigh\n\n\nComp. Pred.\nLow\nDepends\nLow\nMedium\nMedium\n\n\nNon-linearity\nLow\nMedium\nMedium\nHigh\nHigh\n\n\nInterp.\nHigh\nLow\nHigh\nMedium/Low\nMedium/Low\n\n\nAcc.\nLow\nDepends\nLow\nMedium/High\nHigh"
  },
  {
    "objectID": "slides/27-more-classification.html#data-voter-frequency",
    "href": "slides/27-more-classification.html#data-voter-frequency",
    "title": "MATH 427: More on Classification",
    "section": "Data: Voter Frequency",
    "text": "Data: Voter Frequency\n\nInfo about data\nGoal: Identify individuals who are unlikely to vote to help organization target “get out the vote” effort.\n\n\nvoter_data &lt;- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nvoter_clean &lt;- voter_data |&gt;\n  select(-RespId, -weight, -Q1) |&gt;\n  mutate(\n    educ = factor(educ, levels = c(\"High school or less\", \"Some college\", \"College\")),\n    income_cat = factor(income_cat, levels = c(\"Less than $40k\", \"$40-75k \",\n                                               \"$75-125k\", \"$125k or more\")),\n    voter_category = factor(voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n  ) |&gt;\n  filter(Q22 != 5 | is.na(Q22)) |&gt;\n  mutate(Q22 = as_factor(Q22),\n         Q22 = if_else(is.na(Q22), \"Not Asked\", Q22),\n         across(Q28_1:Q28_8, ~if_else(.x == -1, 0, .x)),\n         across(Q28_1:Q28_8, ~ as_factor(.x)),\n         across(Q28_1:Q28_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n         across(Q29_1:Q29_10, ~if_else(.x == -1, 0, .x)),\n         across(Q29_1:Q29_10, ~ as_factor(.x)),\n         across(Q29_1:Q29_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n        Party_ID = as_factor(case_when(\n          Q31 == 1 ~ \"Strong Republican\",\n          Q31 == 2 ~ \"Republican\",\n          Q32 == 1  ~ \"Strong Democrat\",\n          Q32 == 2 ~ \"Democrat\",\n          Q33 == 1 ~ \"Lean Republican\",\n          Q33 == 2 ~ \"Lean Democrat\",\n          TRUE ~ \"Other\"\n        )),\n        Party_ID = factor(Party_ID, levels =c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n                                                \"Other\", \"Lean Democrat\", \"Democrat\", \"Strong Democrat\")),\n        across(!ppage, ~as_factor(if_else(.x == -1, NA, .x))))"
  },
  {
    "objectID": "slides/27-more-classification.html#split-data",
    "href": "slides/27-more-classification.html#split-data",
    "title": "MATH 427: More on Classification",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\n\nvoter_splits &lt;- initial_split(voter_clean, prop = 0.7, strata = voter_category)\nvoter_train &lt;- training(voter_splits)\nvoter_test &lt;- testing(voter_splits)"
  },
  {
    "objectID": "slides/27-more-classification.html#problem-more-than-two-categories",
    "href": "slides/27-more-classification.html#problem-more-than-two-categories",
    "title": "MATH 427: More on Classification",
    "section": "Problem: More than two categories",
    "text": "Problem: More than two categories\n\nvoter_train |&gt; \n  ggplot(aes(x = voter_category)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/27-more-classification.html#define-model-multinomial-regression",
    "href": "slides/27-more-classification.html#define-model-multinomial-regression",
    "title": "MATH 427: More on Classification",
    "section": "Define Model: Multinomial Regression",
    "text": "Define Model: Multinomial Regression\n\nmn_reg_model &lt;- multinom_reg(mixture = 1, penalty = 0.005) |&gt; # I chose this penalty arbitrarily\n  set_engine(\"glmnet\", family = \"multinomial\") |&gt; \n  set_mode(\"classification\")"
  },
  {
    "objectID": "slides/27-more-classification.html#define-recipe",
    "href": "slides/27-more-classification.html#define-recipe",
    "title": "MATH 427: More on Classification",
    "section": "Define Recipe",
    "text": "Define Recipe\n\nmr_recipe &lt;- recipe(voter_category ~ . , data = voter_train) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |&gt;\n  step_impute_median(all_numeric_predictors()) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/27-more-classification.html#define-workflow-and-fit",
    "href": "slides/27-more-classification.html#define-workflow-and-fit",
    "title": "MATH 427: More on Classification",
    "section": "Define Workflow and Fit",
    "text": "Define Workflow and Fit\n\nmr_fit &lt;- workflow() |&gt;\n  add_model(mn_reg_model) |&gt;\n  add_recipe(mr_recipe) |&gt; \n  fit(voter_train)"
  },
  {
    "objectID": "slides/27-more-classification.html#look-at-predictions",
    "href": "slides/27-more-classification.html#look-at-predictions",
    "title": "MATH 427: More on Classification",
    "section": "Look at Predictions",
    "text": "Look at Predictions\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; slice_sample(n=10) |&gt; select(1:4) |&gt; head() |&gt;  kable()\n\n\n\n\n.pred_class\n.pred_rarely/never\n.pred_sporadic\n.pred_always\n\n\n\n\nsporadic\n0.0317402\n0.5122327\n0.4560271\n\n\nalways\n0.0463060\n0.4464935\n0.5072005\n\n\nsporadic\n0.0553279\n0.6529393\n0.2917328\n\n\nalways\n0.0388506\n0.4351109\n0.5260385\n\n\nalways\n0.0361359\n0.4164662\n0.5473979\n\n\nalways\n0.0991499\n0.4310622\n0.4697879"
  },
  {
    "objectID": "slides/27-more-classification.html#confusion-matrix",
    "href": "slides/27-more-classification.html#confusion-matrix",
    "title": "MATH 427: More on Classification",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  conf_mat(truth = voter_category, estimate = .pred_class) |&gt; autoplot(\"heatmap\")"
  },
  {
    "objectID": "slides/27-more-classification.html#evaluating-multiclass-models",
    "href": "slides/27-more-classification.html#evaluating-multiclass-models",
    "title": "MATH 427: More on Classification",
    "section": "Evaluating Multiclass Models",
    "text": "Evaluating Multiclass Models\n\nNo “Positive” and “Negative” anymore\nAccuracy: \\[\\frac{\\text{Correct classifications}}{\\text{total observations}} = \\frac{285+559+221}{1746} \\approx 0.61\\]\nMost of our metrics were based on having “Positive” vs. “Negative”\n\nPrecision/PPV\nRecall/Sensitivity\nSpecificity\nNPV\nROC/AUC"
  },
  {
    "objectID": "slides/27-more-classification.html#evaluating-multiclass-models-1",
    "href": "slides/27-more-classification.html#evaluating-multiclass-models-1",
    "title": "MATH 427: More on Classification",
    "section": "Evaluating Multiclass Models",
    "text": "Evaluating Multiclass Models\n\nSolution 1: Apply binary metrics to each class in turn\n\nE.g. report three different Recall values\n\nRarely: \\(\\frac{285}{285+110+35} = \\approx 0.66\\)\nSporadic: \\(\\frac{559}{47+559+173} = \\approx 0.72\\)\nAlways: \\(\\frac{221}{21+301+221} = \\approx 0.41\\)\n\n\nCompute the Precision for each class:\n\nReminder: Precision = TP/(TP+FP)"
  },
  {
    "objectID": "slides/27-more-classification.html#solution-1-in-r",
    "href": "slides/27-more-classification.html#solution-1-in-r",
    "title": "MATH 427: More on Classification",
    "section": "Solution 1 in R",
    "text": "Solution 1 in R\n\nNo automatic implementation in yardstick (can hack together using group_by sometimes)\n\n\n\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  group_by(voter_category) |&gt; \n  recall(truth = voter_category, estimate = .pred_class) |&gt; \n  kable()\n\n\n\n\nvoter_category\n.metric\n.estimator\n.estimate\n\n\n\n\nrarely/never\nrecall\nmacro\n0.6627907\n\n\nsporadic\nrecall\nmacro\n0.7231565\n\n\nalways\nrecall\nmacro\n0.4069982\n\n\n\n\n\n\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  group_by(voter_category) |&gt; \n  recall(truth = voter_category, estimate = .pred_class) |&gt; \n  kable()\n\n\n\n\nvoter_category\n.metric\n.estimator\n.estimate\n\n\n\n\nrarely/never\nrecall\nmacro\n0.6627907\n\n\nsporadic\nrecall\nmacro\n0.7231565\n\n\nalways\nrecall\nmacro\n0.4069982"
  },
  {
    "objectID": "slides/27-more-classification.html#evaluating-multiclass-models-2",
    "href": "slides/27-more-classification.html#evaluating-multiclass-models-2",
    "title": "MATH 427: More on Classification",
    "section": "Evaluating Multiclass Models",
    "text": "Evaluating Multiclass Models\n\nSolution 2: Average metrics across labels\n\nMacro-averaging average one-versus-all metrics\n\nRecall: \\(\\frac{0.66+0.72+0.41}{3} \\approx 0.60\\)\n\nMacro-weighted averaging same but weight by class size\n\nRecall: \\(\\frac{430\\times 0.66+773\\times 0.72+543\\times 0.41}{1746} \\approx 0.61\\)\n\nMicro-averaging compute contribution for each class, aggregates them, then computes a single metric\n\nRecall: \\(\\frac{285+559+221}{430+779+543} \\approx 0.61\\)"
  },
  {
    "objectID": "slides/27-more-classification.html#macro-averaging-in-r",
    "href": "slides/27-more-classification.html#macro-averaging-in-r",
    "title": "MATH 427: More on Classification",
    "section": "Macro-Averaging in R",
    "text": "Macro-Averaging in R\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  recall(truth = voter_category, estimate = .pred_class, estimator = \"macro\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrecall\nmacro\n0.5976485"
  },
  {
    "objectID": "slides/27-more-classification.html#macro-weighted-averaging-in-r",
    "href": "slides/27-more-classification.html#macro-weighted-averaging-in-r",
    "title": "MATH 427: More on Classification",
    "section": "Macro-Weighted Averaging in R",
    "text": "Macro-Weighted Averaging in R\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  recall(truth = voter_category, estimate = .pred_class, estimator = \"macro_weighted\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrecall\nmacro_weighted\n0.6099656"
  },
  {
    "objectID": "slides/27-more-classification.html#micro-averaging-in-r",
    "href": "slides/27-more-classification.html#micro-averaging-in-r",
    "title": "MATH 427: More on Classification",
    "section": "Micro-Averaging in R",
    "text": "Micro-Averaging in R\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  recall(truth = voter_category, estimate = .pred_class, estimator = \"micro\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrecall\nmicro\n0.6099656"
  },
  {
    "objectID": "slides/27-more-classification.html#what-if-output-is-probability",
    "href": "slides/27-more-classification.html#what-if-output-is-probability",
    "title": "MATH 427: More on Classification",
    "section": "What if output is probability?",
    "text": "What if output is probability?\n\nFor binary case we used ROC curve and AUC…\nSimilar ideas apply here:\n\nOne vs. all\nMacro Averaging\nNO MICRO AVERAGING!\nHand and Till extension of AUC"
  },
  {
    "objectID": "slides/27-more-classification.html#plotting-one-vs.-all",
    "href": "slides/27-more-classification.html#plotting-one-vs.-all",
    "title": "MATH 427: More on Classification",
    "section": "Plotting one-vs.-all",
    "text": "Plotting one-vs.-all\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_curve(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always) |&gt; \n  autoplot()"
  },
  {
    "objectID": "slides/27-more-classification.html#macro-averaged-auc",
    "href": "slides/27-more-classification.html#macro-averaged-auc",
    "title": "MATH 427: More on Classification",
    "section": "Macro Averaged AUC",
    "text": "Macro Averaged AUC\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always, \n          estimator = \"macro\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nmacro\n0.7802495"
  },
  {
    "objectID": "slides/27-more-classification.html#macro-weighted-averaged-auc",
    "href": "slides/27-more-classification.html#macro-weighted-averaged-auc",
    "title": "MATH 427: More on Classification",
    "section": "Macro-Weighted Averaged AUC",
    "text": "Macro-Weighted Averaged AUC\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always, \n          estimator = \"macro_weighted\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nmacro_weighted\n0.7636223"
  },
  {
    "objectID": "slides/27-more-classification.html#hand-and-till-auc",
    "href": "slides/27-more-classification.html#hand-and-till-auc",
    "title": "MATH 427: More on Classification",
    "section": "Hand and Till AUC",
    "text": "Hand and Till AUC\n\nPaper\n\nBasic Idea: Do pairwise comparison of classes and average\n\n\n\nmr_fit |&gt; augment(new_data = voter_test) |&gt; \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always) |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nhand_till\n0.7967478"
  },
  {
    "objectID": "slides/27-more-classification.html#discussion",
    "href": "slides/27-more-classification.html#discussion",
    "title": "MATH 427: More on Classification",
    "section": "Discussion",
    "text": "Discussion\nHow would heavily imbalanced classes impact each type of macro vs. micro averaging?"
  },
  {
    "objectID": "slides/27-more-classification.html#class-imbalance",
    "href": "slides/27-more-classification.html#class-imbalance",
    "title": "MATH 427: More on Classification",
    "section": "Class-Imbalance",
    "text": "Class-Imbalance\n\nClass-imbalance occurs where your the classes in your response greatly differ in terms of how common they are\nOccurs frequently:\n\nMedicine: survival/death\nAdmission: enrollment/non-enrollment\nFinance: repaid loan/defaulted\nTech: Clicked on ad/Didn’t click\nTech: Churn rate\nFinance: Fraud"
  },
  {
    "objectID": "slides/27-more-classification.html#data-haberman",
    "href": "slides/27-more-classification.html#data-haberman",
    "title": "MATH 427: More on Classification",
    "section": "Data: haberman",
    "text": "Data: haberman\nStudy conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\nGoal: predict whether a patient survived after undergoing surgery for breast cancer.\n\nhaberman &lt;- read_csv(\"../data/haberman.data\",\n                     col_names = c(\"Age\", \"OpYear\", \"AxNodes\", \"Survival\"))\nglimpse(haberman)\n\nRows: 306\nColumns: 4\n$ Age      &lt;dbl&gt; 30, 30, 30, 31, 31, 33, 33, 34, 34, 34, 34, 34, 34, 34, 35, 3…\n$ OpYear   &lt;dbl&gt; 64, 62, 65, 59, 65, 58, 60, 59, 66, 58, 60, 61, 67, 60, 64, 6…\n$ AxNodes  &lt;dbl&gt; 1, 3, 0, 2, 4, 10, 0, 0, 9, 30, 1, 10, 7, 0, 13, 0, 1, 0, 0, …\n$ Survival &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…"
  },
  {
    "objectID": "slides/27-more-classification.html#quick-clean",
    "href": "slides/27-more-classification.html#quick-clean",
    "title": "MATH 427: More on Classification",
    "section": "Quick Clean",
    "text": "Quick Clean\nStudy conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\nGoal: predict whether a patient died after undergoing surgery for breast cancer.\n\nhaberman &lt;- haberman |&gt; \n  mutate(Survival = factor(if_else(Survival == 1, \"Survived\", \"Died\"),\n                           levels = c(\"Died\", \"Survived\")))\nglimpse(haberman)\n\nRows: 306\nColumns: 4\n$ Age      &lt;dbl&gt; 30, 30, 30, 31, 31, 33, 33, 34, 34, 34, 34, 34, 34, 34, 35, 3…\n$ OpYear   &lt;dbl&gt; 64, 62, 65, 59, 65, 58, 60, 59, 66, 58, 60, 61, 67, 60, 64, 6…\n$ AxNodes  &lt;dbl&gt; 1, 3, 0, 2, 4, 10, 0, 0, 9, 30, 1, 10, 7, 0, 13, 0, 1, 0, 0, …\n$ Survival &lt;fct&gt; Survived, Survived, Survived, Survived, Survived, Survived, S…"
  },
  {
    "objectID": "slides/27-more-classification.html#split-data-1",
    "href": "slides/27-more-classification.html#split-data-1",
    "title": "MATH 427: More on Classification",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\nhab_splits &lt;- initial_split(haberman, prop = 0.75, strata = Survival)\nhab_train &lt;- training(hab_splits)\nhab_test &lt;- testing(hab_splits)"
  },
  {
    "objectID": "slides/27-more-classification.html#visualizing-response",
    "href": "slides/27-more-classification.html#visualizing-response",
    "title": "MATH 427: More on Classification",
    "section": "Visualizing Response",
    "text": "Visualizing Response\n\nhab_train |&gt; \n  ggplot(aes(y = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/27-more-classification.html#fitting-model",
    "href": "slides/27-more-classification.html#fitting-model",
    "title": "MATH 427: More on Classification",
    "section": "Fitting Model",
    "text": "Fitting Model\n\nlr_fit &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(Survival ~ . , data = hab_train)"
  },
  {
    "objectID": "slides/27-more-classification.html#confusion-matrix-1",
    "href": "slides/27-more-classification.html#confusion-matrix-1",
    "title": "MATH 427: More on Classification",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  conf_mat(truth = Survival, estimate = .pred_class) |&gt; autoplot(\"heatmap\")"
  },
  {
    "objectID": "slides/27-more-classification.html#performance-metrics",
    "href": "slides/27-more-classification.html#performance-metrics",
    "title": "MATH 427: More on Classification",
    "section": "Performance Metrics",
    "text": "Performance Metrics\n\nhab_metrics &lt;- metric_set(accuracy, precision, recall)\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7284879\n\n\n\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7692308\n\n\nprecision\nbinary\n0.8000000\n\n\nrecall\nbinary\n0.1904762"
  },
  {
    "objectID": "slides/27-more-classification.html#recall-is-bad",
    "href": "slides/27-more-classification.html#recall-is-bad",
    "title": "MATH 427: More on Classification",
    "section": "Recall is BAD!",
    "text": "Recall is BAD!\n\nSince there are so few deaths, model always predicts a low probability of death\nIdea: just because you you have a HIGHER probability of death doesn’t mean have a HIGH probability of death"
  },
  {
    "objectID": "slides/27-more-classification.html#what-do-we-do",
    "href": "slides/27-more-classification.html#what-do-we-do",
    "title": "MATH 427: More on Classification",
    "section": "What do we do?",
    "text": "What do we do?\n\nDepends on what your goal is…\nAsk yourself: What is most important to my problem?\n\nAccurate probabilities?\nEffective Separation?\nEffective identification of positives?\nLow false-positive rate?\n\nDiscussion: Let’s think of scenarios where each one of these is the most important.\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/33-PCA-2.html#computational-set-up",
    "href": "slides/33-PCA-2.html#computational-set-up",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/33-PCA-2.html#data-mnist",
    "href": "slides/33-PCA-2.html#data-mnist",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Data: mnist",
    "text": "Data: mnist\n\nMNIST Database: Modified National Institute of Standards and Technology Database\nLarge database of handwritten digits\n\n60,000 training images\n10,000 test images\n\nEach image:\n\n28x28 black and white pixels\n\\(28\\times 28\\times 1 = 784\\)"
  },
  {
    "objectID": "slides/33-PCA-2.html#loading-data",
    "href": "slides/33-PCA-2.html#loading-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Loading data",
    "text": "Loading data\n\nlibrary(dslabs)\nmnist &lt;- read_mnist()\nmnist_train &lt;- mnist$train$images\nmnist_train |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n18\n18\n18\n126\n136\n175\n26\n166\n255\n247\n127\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n30\n36\n94\n154\n170\n253\n253\n253\n253\n253\n225\n172\n253\n242\n195\n64\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n49\n238\n253\n253\n253\n253\n253\n253\n253\n253\n251\n93\n82\n82\n56\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n219\n253\n253\n253\n253\n253\n198\n182\n247\n241\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n80\n156\n107\n253\n253\n205\n11\n0\n43\n154\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n1\n154\n253\n90\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n139\n253\n190\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n190\n253\n70\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n35\n241\n225\n160\n108\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n81\n240\n253\n253\n119\n25\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n45\n186\n253\n253\n150\n27\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n93\n252\n253\n187\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n249\n64\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n46\n130\n183\n253\n253\n207\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n39\n148\n229\n253\n253\n253\n250\n182\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n114\n221\n253\n253\n253\n253\n201\n78\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n23\n66\n213\n253\n253\n253\n253\n198\n81\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n171\n219\n253\n253\n253\n253\n195\n80\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n55\n172\n226\n253\n253\n253\n253\n244\n133\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n136\n253\n253\n253\n212\n135\n132\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n51\n159\n253\n159\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n238\n252\n252\n252\n237\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n54\n227\n253\n252\n239\n233\n252\n57\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n60\n224\n252\n253\n252\n202\n84\n252\n253\n122\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n163\n252\n252\n252\n253\n252\n252\n96\n189\n253\n167\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n51\n238\n253\n253\n190\n114\n253\n228\n47\n79\n255\n168\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n238\n252\n252\n179\n12\n75\n121\n21\n0\n0\n253\n243\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n38\n165\n253\n233\n208\n84\n0\n0\n0\n0\n0\n0\n253\n252\n165\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n7\n178\n252\n240\n71\n19\n28\n0\n0\n0\n0\n0\n0\n253\n252\n195\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n57\n252\n252\n63\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n252\n195\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n198\n253\n190\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n255\n253\n196\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n76\n246\n252\n112\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n252\n148\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n230\n25\n0\n0\n0\n0\n0\n0\n0\n0\n7\n135\n253\n186\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n223\n0\n0\n0\n0\n0\n0\n0\n0\n7\n131\n252\n225\n71\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n145\n0\n0\n0\n0\n0\n0\n0\n48\n165\n252\n173\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n86\n253\n225\n0\n0\n0\n0\n0\n0\n114\n238\n253\n162\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n249\n146\n48\n29\n85\n178\n225\n253\n223\n167\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n85\n252\n252\n252\n229\n215\n252\n252\n252\n196\n130\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n28\n199\n252\n252\n253\n252\n252\n233\n145\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n25\n128\n252\n253\n252\n141\n37\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n67\n232\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n62\n81\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n120\n180\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n126\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n153\n210\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n220\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n27\n254\n162\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n222\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n183\n254\n125\n0\n0\n0\n0\n0\n0\n0\n0\n0\n46\n245\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n198\n254\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n120\n254\n163\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n23\n231\n254\n29\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n254\n120\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n163\n254\n216\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n254\n67\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n86\n178\n248\n254\n91\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n254\n85\n0\n0\n0\n47\n49\n116\n144\n150\n241\n243\n234\n179\n241\n252\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n150\n253\n237\n207\n207\n207\n253\n254\n250\n240\n198\n143\n91\n28\n5\n233\n250\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n119\n177\n177\n177\n177\n177\n98\n56\n0\n0\n0\n0\n0\n102\n254\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n137\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n255\n94\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n96\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n254\n153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n169\n255\n153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n254\n153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n124\n253\n255\n63\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n244\n251\n253\n62\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n127\n251\n251\n253\n62\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n68\n236\n251\n211\n31\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n60\n228\n251\n251\n94\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n155\n253\n253\n189\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n20\n253\n251\n235\n66\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n32\n205\n253\n251\n126\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n104\n251\n253\n184\n15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n80\n240\n251\n193\n23\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n32\n253\n253\n253\n159\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n151\n251\n251\n251\n39\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n221\n251\n251\n172\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n234\n251\n251\n196\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n251\n251\n89\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n159\n255\n253\n253\n31\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n228\n253\n247\n140\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n64\n251\n253\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n64\n251\n253\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n193\n253\n220\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n55\n148\n210\n253\n253\n113\n87\n148\n55\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n87\n232\n252\n253\n189\n210\n252\n252\n253\n168\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n57\n242\n252\n190\n65\n5\n12\n182\n252\n253\n116\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n252\n252\n183\n14\n0\n0\n92\n252\n252\n225\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n132\n253\n252\n146\n14\n0\n0\n0\n215\n252\n252\n79\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n126\n253\n247\n176\n9\n0\n0\n8\n78\n245\n253\n129\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n232\n252\n176\n0\n0\n0\n36\n201\n252\n252\n169\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22\n252\n252\n30\n22\n119\n197\n241\n253\n252\n251\n77\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n231\n252\n253\n252\n252\n252\n226\n227\n252\n231\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n55\n235\n253\n217\n138\n42\n24\n192\n252\n143\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n62\n255\n253\n109\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n71\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n71\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n106\n253\n252\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n45\n255\n253\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n218\n252\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n252\n189\n42\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n184\n252\n170\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n147\n252\n42\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n25\n100\n122\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n33\n151\n208\n252\n252\n252\n146\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n40\n152\n244\n252\n253\n224\n211\n252\n232\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n15\n152\n239\n252\n252\n252\n216\n31\n37\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n96\n252\n252\n252\n252\n217\n29\n0\n37\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n181\n252\n252\n220\n167\n30\n0\n0\n77\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n26\n128\n58\n22\n0\n0\n0\n0\n100\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n157\n252\n252\n60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n110\n121\n122\n121\n202\n252\n194\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n53\n179\n253\n253\n255\n253\n253\n228\n35\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n54\n227\n252\n243\n228\n170\n242\n252\n252\n231\n117\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n78\n252\n252\n125\n59\n0\n18\n208\n252\n252\n252\n252\n87\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n135\n252\n252\n180\n16\n0\n21\n203\n253\n247\n129\n173\n252\n252\n184\n66\n49\n49\n0\n0\n0\n0\n0\n0\n0\n0\n3\n136\n252\n241\n106\n17\n0\n53\n200\n252\n216\n65\n0\n14\n72\n163\n241\n252\n252\n223\n0\n0\n0\n0\n0\n0\n0\n0\n105\n252\n242\n88\n18\n73\n170\n244\n252\n126\n29\n0\n0\n0\n0\n0\n89\n180\n180\n37\n0\n0\n0\n0\n0\n0\n0\n0\n231\n252\n245\n205\n216\n252\n252\n252\n124\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n207\n252\n252\n252\n252\n178\n116\n36\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n93\n143\n121\n23\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "slides/33-PCA-2.html#digits",
    "href": "slides/33-PCA-2.html#digits",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[1,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#digits-1",
    "href": "slides/33-PCA-2.html#digits-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[2,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#digits-2",
    "href": "slides/33-PCA-2.html#digits-2",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[3,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#digits-3",
    "href": "slides/33-PCA-2.html#digits-3",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Digits",
    "text": "Digits\n\n\nCode\nimage(x = 1:28, y = 1:28,\n      z = matrix(mnist_train[4,], nrow = 28, byrow=FALSE)[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#unsupervised-learning-dimensionality-reduction",
    "href": "slides/33-PCA-2.html#unsupervised-learning-dimensionality-reduction",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Unsupervised Learning & Dimensionality Reduction",
    "text": "Unsupervised Learning & Dimensionality Reduction\n\nUnsupervised Learning: ML for unlabeled data (i.e. no response variables)\n\nGoal: Uncover patterns/structure within data\nTasks:\n\nClustering: finding sub-groups within our data\nDimensionality Reduction: reducing the number of columns in our data set… why?"
  },
  {
    "objectID": "slides/33-PCA-2.html#dimensionality-reduction",
    "href": "slides/33-PCA-2.html#dimensionality-reduction",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nGoal phrasing 1: Reduce the number of columns, while losing as little information as possible\nGoal phrasing 2: Extract lower-dimensional structure from our data"
  },
  {
    "objectID": "slides/33-PCA-2.html#last-time",
    "href": "slides/33-PCA-2.html#last-time",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Last Time",
    "text": "Last Time\n\nVectors and Projects"
  },
  {
    "objectID": "slides/33-PCA-2.html#pca-vocabulary",
    "href": "slides/33-PCA-2.html#pca-vocabulary",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "PCA Vocabulary",
    "text": "PCA Vocabulary\n\nPrincipal Component (PC1): direction in \\(p\\)-dimensional space (e.g. \\(\\langle 1, 1, 2\\rangle\\))\nScores: our new variables (e.g. \\((-0.56\\times 1 + -0.996\\times 1 + -1.56\\times 2)/6 = -0.778\\))\nLoadings: For direction above’\n\nLoading on \\(x\\) is 1\nLoading on \\(y\\) is 1\nLoading on \\(z\\) is 2"
  },
  {
    "objectID": "slides/33-PCA-2.html#recall-variance",
    "href": "slides/33-PCA-2.html#recall-variance",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Recall: Variance",
    "text": "Recall: Variance\n\n\nWhat is variance?\nIntuitively: what does variance measure?\nVariance: \\(\\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar{x})^2\\)\n\nAverage of the squared distance from zero of each observation"
  },
  {
    "objectID": "slides/33-PCA-2.html#idea-behind-pca",
    "href": "slides/33-PCA-2.html#idea-behind-pca",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Idea behind PCA",
    "text": "Idea behind PCA\n\nSelect first PC so variance of scores is the maximum\nIteratively:\n\nSelect next PC so variance of scores is maximize AND new PC is orthogonal to all other PCs\n\n\n\n\nWhat does orthogonal mean?"
  },
  {
    "objectID": "slides/33-PCA-2.html#easy-example",
    "href": "slides/33-PCA-2.html#easy-example",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Easy Example",
    "text": "Easy Example\n\nExercise: What should the first and second PCs be?"
  },
  {
    "objectID": "slides/33-PCA-2.html#how-much-variance-is-explained-by-each-of-the-pcs",
    "href": "slides/33-PCA-2.html#how-much-variance-is-explained-by-each-of-the-pcs",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "How much variance is explained by each of the PC’s?",
    "text": "How much variance is explained by each of the PC’s?\n\nvar_exp &lt;- easy_ex |&gt; \n  mutate(PC1 = x,\n         PC2 = y) |&gt; \n  summarize(var1 = var(PC1), \n            var2 = var(PC2))\n\nvar_exp |&gt; kable()\n\n\n\n\nvar1\nvar2\n\n\n\n\n0.9834589\n0.0637151"
  },
  {
    "objectID": "slides/33-PCA-2.html#what-proportion-of-variance-is-explained-by-each-of-the-pcs",
    "href": "slides/33-PCA-2.html#what-proportion-of-variance-is-explained-by-each-of-the-pcs",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "What proportion of variance is explained by each of the PC’s?",
    "text": "What proportion of variance is explained by each of the PC’s?\n\nvar_exp |&gt; \n  pivot_longer(everything()) |&gt; \n  mutate(proportion = value/sum(value)) |&gt; \n  kable()\n\n\n\n\nname\nvalue\nproportion\n\n\n\n\nvar1\n0.9834589\n0.9391552\n\n\nvar2\n0.0637151\n0.0608448\n\n\n\n\n\n\n93% of our variance (information) is contained in our first PC"
  },
  {
    "objectID": "slides/33-PCA-2.html#harder-example",
    "href": "slides/33-PCA-2.html#harder-example",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Harder Example",
    "text": "Harder Example\n\nExercise: What should the first and second PCs be?"
  },
  {
    "objectID": "slides/33-PCA-2.html#how-much-variance-is-explained-by-each-of-the-pcs-1",
    "href": "slides/33-PCA-2.html#how-much-variance-is-explained-by-each-of-the-pcs-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "How much variance is explained by each of the PC’s?",
    "text": "How much variance is explained by each of the PC’s?\n\nvar_exp &lt;- harder_ex |&gt; \n  mutate(PC1 = (x + y)/2,\n         PC2 = (x-y)/2) |&gt; \n  summarize(var1 = var(PC1), \n            var2 = var(PC2))\n\nvar_exp |&gt; kable()\n\n\n\n\nvar1\nvar2\n\n\n\n\n1.021035\n0.0159288"
  },
  {
    "objectID": "slides/33-PCA-2.html#question",
    "href": "slides/33-PCA-2.html#question",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Question?",
    "text": "Question?\n\n\nWill our PCs depend on the scale of our data?\nWhat does this mean we should do?"
  },
  {
    "objectID": "slides/33-PCA-2.html#how-is-pca-actually-accomplished",
    "href": "slides/33-PCA-2.html#how-is-pca-actually-accomplished",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "How is PCA actually accomplished",
    "text": "How is PCA actually accomplished\n\nCompute Variance-Covariance Matrix\nCompute Eigen-decomposition of variance-covariance matrix\n\nEigen-vectors = Principal components\nEigen-values = Variance explained by corresponding component"
  },
  {
    "objectID": "slides/33-PCA-2.html#recipe",
    "href": "slides/33-PCA-2.html#recipe",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Recipe",
    "text": "Recipe\n\nDocumentation for step_normalize\n\n\npca_recipe &lt;- recipe(~ ., data = mnist_train) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_pca(all_numeric_predictors(), num_comp = 20)"
  },
  {
    "objectID": "slides/33-PCA-2.html#prepping-and-baking-recipes",
    "href": "slides/33-PCA-2.html#prepping-and-baking-recipes",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Prepping and Baking recipes",
    "text": "Prepping and Baking recipes\n\nIf we want to apply a recipe to a data set outside of a workflow:\n\nFirst, prep using training set: this estimates any necessary quantities\nSecond, bake this applies the recipe to a new data set (in our case the new data set in the same data set)"
  },
  {
    "objectID": "slides/33-PCA-2.html#applying-recipe-to-training-data",
    "href": "slides/33-PCA-2.html#applying-recipe-to-training-data",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Applying recipe to training data",
    "text": "Applying recipe to training data\n\nprepped_pca &lt;- pca_recipe |&gt; prep(mnist_train)\nmnist_pca &lt;- prepped_pca |&gt; bake(new_data = mnist_train)"
  },
  {
    "objectID": "slides/33-PCA-2.html#using-the-results",
    "href": "slides/33-PCA-2.html#using-the-results",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Using the results",
    "text": "Using the results\n\nThree things of interest\n\nNew scores: this is our new data… use for plotting and analysis\nPCs: these are our new directions… by looking at loadings we can interpret structure\nEigenvalues: this will tell us how much information is lost"
  },
  {
    "objectID": "slides/33-PCA-2.html#how-many-pcs-should-we-use",
    "href": "slides/33-PCA-2.html#how-many-pcs-should-we-use",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "How many PCs Should We Use?",
    "text": "How many PCs Should We Use?\n\n\n\nprepped_pca |&gt; \n  tidy(2, type = \"variance\") |&gt; \n  filter(terms == \"variance\") |&gt; \n  kable()\n\n\n\n\nterms\nvalue\ncomponent\nid\n\n\n\n\nvariance\n40.4869603\n1\npca_XZ5Kg\n\n\nvariance\n29.2412102\n2\npca_XZ5Kg\n\n\nvariance\n26.8113576\n3\npca_XZ5Kg\n\n\nvariance\n20.6862734\n4\npca_XZ5Kg\n\n\nvariance\n18.0763488\n5\npca_XZ5Kg\n\n\nvariance\n15.7329156\n6\npca_XZ5Kg\n\n\nvariance\n13.7903793\n7\npca_XZ5Kg\n\n\nvariance\n12.5173805\n8\npca_XZ5Kg\n\n\nvariance\n11.0066118\n9\npca_XZ5Kg\n\n\nvariance\n10.0503295\n10\npca_XZ5Kg\n\n\nvariance\n9.6202975\n11\npca_XZ5Kg\n\n\nvariance\n8.6308297\n12\npca_XZ5Kg\n\n\nvariance\n7.9914637\n13\npca_XZ5Kg\n\n\nvariance\n7.8147519\n14\npca_XZ5Kg\n\n\nvariance\n7.3754149\n15\npca_XZ5Kg\n\n\nvariance\n7.1304687\n16\npca_XZ5Kg\n\n\nvariance\n6.7138681\n17\npca_XZ5Kg\n\n\nvariance\n6.6038974\n18\npca_XZ5Kg\n\n\nvariance\n6.4059417\n19\npca_XZ5Kg\n\n\nvariance\n6.2372735\n20\npca_XZ5Kg\n\n\nvariance\n5.9321928\n21\npca_XZ5Kg\n\n\nvariance\n5.7605025\n22\npca_XZ5Kg\n\n\nvariance\n5.4839422\n23\npca_XZ5Kg\n\n\nvariance\n5.3185086\n24\npca_XZ5Kg\n\n\nvariance\n5.1286499\n25\npca_XZ5Kg\n\n\nvariance\n4.9605418\n26\npca_XZ5Kg\n\n\nvariance\n4.9052549\n27\npca_XZ5Kg\n\n\nvariance\n4.7083565\n28\npca_XZ5Kg\n\n\nvariance\n4.5291221\n29\npca_XZ5Kg\n\n\nvariance\n4.3946352\n30\npca_XZ5Kg\n\n\nvariance\n4.2751505\n31\npca_XZ5Kg\n\n\nvariance\n4.2139267\n32\npca_XZ5Kg\n\n\nvariance\n4.0983125\n33\npca_XZ5Kg\n\n\nvariance\n4.0317442\n34\npca_XZ5Kg\n\n\nvariance\n3.9770700\n35\npca_XZ5Kg\n\n\nvariance\n3.8604597\n36\npca_XZ5Kg\n\n\nvariance\n3.8085767\n37\npca_XZ5Kg\n\n\nvariance\n3.7255722\n38\npca_XZ5Kg\n\n\nvariance\n3.6438747\n39\npca_XZ5Kg\n\n\nvariance\n3.4416399\n40\npca_XZ5Kg\n\n\nvariance\n3.4161882\n41\npca_XZ5Kg\n\n\nvariance\n3.3637292\n42\npca_XZ5Kg\n\n\nvariance\n3.2576820\n43\npca_XZ5Kg\n\n\nvariance\n3.2361493\n44\npca_XZ5Kg\n\n\nvariance\n3.2047276\n45\npca_XZ5Kg\n\n\nvariance\n3.1790572\n46\npca_XZ5Kg\n\n\nvariance\n3.1420049\n47\npca_XZ5Kg\n\n\nvariance\n3.0858372\n48\npca_XZ5Kg\n\n\nvariance\n3.0607145\n49\npca_XZ5Kg\n\n\nvariance\n3.0375491\n50\npca_XZ5Kg\n\n\nvariance\n2.9016712\n51\npca_XZ5Kg\n\n\nvariance\n2.8640379\n52\npca_XZ5Kg\n\n\nvariance\n2.8497604\n53\npca_XZ5Kg\n\n\nvariance\n2.8236951\n54\npca_XZ5Kg\n\n\nvariance\n2.7662834\n55\npca_XZ5Kg\n\n\nvariance\n2.7177360\n56\npca_XZ5Kg\n\n\nvariance\n2.6916372\n57\npca_XZ5Kg\n\n\nvariance\n2.6584628\n58\npca_XZ5Kg\n\n\nvariance\n2.6166512\n59\npca_XZ5Kg\n\n\nvariance\n2.5761851\n60\npca_XZ5Kg\n\n\nvariance\n2.5265801\n61\npca_XZ5Kg\n\n\nvariance\n2.4936804\n62\npca_XZ5Kg\n\n\nvariance\n2.4694291\n63\npca_XZ5Kg\n\n\nvariance\n2.4368544\n64\npca_XZ5Kg\n\n\nvariance\n2.4087941\n65\npca_XZ5Kg\n\n\nvariance\n2.4011323\n66\npca_XZ5Kg\n\n\nvariance\n2.3794627\n67\npca_XZ5Kg\n\n\nvariance\n2.3160961\n68\npca_XZ5Kg\n\n\nvariance\n2.2677059\n69\npca_XZ5Kg\n\n\nvariance\n2.2459613\n70\npca_XZ5Kg\n\n\nvariance\n2.2279444\n71\npca_XZ5Kg\n\n\nvariance\n2.2029316\n72\npca_XZ5Kg\n\n\nvariance\n2.1862348\n73\npca_XZ5Kg\n\n\nvariance\n2.1704822\n74\npca_XZ5Kg\n\n\nvariance\n2.1473098\n75\npca_XZ5Kg\n\n\nvariance\n2.1349476\n76\npca_XZ5Kg\n\n\nvariance\n2.1155197\n77\npca_XZ5Kg\n\n\nvariance\n2.0824414\n78\npca_XZ5Kg\n\n\nvariance\n2.0567589\n79\npca_XZ5Kg\n\n\nvariance\n2.0483094\n80\npca_XZ5Kg\n\n\nvariance\n2.0319621\n81\npca_XZ5Kg\n\n\nvariance\n2.0264366\n82\npca_XZ5Kg\n\n\nvariance\n2.0043811\n83\npca_XZ5Kg\n\n\nvariance\n2.0026194\n84\npca_XZ5Kg\n\n\nvariance\n1.9969826\n85\npca_XZ5Kg\n\n\nvariance\n1.9893557\n86\npca_XZ5Kg\n\n\nvariance\n1.9782088\n87\npca_XZ5Kg\n\n\nvariance\n1.9662099\n88\npca_XZ5Kg\n\n\nvariance\n1.9460140\n89\npca_XZ5Kg\n\n\nvariance\n1.9306153\n90\npca_XZ5Kg\n\n\nvariance\n1.9106927\n91\npca_XZ5Kg\n\n\nvariance\n1.8898779\n92\npca_XZ5Kg\n\n\nvariance\n1.8854346\n93\npca_XZ5Kg\n\n\nvariance\n1.8716125\n94\npca_XZ5Kg\n\n\nvariance\n1.8557891\n95\npca_XZ5Kg\n\n\nvariance\n1.8367849\n96\npca_XZ5Kg\n\n\nvariance\n1.8200748\n97\npca_XZ5Kg\n\n\nvariance\n1.7957069\n98\npca_XZ5Kg\n\n\nvariance\n1.7769367\n99\npca_XZ5Kg\n\n\nvariance\n1.7568906\n100\npca_XZ5Kg\n\n\nvariance\n1.7376257\n101\npca_XZ5Kg\n\n\nvariance\n1.7355974\n102\npca_XZ5Kg\n\n\nvariance\n1.7127369\n103\npca_XZ5Kg\n\n\nvariance\n1.7025502\n104\npca_XZ5Kg\n\n\nvariance\n1.6893123\n105\npca_XZ5Kg\n\n\nvariance\n1.6709916\n106\npca_XZ5Kg\n\n\nvariance\n1.6346171\n107\npca_XZ5Kg\n\n\nvariance\n1.6216691\n108\npca_XZ5Kg\n\n\nvariance\n1.5977021\n109\npca_XZ5Kg\n\n\nvariance\n1.5949101\n110\npca_XZ5Kg\n\n\nvariance\n1.5642744\n111\npca_XZ5Kg\n\n\nvariance\n1.5577313\n112\npca_XZ5Kg\n\n\nvariance\n1.5363646\n113\npca_XZ5Kg\n\n\nvariance\n1.5195934\n114\npca_XZ5Kg\n\n\nvariance\n1.5126694\n115\npca_XZ5Kg\n\n\nvariance\n1.4865576\n116\npca_XZ5Kg\n\n\nvariance\n1.4681373\n117\npca_XZ5Kg\n\n\nvariance\n1.4653200\n118\npca_XZ5Kg\n\n\nvariance\n1.4512690\n119\npca_XZ5Kg\n\n\nvariance\n1.4373137\n120\npca_XZ5Kg\n\n\nvariance\n1.4255536\n121\npca_XZ5Kg\n\n\nvariance\n1.3996957\n122\npca_XZ5Kg\n\n\nvariance\n1.3890965\n123\npca_XZ5Kg\n\n\nvariance\n1.3773802\n124\npca_XZ5Kg\n\n\nvariance\n1.3746028\n125\npca_XZ5Kg\n\n\nvariance\n1.3608811\n126\npca_XZ5Kg\n\n\nvariance\n1.3414316\n127\npca_XZ5Kg\n\n\nvariance\n1.3374608\n128\npca_XZ5Kg\n\n\nvariance\n1.3000642\n129\npca_XZ5Kg\n\n\nvariance\n1.2906378\n130\npca_XZ5Kg\n\n\nvariance\n1.2848213\n131\npca_XZ5Kg\n\n\nvariance\n1.2832380\n132\npca_XZ5Kg\n\n\nvariance\n1.2687347\n133\npca_XZ5Kg\n\n\nvariance\n1.2630508\n134\npca_XZ5Kg\n\n\nvariance\n1.2532928\n135\npca_XZ5Kg\n\n\nvariance\n1.2403005\n136\npca_XZ5Kg\n\n\nvariance\n1.2333593\n137\npca_XZ5Kg\n\n\nvariance\n1.2097735\n138\npca_XZ5Kg\n\n\nvariance\n1.2082668\n139\npca_XZ5Kg\n\n\nvariance\n1.1962536\n140\npca_XZ5Kg\n\n\nvariance\n1.1810312\n141\npca_XZ5Kg\n\n\nvariance\n1.1800024\n142\npca_XZ5Kg\n\n\nvariance\n1.1779849\n143\npca_XZ5Kg\n\n\nvariance\n1.1578531\n144\npca_XZ5Kg\n\n\nvariance\n1.1507152\n145\npca_XZ5Kg\n\n\nvariance\n1.1394010\n146\npca_XZ5Kg\n\n\nvariance\n1.1291999\n147\npca_XZ5Kg\n\n\nvariance\n1.1179302\n148\npca_XZ5Kg\n\n\nvariance\n1.1159241\n149\npca_XZ5Kg\n\n\nvariance\n1.1089527\n150\npca_XZ5Kg\n\n\nvariance\n1.1044876\n151\npca_XZ5Kg\n\n\nvariance\n1.0870054\n152\npca_XZ5Kg\n\n\nvariance\n1.0774471\n153\npca_XZ5Kg\n\n\nvariance\n1.0666133\n154\npca_XZ5Kg\n\n\nvariance\n1.0576139\n155\npca_XZ5Kg\n\n\nvariance\n1.0527006\n156\npca_XZ5Kg\n\n\nvariance\n1.0454060\n157\npca_XZ5Kg\n\n\nvariance\n1.0437201\n158\npca_XZ5Kg\n\n\nvariance\n1.0377631\n159\npca_XZ5Kg\n\n\nvariance\n1.0245582\n160\npca_XZ5Kg\n\n\nvariance\n1.0113845\n161\npca_XZ5Kg\n\n\nvariance\n1.0033598\n162\npca_XZ5Kg\n\n\nvariance\n1.0017151\n163\npca_XZ5Kg\n\n\nvariance\n1.0004519\n164\npca_XZ5Kg\n\n\nvariance\n0.9991753\n165\npca_XZ5Kg\n\n\nvariance\n0.9982445\n166\npca_XZ5Kg\n\n\nvariance\n0.9950035\n167\npca_XZ5Kg\n\n\nvariance\n0.9918955\n168\npca_XZ5Kg\n\n\nvariance\n0.9881412\n169\npca_XZ5Kg\n\n\nvariance\n0.9811765\n170\npca_XZ5Kg\n\n\nvariance\n0.9763066\n171\npca_XZ5Kg\n\n\nvariance\n0.9738467\n172\npca_XZ5Kg\n\n\nvariance\n0.9586386\n173\npca_XZ5Kg\n\n\nvariance\n0.9529320\n174\npca_XZ5Kg\n\n\nvariance\n0.9396962\n175\npca_XZ5Kg\n\n\nvariance\n0.9342019\n176\npca_XZ5Kg\n\n\nvariance\n0.9272555\n177\npca_XZ5Kg\n\n\nvariance\n0.9194858\n178\npca_XZ5Kg\n\n\nvariance\n0.9135065\n179\npca_XZ5Kg\n\n\nvariance\n0.9093148\n180\npca_XZ5Kg\n\n\nvariance\n0.8985569\n181\npca_XZ5Kg\n\n\nvariance\n0.8894010\n182\npca_XZ5Kg\n\n\nvariance\n0.8817919\n183\npca_XZ5Kg\n\n\nvariance\n0.8720007\n184\npca_XZ5Kg\n\n\nvariance\n0.8711999\n185\npca_XZ5Kg\n\n\nvariance\n0.8604178\n186\npca_XZ5Kg\n\n\nvariance\n0.8538532\n187\npca_XZ5Kg\n\n\nvariance\n0.8524735\n188\npca_XZ5Kg\n\n\nvariance\n0.8432218\n189\npca_XZ5Kg\n\n\nvariance\n0.8371707\n190\npca_XZ5Kg\n\n\nvariance\n0.8333971\n191\npca_XZ5Kg\n\n\nvariance\n0.8231037\n192\npca_XZ5Kg\n\n\nvariance\n0.8152543\n193\npca_XZ5Kg\n\n\nvariance\n0.8082016\n194\npca_XZ5Kg\n\n\nvariance\n0.8034358\n195\npca_XZ5Kg\n\n\nvariance\n0.7844609\n196\npca_XZ5Kg\n\n\nvariance\n0.7782716\n197\npca_XZ5Kg\n\n\nvariance\n0.7673940\n198\npca_XZ5Kg\n\n\nvariance\n0.7628960\n199\npca_XZ5Kg\n\n\nvariance\n0.7562519\n200\npca_XZ5Kg\n\n\nvariance\n0.7501862\n201\npca_XZ5Kg\n\n\nvariance\n0.7428395\n202\npca_XZ5Kg\n\n\nvariance\n0.7404320\n203\npca_XZ5Kg\n\n\nvariance\n0.7318651\n204\npca_XZ5Kg\n\n\nvariance\n0.7229137\n205\npca_XZ5Kg\n\n\nvariance\n0.7181321\n206\npca_XZ5Kg\n\n\nvariance\n0.7154646\n207\npca_XZ5Kg\n\n\nvariance\n0.7112162\n208\npca_XZ5Kg\n\n\nvariance\n0.7007059\n209\npca_XZ5Kg\n\n\nvariance\n0.6944100\n210\npca_XZ5Kg\n\n\nvariance\n0.6869210\n211\npca_XZ5Kg\n\n\nvariance\n0.6857158\n212\npca_XZ5Kg\n\n\nvariance\n0.6795213\n213\npca_XZ5Kg\n\n\nvariance\n0.6786213\n214\npca_XZ5Kg\n\n\nvariance\n0.6766365\n215\npca_XZ5Kg\n\n\nvariance\n0.6644770\n216\npca_XZ5Kg\n\n\nvariance\n0.6577775\n217\npca_XZ5Kg\n\n\nvariance\n0.6515223\n218\npca_XZ5Kg\n\n\nvariance\n0.6434857\n219\npca_XZ5Kg\n\n\nvariance\n0.6392533\n220\npca_XZ5Kg\n\n\nvariance\n0.6365603\n221\npca_XZ5Kg\n\n\nvariance\n0.6301616\n222\npca_XZ5Kg\n\n\nvariance\n0.6231846\n223\npca_XZ5Kg\n\n\nvariance\n0.6181426\n224\npca_XZ5Kg\n\n\nvariance\n0.6170199\n225\npca_XZ5Kg\n\n\nvariance\n0.6061181\n226\npca_XZ5Kg\n\n\nvariance\n0.6023459\n227\npca_XZ5Kg\n\n\nvariance\n0.5992154\n228\npca_XZ5Kg\n\n\nvariance\n0.5908241\n229\npca_XZ5Kg\n\n\nvariance\n0.5884995\n230\npca_XZ5Kg\n\n\nvariance\n0.5828980\n231\npca_XZ5Kg\n\n\nvariance\n0.5809037\n232\npca_XZ5Kg\n\n\nvariance\n0.5678429\n233\npca_XZ5Kg\n\n\nvariance\n0.5616150\n234\npca_XZ5Kg\n\n\nvariance\n0.5583740\n235\npca_XZ5Kg\n\n\nvariance\n0.5531686\n236\npca_XZ5Kg\n\n\nvariance\n0.5494885\n237\npca_XZ5Kg\n\n\nvariance\n0.5378721\n238\npca_XZ5Kg\n\n\nvariance\n0.5331150\n239\npca_XZ5Kg\n\n\nvariance\n0.5298584\n240\npca_XZ5Kg\n\n\nvariance\n0.5251532\n241\npca_XZ5Kg\n\n\nvariance\n0.5213120\n242\npca_XZ5Kg\n\n\nvariance\n0.5202090\n243\npca_XZ5Kg\n\n\nvariance\n0.5139262\n244\npca_XZ5Kg\n\n\nvariance\n0.5098166\n245\npca_XZ5Kg\n\n\nvariance\n0.5037062\n246\npca_XZ5Kg\n\n\nvariance\n0.4979692\n247\npca_XZ5Kg\n\n\nvariance\n0.4977104\n248\npca_XZ5Kg\n\n\nvariance\n0.4871778\n249\npca_XZ5Kg\n\n\nvariance\n0.4864240\n250\npca_XZ5Kg\n\n\nvariance\n0.4849756\n251\npca_XZ5Kg\n\n\nvariance\n0.4814601\n252\npca_XZ5Kg\n\n\nvariance\n0.4755758\n253\npca_XZ5Kg\n\n\nvariance\n0.4696874\n254\npca_XZ5Kg\n\n\nvariance\n0.4673607\n255\npca_XZ5Kg\n\n\nvariance\n0.4621037\n256\npca_XZ5Kg\n\n\nvariance\n0.4598385\n257\npca_XZ5Kg\n\n\nvariance\n0.4562466\n258\npca_XZ5Kg\n\n\nvariance\n0.4532367\n259\npca_XZ5Kg\n\n\nvariance\n0.4501816\n260\npca_XZ5Kg\n\n\nvariance\n0.4474510\n261\npca_XZ5Kg\n\n\nvariance\n0.4446880\n262\npca_XZ5Kg\n\n\nvariance\n0.4366686\n263\npca_XZ5Kg\n\n\nvariance\n0.4343969\n264\npca_XZ5Kg\n\n\nvariance\n0.4291821\n265\npca_XZ5Kg\n\n\nvariance\n0.4243615\n266\npca_XZ5Kg\n\n\nvariance\n0.4184784\n267\npca_XZ5Kg\n\n\nvariance\n0.4164061\n268\npca_XZ5Kg\n\n\nvariance\n0.4124427\n269\npca_XZ5Kg\n\n\nvariance\n0.4105753\n270\npca_XZ5Kg\n\n\nvariance\n0.4084651\n271\npca_XZ5Kg\n\n\nvariance\n0.4043377\n272\npca_XZ5Kg\n\n\nvariance\n0.4006646\n273\npca_XZ5Kg\n\n\nvariance\n0.3990587\n274\npca_XZ5Kg\n\n\nvariance\n0.3955736\n275\npca_XZ5Kg\n\n\nvariance\n0.3944960\n276\npca_XZ5Kg\n\n\nvariance\n0.3905095\n277\npca_XZ5Kg\n\n\nvariance\n0.3876988\n278\npca_XZ5Kg\n\n\nvariance\n0.3863032\n279\npca_XZ5Kg\n\n\nvariance\n0.3817634\n280\npca_XZ5Kg\n\n\nvariance\n0.3796827\n281\npca_XZ5Kg\n\n\nvariance\n0.3772863\n282\npca_XZ5Kg\n\n\nvariance\n0.3745496\n283\npca_XZ5Kg\n\n\nvariance\n0.3719924\n284\npca_XZ5Kg\n\n\nvariance\n0.3691151\n285\npca_XZ5Kg\n\n\nvariance\n0.3644745\n286\npca_XZ5Kg\n\n\nvariance\n0.3637667\n287\npca_XZ5Kg\n\n\nvariance\n0.3607259\n288\npca_XZ5Kg\n\n\nvariance\n0.3590507\n289\npca_XZ5Kg\n\n\nvariance\n0.3563793\n290\npca_XZ5Kg\n\n\nvariance\n0.3529852\n291\npca_XZ5Kg\n\n\nvariance\n0.3461941\n292\npca_XZ5Kg\n\n\nvariance\n0.3438409\n293\npca_XZ5Kg\n\n\nvariance\n0.3419465\n294\npca_XZ5Kg\n\n\nvariance\n0.3412938\n295\npca_XZ5Kg\n\n\nvariance\n0.3377819\n296\npca_XZ5Kg\n\n\nvariance\n0.3341217\n297\npca_XZ5Kg\n\n\nvariance\n0.3289753\n298\npca_XZ5Kg\n\n\nvariance\n0.3274883\n299\npca_XZ5Kg\n\n\nvariance\n0.3222247\n300\npca_XZ5Kg\n\n\nvariance\n0.3201990\n301\npca_XZ5Kg\n\n\nvariance\n0.3173841\n302\npca_XZ5Kg\n\n\nvariance\n0.3143222\n303\npca_XZ5Kg\n\n\nvariance\n0.3141856\n304\npca_XZ5Kg\n\n\nvariance\n0.3099768\n305\npca_XZ5Kg\n\n\nvariance\n0.3088122\n306\npca_XZ5Kg\n\n\nvariance\n0.3045606\n307\npca_XZ5Kg\n\n\nvariance\n0.3020396\n308\npca_XZ5Kg\n\n\nvariance\n0.3005910\n309\npca_XZ5Kg\n\n\nvariance\n0.2976833\n310\npca_XZ5Kg\n\n\nvariance\n0.2963184\n311\npca_XZ5Kg\n\n\nvariance\n0.2942197\n312\npca_XZ5Kg\n\n\nvariance\n0.2928602\n313\npca_XZ5Kg\n\n\nvariance\n0.2895027\n314\npca_XZ5Kg\n\n\nvariance\n0.2878975\n315\npca_XZ5Kg\n\n\nvariance\n0.2856904\n316\npca_XZ5Kg\n\n\nvariance\n0.2834017\n317\npca_XZ5Kg\n\n\nvariance\n0.2811310\n318\npca_XZ5Kg\n\n\nvariance\n0.2796897\n319\npca_XZ5Kg\n\n\nvariance\n0.2772879\n320\npca_XZ5Kg\n\n\nvariance\n0.2769102\n321\npca_XZ5Kg\n\n\nvariance\n0.2746351\n322\npca_XZ5Kg\n\n\nvariance\n0.2717807\n323\npca_XZ5Kg\n\n\nvariance\n0.2696303\n324\npca_XZ5Kg\n\n\nvariance\n0.2690399\n325\npca_XZ5Kg\n\n\nvariance\n0.2656620\n326\npca_XZ5Kg\n\n\nvariance\n0.2628185\n327\npca_XZ5Kg\n\n\nvariance\n0.2608510\n328\npca_XZ5Kg\n\n\nvariance\n0.2577386\n329\npca_XZ5Kg\n\n\nvariance\n0.2568700\n330\npca_XZ5Kg\n\n\nvariance\n0.2545998\n331\npca_XZ5Kg\n\n\nvariance\n0.2536222\n332\npca_XZ5Kg\n\n\nvariance\n0.2527428\n333\npca_XZ5Kg\n\n\nvariance\n0.2498390\n334\npca_XZ5Kg\n\n\nvariance\n0.2475591\n335\npca_XZ5Kg\n\n\nvariance\n0.2451940\n336\npca_XZ5Kg\n\n\nvariance\n0.2430604\n337\npca_XZ5Kg\n\n\nvariance\n0.2413038\n338\npca_XZ5Kg\n\n\nvariance\n0.2400498\n339\npca_XZ5Kg\n\n\nvariance\n0.2382887\n340\npca_XZ5Kg\n\n\nvariance\n0.2366789\n341\npca_XZ5Kg\n\n\nvariance\n0.2357505\n342\npca_XZ5Kg\n\n\nvariance\n0.2329881\n343\npca_XZ5Kg\n\n\nvariance\n0.2316563\n344\npca_XZ5Kg\n\n\nvariance\n0.2305513\n345\npca_XZ5Kg\n\n\nvariance\n0.2289599\n346\npca_XZ5Kg\n\n\nvariance\n0.2258230\n347\npca_XZ5Kg\n\n\nvariance\n0.2230927\n348\npca_XZ5Kg\n\n\nvariance\n0.2222058\n349\npca_XZ5Kg\n\n\nvariance\n0.2214328\n350\npca_XZ5Kg\n\n\nvariance\n0.2197431\n351\npca_XZ5Kg\n\n\nvariance\n0.2184000\n352\npca_XZ5Kg\n\n\nvariance\n0.2166531\n353\npca_XZ5Kg\n\n\nvariance\n0.2154478\n354\npca_XZ5Kg\n\n\nvariance\n0.2134502\n355\npca_XZ5Kg\n\n\nvariance\n0.2125957\n356\npca_XZ5Kg\n\n\nvariance\n0.2110718\n357\npca_XZ5Kg\n\n\nvariance\n0.2081271\n358\npca_XZ5Kg\n\n\nvariance\n0.2078525\n359\npca_XZ5Kg\n\n\nvariance\n0.2061061\n360\npca_XZ5Kg\n\n\nvariance\n0.2047279\n361\npca_XZ5Kg\n\n\nvariance\n0.2033288\n362\npca_XZ5Kg\n\n\nvariance\n0.2024572\n363\npca_XZ5Kg\n\n\nvariance\n0.2008953\n364\npca_XZ5Kg\n\n\nvariance\n0.1988366\n365\npca_XZ5Kg\n\n\nvariance\n0.1982401\n366\npca_XZ5Kg\n\n\nvariance\n0.1953853\n367\npca_XZ5Kg\n\n\nvariance\n0.1939653\n368\npca_XZ5Kg\n\n\nvariance\n0.1936169\n369\npca_XZ5Kg\n\n\nvariance\n0.1907232\n370\npca_XZ5Kg\n\n\nvariance\n0.1905265\n371\npca_XZ5Kg\n\n\nvariance\n0.1885672\n372\npca_XZ5Kg\n\n\nvariance\n0.1864355\n373\npca_XZ5Kg\n\n\nvariance\n0.1859439\n374\npca_XZ5Kg\n\n\nvariance\n0.1832943\n375\npca_XZ5Kg\n\n\nvariance\n0.1820029\n376\npca_XZ5Kg\n\n\nvariance\n0.1814518\n377\npca_XZ5Kg\n\n\nvariance\n0.1801457\n378\npca_XZ5Kg\n\n\nvariance\n0.1790579\n379\npca_XZ5Kg\n\n\nvariance\n0.1780088\n380\npca_XZ5Kg\n\n\nvariance\n0.1752524\n381\npca_XZ5Kg\n\n\nvariance\n0.1749641\n382\npca_XZ5Kg\n\n\nvariance\n0.1744172\n383\npca_XZ5Kg\n\n\nvariance\n0.1736876\n384\npca_XZ5Kg\n\n\nvariance\n0.1704834\n385\npca_XZ5Kg\n\n\nvariance\n0.1699440\n386\npca_XZ5Kg\n\n\nvariance\n0.1693458\n387\npca_XZ5Kg\n\n\nvariance\n0.1684624\n388\npca_XZ5Kg\n\n\nvariance\n0.1671434\n389\npca_XZ5Kg\n\n\nvariance\n0.1653553\n390\npca_XZ5Kg\n\n\nvariance\n0.1647001\n391\npca_XZ5Kg\n\n\nvariance\n0.1642354\n392\npca_XZ5Kg\n\n\nvariance\n0.1628556\n393\npca_XZ5Kg\n\n\nvariance\n0.1617490\n394\npca_XZ5Kg\n\n\nvariance\n0.1611793\n395\npca_XZ5Kg\n\n\nvariance\n0.1607213\n396\npca_XZ5Kg\n\n\nvariance\n0.1589901\n397\npca_XZ5Kg\n\n\nvariance\n0.1587391\n398\npca_XZ5Kg\n\n\nvariance\n0.1578149\n399\npca_XZ5Kg\n\n\nvariance\n0.1565026\n400\npca_XZ5Kg\n\n\nvariance\n0.1557176\n401\npca_XZ5Kg\n\n\nvariance\n0.1550580\n402\npca_XZ5Kg\n\n\nvariance\n0.1539537\n403\npca_XZ5Kg\n\n\nvariance\n0.1529761\n404\npca_XZ5Kg\n\n\nvariance\n0.1516933\n405\npca_XZ5Kg\n\n\nvariance\n0.1509148\n406\npca_XZ5Kg\n\n\nvariance\n0.1495823\n407\npca_XZ5Kg\n\n\nvariance\n0.1484793\n408\npca_XZ5Kg\n\n\nvariance\n0.1478633\n409\npca_XZ5Kg\n\n\nvariance\n0.1460778\n410\npca_XZ5Kg\n\n\nvariance\n0.1453299\n411\npca_XZ5Kg\n\n\nvariance\n0.1433647\n412\npca_XZ5Kg\n\n\nvariance\n0.1432810\n413\npca_XZ5Kg\n\n\nvariance\n0.1421258\n414\npca_XZ5Kg\n\n\nvariance\n0.1419923\n415\npca_XZ5Kg\n\n\nvariance\n0.1407144\n416\npca_XZ5Kg\n\n\nvariance\n0.1400999\n417\npca_XZ5Kg\n\n\nvariance\n0.1366408\n418\npca_XZ5Kg\n\n\nvariance\n0.1362189\n419\npca_XZ5Kg\n\n\nvariance\n0.1355913\n420\npca_XZ5Kg\n\n\nvariance\n0.1352580\n421\npca_XZ5Kg\n\n\nvariance\n0.1342332\n422\npca_XZ5Kg\n\n\nvariance\n0.1327628\n423\npca_XZ5Kg\n\n\nvariance\n0.1325821\n424\npca_XZ5Kg\n\n\nvariance\n0.1314736\n425\npca_XZ5Kg\n\n\nvariance\n0.1295175\n426\npca_XZ5Kg\n\n\nvariance\n0.1284412\n427\npca_XZ5Kg\n\n\nvariance\n0.1283935\n428\npca_XZ5Kg\n\n\nvariance\n0.1273082\n429\npca_XZ5Kg\n\n\nvariance\n0.1263684\n430\npca_XZ5Kg\n\n\nvariance\n0.1262164\n431\npca_XZ5Kg\n\n\nvariance\n0.1255431\n432\npca_XZ5Kg\n\n\nvariance\n0.1242021\n433\npca_XZ5Kg\n\n\nvariance\n0.1241411\n434\npca_XZ5Kg\n\n\nvariance\n0.1226541\n435\npca_XZ5Kg\n\n\nvariance\n0.1221982\n436\npca_XZ5Kg\n\n\nvariance\n0.1219637\n437\npca_XZ5Kg\n\n\nvariance\n0.1209295\n438\npca_XZ5Kg\n\n\nvariance\n0.1206559\n439\npca_XZ5Kg\n\n\nvariance\n0.1196897\n440\npca_XZ5Kg\n\n\nvariance\n0.1190129\n441\npca_XZ5Kg\n\n\nvariance\n0.1186161\n442\npca_XZ5Kg\n\n\nvariance\n0.1181169\n443\npca_XZ5Kg\n\n\nvariance\n0.1174646\n444\npca_XZ5Kg\n\n\nvariance\n0.1173340\n445\npca_XZ5Kg\n\n\nvariance\n0.1167215\n446\npca_XZ5Kg\n\n\nvariance\n0.1158716\n447\npca_XZ5Kg\n\n\nvariance\n0.1151217\n448\npca_XZ5Kg\n\n\nvariance\n0.1140735\n449\npca_XZ5Kg\n\n\nvariance\n0.1126847\n450\npca_XZ5Kg\n\n\nvariance\n0.1121497\n451\npca_XZ5Kg\n\n\nvariance\n0.1114590\n452\npca_XZ5Kg\n\n\nvariance\n0.1103357\n453\npca_XZ5Kg\n\n\nvariance\n0.1101881\n454\npca_XZ5Kg\n\n\nvariance\n0.1092493\n455\npca_XZ5Kg\n\n\nvariance\n0.1084457\n456\npca_XZ5Kg\n\n\nvariance\n0.1075104\n457\npca_XZ5Kg\n\n\nvariance\n0.1071219\n458\npca_XZ5Kg\n\n\nvariance\n0.1069047\n459\npca_XZ5Kg\n\n\nvariance\n0.1061041\n460\npca_XZ5Kg\n\n\nvariance\n0.1055952\n461\npca_XZ5Kg\n\n\nvariance\n0.1055556\n462\npca_XZ5Kg\n\n\nvariance\n0.1052098\n463\npca_XZ5Kg\n\n\nvariance\n0.1039019\n464\npca_XZ5Kg\n\n\nvariance\n0.1029672\n465\npca_XZ5Kg\n\n\nvariance\n0.1028771\n466\npca_XZ5Kg\n\n\nvariance\n0.1025320\n467\npca_XZ5Kg\n\n\nvariance\n0.1014781\n468\npca_XZ5Kg\n\n\nvariance\n0.1012604\n469\npca_XZ5Kg\n\n\nvariance\n0.1010148\n470\npca_XZ5Kg\n\n\nvariance\n0.1000882\n471\npca_XZ5Kg\n\n\nvariance\n0.0999876\n472\npca_XZ5Kg\n\n\nvariance\n0.0991322\n473\npca_XZ5Kg\n\n\nvariance\n0.0986695\n474\npca_XZ5Kg\n\n\nvariance\n0.0981199\n475\npca_XZ5Kg\n\n\nvariance\n0.0976235\n476\npca_XZ5Kg\n\n\nvariance\n0.0962215\n477\npca_XZ5Kg\n\n\nvariance\n0.0960705\n478\npca_XZ5Kg\n\n\nvariance\n0.0956913\n479\npca_XZ5Kg\n\n\nvariance\n0.0944556\n480\npca_XZ5Kg\n\n\nvariance\n0.0941279\n481\npca_XZ5Kg\n\n\nvariance\n0.0939418\n482\npca_XZ5Kg\n\n\nvariance\n0.0934708\n483\npca_XZ5Kg\n\n\nvariance\n0.0931274\n484\npca_XZ5Kg\n\n\nvariance\n0.0927579\n485\npca_XZ5Kg\n\n\nvariance\n0.0918690\n486\npca_XZ5Kg\n\n\nvariance\n0.0910154\n487\npca_XZ5Kg\n\n\nvariance\n0.0907436\n488\npca_XZ5Kg\n\n\nvariance\n0.0901512\n489\npca_XZ5Kg\n\n\nvariance\n0.0896621\n490\npca_XZ5Kg\n\n\nvariance\n0.0892178\n491\npca_XZ5Kg\n\n\nvariance\n0.0887262\n492\npca_XZ5Kg\n\n\nvariance\n0.0880554\n493\npca_XZ5Kg\n\n\nvariance\n0.0880271\n494\npca_XZ5Kg\n\n\nvariance\n0.0874531\n495\npca_XZ5Kg\n\n\nvariance\n0.0873566\n496\npca_XZ5Kg\n\n\nvariance\n0.0862892\n497\npca_XZ5Kg\n\n\nvariance\n0.0861687\n498\npca_XZ5Kg\n\n\nvariance\n0.0855976\n499\npca_XZ5Kg\n\n\nvariance\n0.0852034\n500\npca_XZ5Kg\n\n\nvariance\n0.0846459\n501\npca_XZ5Kg\n\n\nvariance\n0.0844515\n502\npca_XZ5Kg\n\n\nvariance\n0.0836709\n503\npca_XZ5Kg\n\n\nvariance\n0.0830853\n504\npca_XZ5Kg\n\n\nvariance\n0.0825595\n505\npca_XZ5Kg\n\n\nvariance\n0.0818487\n506\npca_XZ5Kg\n\n\nvariance\n0.0816690\n507\npca_XZ5Kg\n\n\nvariance\n0.0811529\n508\npca_XZ5Kg\n\n\nvariance\n0.0806626\n509\npca_XZ5Kg\n\n\nvariance\n0.0804725\n510\npca_XZ5Kg\n\n\nvariance\n0.0798590\n511\npca_XZ5Kg\n\n\nvariance\n0.0795467\n512\npca_XZ5Kg\n\n\nvariance\n0.0787387\n513\npca_XZ5Kg\n\n\nvariance\n0.0784439\n514\npca_XZ5Kg\n\n\nvariance\n0.0780325\n515\npca_XZ5Kg\n\n\nvariance\n0.0777017\n516\npca_XZ5Kg\n\n\nvariance\n0.0773247\n517\npca_XZ5Kg\n\n\nvariance\n0.0768321\n518\npca_XZ5Kg\n\n\nvariance\n0.0767898\n519\npca_XZ5Kg\n\n\nvariance\n0.0766431\n520\npca_XZ5Kg\n\n\nvariance\n0.0762530\n521\npca_XZ5Kg\n\n\nvariance\n0.0757462\n522\npca_XZ5Kg\n\n\nvariance\n0.0754874\n523\npca_XZ5Kg\n\n\nvariance\n0.0751168\n524\npca_XZ5Kg\n\n\nvariance\n0.0746517\n525\npca_XZ5Kg\n\n\nvariance\n0.0740428\n526\npca_XZ5Kg\n\n\nvariance\n0.0738354\n527\npca_XZ5Kg\n\n\nvariance\n0.0735300\n528\npca_XZ5Kg\n\n\nvariance\n0.0732921\n529\npca_XZ5Kg\n\n\nvariance\n0.0729363\n530\npca_XZ5Kg\n\n\nvariance\n0.0722028\n531\npca_XZ5Kg\n\n\nvariance\n0.0717088\n532\npca_XZ5Kg\n\n\nvariance\n0.0715695\n533\npca_XZ5Kg\n\n\nvariance\n0.0710300\n534\npca_XZ5Kg\n\n\nvariance\n0.0708745\n535\npca_XZ5Kg\n\n\nvariance\n0.0707496\n536\npca_XZ5Kg\n\n\nvariance\n0.0698304\n537\npca_XZ5Kg\n\n\nvariance\n0.0696761\n538\npca_XZ5Kg\n\n\nvariance\n0.0692013\n539\npca_XZ5Kg\n\n\nvariance\n0.0689276\n540\npca_XZ5Kg\n\n\nvariance\n0.0685700\n541\npca_XZ5Kg\n\n\nvariance\n0.0681503\n542\npca_XZ5Kg\n\n\nvariance\n0.0678005\n543\npca_XZ5Kg\n\n\nvariance\n0.0674213\n544\npca_XZ5Kg\n\n\nvariance\n0.0673106\n545\npca_XZ5Kg\n\n\nvariance\n0.0665346\n546\npca_XZ5Kg\n\n\nvariance\n0.0662575\n547\npca_XZ5Kg\n\n\nvariance\n0.0662254\n548\npca_XZ5Kg\n\n\nvariance\n0.0660517\n549\npca_XZ5Kg\n\n\nvariance\n0.0651589\n550\npca_XZ5Kg\n\n\nvariance\n0.0647095\n551\npca_XZ5Kg\n\n\nvariance\n0.0642805\n552\npca_XZ5Kg\n\n\nvariance\n0.0636520\n553\npca_XZ5Kg\n\n\nvariance\n0.0633399\n554\npca_XZ5Kg\n\n\nvariance\n0.0630302\n555\npca_XZ5Kg\n\n\nvariance\n0.0627478\n556\npca_XZ5Kg\n\n\nvariance\n0.0627228\n557\npca_XZ5Kg\n\n\nvariance\n0.0622318\n558\npca_XZ5Kg\n\n\nvariance\n0.0617651\n559\npca_XZ5Kg\n\n\nvariance\n0.0611614\n560\npca_XZ5Kg\n\n\nvariance\n0.0607914\n561\npca_XZ5Kg\n\n\nvariance\n0.0606068\n562\npca_XZ5Kg\n\n\nvariance\n0.0605425\n563\npca_XZ5Kg\n\n\nvariance\n0.0603339\n564\npca_XZ5Kg\n\n\nvariance\n0.0600316\n565\npca_XZ5Kg\n\n\nvariance\n0.0596667\n566\npca_XZ5Kg\n\n\nvariance\n0.0588990\n567\npca_XZ5Kg\n\n\nvariance\n0.0588618\n568\npca_XZ5Kg\n\n\nvariance\n0.0585501\n569\npca_XZ5Kg\n\n\nvariance\n0.0579933\n570\npca_XZ5Kg\n\n\nvariance\n0.0578428\n571\npca_XZ5Kg\n\n\nvariance\n0.0576485\n572\npca_XZ5Kg\n\n\nvariance\n0.0572756\n573\npca_XZ5Kg\n\n\nvariance\n0.0568802\n574\npca_XZ5Kg\n\n\nvariance\n0.0563234\n575\npca_XZ5Kg\n\n\nvariance\n0.0561282\n576\npca_XZ5Kg\n\n\nvariance\n0.0560246\n577\npca_XZ5Kg\n\n\nvariance\n0.0556846\n578\npca_XZ5Kg\n\n\nvariance\n0.0553713\n579\npca_XZ5Kg\n\n\nvariance\n0.0548701\n580\npca_XZ5Kg\n\n\nvariance\n0.0543958\n581\npca_XZ5Kg\n\n\nvariance\n0.0542514\n582\npca_XZ5Kg\n\n\nvariance\n0.0541719\n583\npca_XZ5Kg\n\n\nvariance\n0.0535840\n584\npca_XZ5Kg\n\n\nvariance\n0.0534817\n585\npca_XZ5Kg\n\n\nvariance\n0.0532694\n586\npca_XZ5Kg\n\n\nvariance\n0.0526302\n587\npca_XZ5Kg\n\n\nvariance\n0.0525551\n588\npca_XZ5Kg\n\n\nvariance\n0.0520580\n589\npca_XZ5Kg\n\n\nvariance\n0.0520354\n590\npca_XZ5Kg\n\n\nvariance\n0.0514785\n591\npca_XZ5Kg\n\n\nvariance\n0.0514116\n592\npca_XZ5Kg\n\n\nvariance\n0.0510471\n593\npca_XZ5Kg\n\n\nvariance\n0.0505489\n594\npca_XZ5Kg\n\n\nvariance\n0.0499108\n595\npca_XZ5Kg\n\n\nvariance\n0.0497753\n596\npca_XZ5Kg\n\n\nvariance\n0.0495371\n597\npca_XZ5Kg\n\n\nvariance\n0.0494464\n598\npca_XZ5Kg\n\n\nvariance\n0.0489601\n599\npca_XZ5Kg\n\n\nvariance\n0.0486752\n600\npca_XZ5Kg\n\n\nvariance\n0.0484454\n601\npca_XZ5Kg\n\n\nvariance\n0.0484011\n602\npca_XZ5Kg\n\n\nvariance\n0.0481149\n603\npca_XZ5Kg\n\n\nvariance\n0.0478454\n604\npca_XZ5Kg\n\n\nvariance\n0.0476975\n605\npca_XZ5Kg\n\n\nvariance\n0.0471474\n606\npca_XZ5Kg\n\n\nvariance\n0.0469730\n607\npca_XZ5Kg\n\n\nvariance\n0.0467838\n608\npca_XZ5Kg\n\n\nvariance\n0.0464157\n609\npca_XZ5Kg\n\n\nvariance\n0.0460452\n610\npca_XZ5Kg\n\n\nvariance\n0.0458673\n611\npca_XZ5Kg\n\n\nvariance\n0.0454372\n612\npca_XZ5Kg\n\n\nvariance\n0.0451101\n613\npca_XZ5Kg\n\n\nvariance\n0.0449031\n614\npca_XZ5Kg\n\n\nvariance\n0.0444963\n615\npca_XZ5Kg\n\n\nvariance\n0.0443734\n616\npca_XZ5Kg\n\n\nvariance\n0.0441724\n617\npca_XZ5Kg\n\n\nvariance\n0.0438086\n618\npca_XZ5Kg\n\n\nvariance\n0.0435438\n619\npca_XZ5Kg\n\n\nvariance\n0.0433648\n620\npca_XZ5Kg\n\n\nvariance\n0.0431079\n621\npca_XZ5Kg\n\n\nvariance\n0.0429888\n622\npca_XZ5Kg\n\n\nvariance\n0.0425301\n623\npca_XZ5Kg\n\n\nvariance\n0.0423341\n624\npca_XZ5Kg\n\n\nvariance\n0.0420923\n625\npca_XZ5Kg\n\n\nvariance\n0.0419995\n626\npca_XZ5Kg\n\n\nvariance\n0.0415967\n627\npca_XZ5Kg\n\n\nvariance\n0.0415538\n628\npca_XZ5Kg\n\n\nvariance\n0.0410668\n629\npca_XZ5Kg\n\n\nvariance\n0.0408937\n630\npca_XZ5Kg\n\n\nvariance\n0.0406157\n631\npca_XZ5Kg\n\n\nvariance\n0.0404402\n632\npca_XZ5Kg\n\n\nvariance\n0.0403427\n633\npca_XZ5Kg\n\n\nvariance\n0.0401443\n634\npca_XZ5Kg\n\n\nvariance\n0.0399559\n635\npca_XZ5Kg\n\n\nvariance\n0.0398520\n636\npca_XZ5Kg\n\n\nvariance\n0.0394720\n637\npca_XZ5Kg\n\n\nvariance\n0.0393763\n638\npca_XZ5Kg\n\n\nvariance\n0.0389693\n639\npca_XZ5Kg\n\n\nvariance\n0.0386442\n640\npca_XZ5Kg\n\n\nvariance\n0.0383515\n641\npca_XZ5Kg\n\n\nvariance\n0.0382810\n642\npca_XZ5Kg\n\n\nvariance\n0.0378687\n643\npca_XZ5Kg\n\n\nvariance\n0.0374988\n644\npca_XZ5Kg\n\n\nvariance\n0.0372839\n645\npca_XZ5Kg\n\n\nvariance\n0.0371122\n646\npca_XZ5Kg\n\n\nvariance\n0.0367338\n647\npca_XZ5Kg\n\n\nvariance\n0.0365614\n648\npca_XZ5Kg\n\n\nvariance\n0.0361874\n649\npca_XZ5Kg\n\n\nvariance\n0.0359937\n650\npca_XZ5Kg\n\n\nvariance\n0.0359157\n651\npca_XZ5Kg\n\n\nvariance\n0.0354626\n652\npca_XZ5Kg\n\n\nvariance\n0.0354017\n653\npca_XZ5Kg\n\n\nvariance\n0.0350353\n654\npca_XZ5Kg\n\n\nvariance\n0.0348493\n655\npca_XZ5Kg\n\n\nvariance\n0.0346105\n656\npca_XZ5Kg\n\n\nvariance\n0.0345239\n657\npca_XZ5Kg\n\n\nvariance\n0.0343125\n658\npca_XZ5Kg\n\n\nvariance\n0.0341484\n659\npca_XZ5Kg\n\n\nvariance\n0.0339538\n660\npca_XZ5Kg\n\n\nvariance\n0.0333331\n661\npca_XZ5Kg\n\n\nvariance\n0.0330420\n662\npca_XZ5Kg\n\n\nvariance\n0.0330316\n663\npca_XZ5Kg\n\n\nvariance\n0.0328578\n664\npca_XZ5Kg\n\n\nvariance\n0.0324660\n665\npca_XZ5Kg\n\n\nvariance\n0.0323101\n666\npca_XZ5Kg\n\n\nvariance\n0.0321032\n667\npca_XZ5Kg\n\n\nvariance\n0.0317928\n668\npca_XZ5Kg\n\n\nvariance\n0.0315557\n669\npca_XZ5Kg\n\n\nvariance\n0.0313029\n670\npca_XZ5Kg\n\n\nvariance\n0.0311393\n671\npca_XZ5Kg\n\n\nvariance\n0.0308684\n672\npca_XZ5Kg\n\n\nvariance\n0.0306067\n673\npca_XZ5Kg\n\n\nvariance\n0.0300692\n674\npca_XZ5Kg\n\n\nvariance\n0.0299639\n675\npca_XZ5Kg\n\n\nvariance\n0.0297945\n676\npca_XZ5Kg\n\n\nvariance\n0.0294315\n677\npca_XZ5Kg\n\n\nvariance\n0.0292626\n678\npca_XZ5Kg\n\n\nvariance\n0.0288831\n679\npca_XZ5Kg\n\n\nvariance\n0.0286481\n680\npca_XZ5Kg\n\n\nvariance\n0.0285703\n681\npca_XZ5Kg\n\n\nvariance\n0.0283093\n682\npca_XZ5Kg\n\n\nvariance\n0.0278183\n683\npca_XZ5Kg\n\n\nvariance\n0.0277578\n684\npca_XZ5Kg\n\n\nvariance\n0.0274424\n685\npca_XZ5Kg\n\n\nvariance\n0.0273865\n686\npca_XZ5Kg\n\n\nvariance\n0.0270099\n687\npca_XZ5Kg\n\n\nvariance\n0.0268432\n688\npca_XZ5Kg\n\n\nvariance\n0.0265604\n689\npca_XZ5Kg\n\n\nvariance\n0.0261230\n690\npca_XZ5Kg\n\n\nvariance\n0.0260789\n691\npca_XZ5Kg\n\n\nvariance\n0.0256059\n692\npca_XZ5Kg\n\n\nvariance\n0.0254156\n693\npca_XZ5Kg\n\n\nvariance\n0.0251043\n694\npca_XZ5Kg\n\n\nvariance\n0.0248624\n695\npca_XZ5Kg\n\n\nvariance\n0.0244666\n696\npca_XZ5Kg\n\n\nvariance\n0.0242218\n697\npca_XZ5Kg\n\n\nvariance\n0.0239523\n698\npca_XZ5Kg\n\n\nvariance\n0.0237958\n699\npca_XZ5Kg\n\n\nvariance\n0.0235872\n700\npca_XZ5Kg\n\n\nvariance\n0.0233358\n701\npca_XZ5Kg\n\n\nvariance\n0.0229986\n702\npca_XZ5Kg\n\n\nvariance\n0.0222646\n703\npca_XZ5Kg\n\n\nvariance\n0.0216653\n704\npca_XZ5Kg\n\n\nvariance\n0.0214996\n705\npca_XZ5Kg\n\n\nvariance\n0.0210715\n706\npca_XZ5Kg\n\n\nvariance\n0.0202919\n707\npca_XZ5Kg\n\n\nvariance\n0.0189436\n708\npca_XZ5Kg\n\n\nvariance\n0.0183219\n709\npca_XZ5Kg\n\n\nvariance\n0.0138854\n710\npca_XZ5Kg\n\n\nvariance\n0.0071723\n711\npca_XZ5Kg\n\n\nvariance\n0.0065655\n712\npca_XZ5Kg\n\n\nvariance\n0.0000000\n713\npca_XZ5Kg\n\n\nvariance\n0.0000000\n714\npca_XZ5Kg\n\n\nvariance\n0.0000000\n715\npca_XZ5Kg\n\n\nvariance\n0.0000000\n716\npca_XZ5Kg\n\n\nvariance\n0.0000000\n717\npca_XZ5Kg\n\n\nvariance\n0.0000000\n718\npca_XZ5Kg\n\n\nvariance\n0.0000000\n719\npca_XZ5Kg\n\n\nvariance\n0.0000000\n720\npca_XZ5Kg\n\n\nvariance\n0.0000000\n721\npca_XZ5Kg\n\n\nvariance\n0.0000000\n722\npca_XZ5Kg\n\n\nvariance\n0.0000000\n723\npca_XZ5Kg\n\n\nvariance\n0.0000000\n724\npca_XZ5Kg\n\n\nvariance\n0.0000000\n725\npca_XZ5Kg\n\n\nvariance\n0.0000000\n726\npca_XZ5Kg\n\n\nvariance\n0.0000000\n727\npca_XZ5Kg\n\n\nvariance\n0.0000000\n728\npca_XZ5Kg\n\n\nvariance\n0.0000000\n729\npca_XZ5Kg\n\n\nvariance\n0.0000000\n730\npca_XZ5Kg\n\n\nvariance\n0.0000000\n731\npca_XZ5Kg\n\n\nvariance\n0.0000000\n732\npca_XZ5Kg\n\n\nvariance\n0.0000000\n733\npca_XZ5Kg\n\n\nvariance\n0.0000000\n734\npca_XZ5Kg\n\n\nvariance\n0.0000000\n735\npca_XZ5Kg\n\n\nvariance\n0.0000000\n736\npca_XZ5Kg\n\n\nvariance\n0.0000000\n737\npca_XZ5Kg\n\n\nvariance\n0.0000000\n738\npca_XZ5Kg\n\n\nvariance\n0.0000000\n739\npca_XZ5Kg\n\n\nvariance\n0.0000000\n740\npca_XZ5Kg\n\n\nvariance\n0.0000000\n741\npca_XZ5Kg\n\n\nvariance\n0.0000000\n742\npca_XZ5Kg\n\n\nvariance\n0.0000000\n743\npca_XZ5Kg\n\n\nvariance\n0.0000000\n744\npca_XZ5Kg\n\n\nvariance\n0.0000000\n745\npca_XZ5Kg\n\n\nvariance\n0.0000000\n746\npca_XZ5Kg\n\n\nvariance\n0.0000000\n747\npca_XZ5Kg\n\n\nvariance\n0.0000000\n748\npca_XZ5Kg\n\n\nvariance\n0.0000000\n749\npca_XZ5Kg\n\n\nvariance\n0.0000000\n750\npca_XZ5Kg\n\n\nvariance\n0.0000000\n751\npca_XZ5Kg\n\n\nvariance\n0.0000000\n752\npca_XZ5Kg\n\n\nvariance\n0.0000000\n753\npca_XZ5Kg\n\n\nvariance\n0.0000000\n754\npca_XZ5Kg\n\n\nvariance\n0.0000000\n755\npca_XZ5Kg\n\n\nvariance\n0.0000000\n756\npca_XZ5Kg\n\n\nvariance\n0.0000000\n757\npca_XZ5Kg\n\n\nvariance\n0.0000000\n758\npca_XZ5Kg\n\n\nvariance\n0.0000000\n759\npca_XZ5Kg\n\n\nvariance\n0.0000000\n760\npca_XZ5Kg\n\n\nvariance\n0.0000000\n761\npca_XZ5Kg\n\n\nvariance\n0.0000000\n762\npca_XZ5Kg\n\n\nvariance\n0.0000000\n763\npca_XZ5Kg\n\n\nvariance\n0.0000000\n764\npca_XZ5Kg\n\n\nvariance\n0.0000000\n765\npca_XZ5Kg\n\n\nvariance\n0.0000000\n766\npca_XZ5Kg\n\n\nvariance\n0.0000000\n767\npca_XZ5Kg\n\n\nvariance\n0.0000000\n768\npca_XZ5Kg\n\n\nvariance\n0.0000000\n769\npca_XZ5Kg\n\n\nvariance\n0.0000000\n770\npca_XZ5Kg\n\n\nvariance\n0.0000000\n771\npca_XZ5Kg\n\n\nvariance\n0.0000000\n772\npca_XZ5Kg\n\n\nvariance\n0.0000000\n773\npca_XZ5Kg\n\n\nvariance\n0.0000000\n774\npca_XZ5Kg\n\n\nvariance\n0.0000000\n775\npca_XZ5Kg\n\n\nvariance\n0.0000000\n776\npca_XZ5Kg\n\n\nvariance\n0.0000000\n777\npca_XZ5Kg\n\n\nvariance\n0.0000000\n778\npca_XZ5Kg\n\n\nvariance\n0.0000000\n779\npca_XZ5Kg\n\n\nvariance\n0.0000000\n780\npca_XZ5Kg\n\n\nvariance\n0.0000000\n781\npca_XZ5Kg\n\n\nvariance\n0.0000000\n782\npca_XZ5Kg\n\n\nvariance\n0.0000000\n783\npca_XZ5Kg\n\n\nvariance\n0.0000000\n784\npca_XZ5Kg\n\n\n\n\n\n\n\nprepped_pca |&gt; \n  tidy(2, type = \"variance\") |&gt; \n  filter(terms == \"percent variance\") |&gt; \n  kable()\n\n\n\n\nterms\nvalue\ncomponent\nid\n\n\n\n\npercent variance\n5.6467169\n1\npca_XZ5Kg\n\n\npercent variance\n4.0782720\n2\npca_XZ5Kg\n\n\npercent variance\n3.7393804\n3\npca_XZ5Kg\n\n\npercent variance\n2.8851148\n4\npca_XZ5Kg\n\n\npercent variance\n2.5211086\n5\npca_XZ5Kg\n\n\npercent variance\n2.1942700\n6\npca_XZ5Kg\n\n\npercent variance\n1.9233444\n7\npca_XZ5Kg\n\n\npercent variance\n1.7457992\n8\npca_XZ5Kg\n\n\npercent variance\n1.5350923\n9\npca_XZ5Kg\n\n\npercent variance\n1.4017196\n10\npca_XZ5Kg\n\n\npercent variance\n1.3417430\n11\npca_XZ5Kg\n\n\npercent variance\n1.2037419\n12\npca_XZ5Kg\n\n\npercent variance\n1.1145695\n13\npca_XZ5Kg\n\n\npercent variance\n1.0899236\n14\npca_XZ5Kg\n\n\npercent variance\n1.0286492\n15\npca_XZ5Kg\n\n\npercent variance\n0.9944866\n16\npca_XZ5Kg\n\n\npercent variance\n0.9363833\n17\npca_XZ5Kg\n\n\npercent variance\n0.9210457\n18\npca_XZ5Kg\n\n\npercent variance\n0.8934368\n19\npca_XZ5Kg\n\n\npercent variance\n0.8699126\n20\npca_XZ5Kg\n\n\npercent variance\n0.8273630\n21\npca_XZ5Kg\n\n\npercent variance\n0.8034174\n22\npca_XZ5Kg\n\n\npercent variance\n0.7648455\n23\npca_XZ5Kg\n\n\npercent variance\n0.7417725\n24\npca_XZ5Kg\n\n\npercent variance\n0.7152929\n25\npca_XZ5Kg\n\n\npercent variance\n0.6918468\n26\npca_XZ5Kg\n\n\npercent variance\n0.6841360\n27\npca_XZ5Kg\n\n\npercent variance\n0.6566745\n28\npca_XZ5Kg\n\n\npercent variance\n0.6316767\n29\npca_XZ5Kg\n\n\npercent variance\n0.6129198\n30\npca_XZ5Kg\n\n\npercent variance\n0.5962553\n31\npca_XZ5Kg\n\n\npercent variance\n0.5877164\n32\npca_XZ5Kg\n\n\npercent variance\n0.5715917\n33\npca_XZ5Kg\n\n\npercent variance\n0.5623074\n34\npca_XZ5Kg\n\n\npercent variance\n0.5546820\n35\npca_XZ5Kg\n\n\npercent variance\n0.5384184\n36\npca_XZ5Kg\n\n\npercent variance\n0.5311822\n37\npca_XZ5Kg\n\n\npercent variance\n0.5196056\n38\npca_XZ5Kg\n\n\npercent variance\n0.5082113\n39\npca_XZ5Kg\n\n\npercent variance\n0.4800056\n40\npca_XZ5Kg\n\n\npercent variance\n0.4764558\n41\npca_XZ5Kg\n\n\npercent variance\n0.4691394\n42\npca_XZ5Kg\n\n\npercent variance\n0.4543490\n43\npca_XZ5Kg\n\n\npercent variance\n0.4513458\n44\npca_XZ5Kg\n\n\npercent variance\n0.4469634\n45\npca_XZ5Kg\n\n\npercent variance\n0.4433832\n46\npca_XZ5Kg\n\n\npercent variance\n0.4382155\n47\npca_XZ5Kg\n\n\npercent variance\n0.4303818\n48\npca_XZ5Kg\n\n\npercent variance\n0.4268779\n49\npca_XZ5Kg\n\n\npercent variance\n0.4236470\n50\npca_XZ5Kg\n\n\npercent variance\n0.4046961\n51\npca_XZ5Kg\n\n\npercent variance\n0.3994474\n52\npca_XZ5Kg\n\n\npercent variance\n0.3974561\n53\npca_XZ5Kg\n\n\npercent variance\n0.3938208\n54\npca_XZ5Kg\n\n\npercent variance\n0.3858136\n55\npca_XZ5Kg\n\n\npercent variance\n0.3790427\n56\npca_XZ5Kg\n\n\npercent variance\n0.3754027\n57\npca_XZ5Kg\n\n\npercent variance\n0.3707758\n58\npca_XZ5Kg\n\n\npercent variance\n0.3649444\n59\npca_XZ5Kg\n\n\npercent variance\n0.3593006\n60\npca_XZ5Kg\n\n\npercent variance\n0.3523822\n61\npca_XZ5Kg\n\n\npercent variance\n0.3477936\n62\npca_XZ5Kg\n\n\npercent variance\n0.3444113\n63\npca_XZ5Kg\n\n\npercent variance\n0.3398681\n64\npca_XZ5Kg\n\n\npercent variance\n0.3359545\n65\npca_XZ5Kg\n\n\npercent variance\n0.3348860\n66\npca_XZ5Kg\n\n\npercent variance\n0.3318637\n67\npca_XZ5Kg\n\n\npercent variance\n0.3230260\n68\npca_XZ5Kg\n\n\npercent variance\n0.3162770\n69\npca_XZ5Kg\n\n\npercent variance\n0.3132442\n70\npca_XZ5Kg\n\n\npercent variance\n0.3107314\n71\npca_XZ5Kg\n\n\npercent variance\n0.3072429\n72\npca_XZ5Kg\n\n\npercent variance\n0.3049142\n73\npca_XZ5Kg\n\n\npercent variance\n0.3027172\n74\npca_XZ5Kg\n\n\npercent variance\n0.2994853\n75\npca_XZ5Kg\n\n\npercent variance\n0.2977612\n76\npca_XZ5Kg\n\n\npercent variance\n0.2950516\n77\npca_XZ5Kg\n\n\npercent variance\n0.2904381\n78\npca_XZ5Kg\n\n\npercent variance\n0.2868562\n79\npca_XZ5Kg\n\n\npercent variance\n0.2856777\n80\npca_XZ5Kg\n\n\npercent variance\n0.2833978\n81\npca_XZ5Kg\n\n\npercent variance\n0.2826271\n82\npca_XZ5Kg\n\n\npercent variance\n0.2795511\n83\npca_XZ5Kg\n\n\npercent variance\n0.2793054\n84\npca_XZ5Kg\n\n\npercent variance\n0.2785192\n85\npca_XZ5Kg\n\n\npercent variance\n0.2774555\n86\npca_XZ5Kg\n\n\npercent variance\n0.2759008\n87\npca_XZ5Kg\n\n\npercent variance\n0.2742273\n88\npca_XZ5Kg\n\n\npercent variance\n0.2714106\n89\npca_XZ5Kg\n\n\npercent variance\n0.2692629\n90\npca_XZ5Kg\n\n\npercent variance\n0.2664843\n91\npca_XZ5Kg\n\n\npercent variance\n0.2635813\n92\npca_XZ5Kg\n\n\npercent variance\n0.2629616\n93\npca_XZ5Kg\n\n\npercent variance\n0.2610338\n94\npca_XZ5Kg\n\n\npercent variance\n0.2588269\n95\npca_XZ5Kg\n\n\npercent variance\n0.2561764\n96\npca_XZ5Kg\n\n\npercent variance\n0.2538459\n97\npca_XZ5Kg\n\n\npercent variance\n0.2504473\n98\npca_XZ5Kg\n\n\npercent variance\n0.2478294\n99\npca_XZ5Kg\n\n\npercent variance\n0.2450336\n100\npca_XZ5Kg\n\n\npercent variance\n0.2423467\n101\npca_XZ5Kg\n\n\npercent variance\n0.2420638\n102\npca_XZ5Kg\n\n\npercent variance\n0.2388754\n103\npca_XZ5Kg\n\n\npercent variance\n0.2374547\n104\npca_XZ5Kg\n\n\npercent variance\n0.2356084\n105\npca_XZ5Kg\n\n\npercent variance\n0.2330532\n106\npca_XZ5Kg\n\n\npercent variance\n0.2279801\n107\npca_XZ5Kg\n\n\npercent variance\n0.2261742\n108\npca_XZ5Kg\n\n\npercent variance\n0.2228315\n109\npca_XZ5Kg\n\n\npercent variance\n0.2224421\n110\npca_XZ5Kg\n\n\npercent variance\n0.2181694\n111\npca_XZ5Kg\n\n\npercent variance\n0.2172568\n112\npca_XZ5Kg\n\n\npercent variance\n0.2142768\n113\npca_XZ5Kg\n\n\npercent variance\n0.2119377\n114\npca_XZ5Kg\n\n\npercent variance\n0.2109720\n115\npca_XZ5Kg\n\n\npercent variance\n0.2073302\n116\npca_XZ5Kg\n\n\npercent variance\n0.2047611\n117\npca_XZ5Kg\n\n\npercent variance\n0.2043682\n118\npca_XZ5Kg\n\n\npercent variance\n0.2024085\n119\npca_XZ5Kg\n\n\npercent variance\n0.2004622\n120\npca_XZ5Kg\n\n\npercent variance\n0.1988220\n121\npca_XZ5Kg\n\n\npercent variance\n0.1952156\n122\npca_XZ5Kg\n\n\npercent variance\n0.1937373\n123\npca_XZ5Kg\n\n\npercent variance\n0.1921032\n124\npca_XZ5Kg\n\n\npercent variance\n0.1917159\n125\npca_XZ5Kg\n\n\npercent variance\n0.1898021\n126\npca_XZ5Kg\n\n\npercent variance\n0.1870895\n127\npca_XZ5Kg\n\n\npercent variance\n0.1865357\n128\npca_XZ5Kg\n\n\npercent variance\n0.1813200\n129\npca_XZ5Kg\n\n\npercent variance\n0.1800053\n130\npca_XZ5Kg\n\n\npercent variance\n0.1791940\n131\npca_XZ5Kg\n\n\npercent variance\n0.1789732\n132\npca_XZ5Kg\n\n\npercent variance\n0.1769504\n133\npca_XZ5Kg\n\n\npercent variance\n0.1761577\n134\npca_XZ5Kg\n\n\npercent variance\n0.1747968\n135\npca_XZ5Kg\n\n\npercent variance\n0.1729847\n136\npca_XZ5Kg\n\n\npercent variance\n0.1720166\n137\npca_XZ5Kg\n\n\npercent variance\n0.1687271\n138\npca_XZ5Kg\n\n\npercent variance\n0.1685170\n139\npca_XZ5Kg\n\n\npercent variance\n0.1668415\n140\npca_XZ5Kg\n\n\npercent variance\n0.1647184\n141\npca_XZ5Kg\n\n\npercent variance\n0.1645749\n142\npca_XZ5Kg\n\n\npercent variance\n0.1642936\n143\npca_XZ5Kg\n\n\npercent variance\n0.1614858\n144\npca_XZ5Kg\n\n\npercent variance\n0.1604903\n145\npca_XZ5Kg\n\n\npercent variance\n0.1589123\n146\npca_XZ5Kg\n\n\npercent variance\n0.1574895\n147\npca_XZ5Kg\n\n\npercent variance\n0.1559177\n148\npca_XZ5Kg\n\n\npercent variance\n0.1556379\n149\npca_XZ5Kg\n\n\npercent variance\n0.1546657\n150\npca_XZ5Kg\n\n\npercent variance\n0.1540429\n151\npca_XZ5Kg\n\n\npercent variance\n0.1516047\n152\npca_XZ5Kg\n\n\npercent variance\n0.1502716\n153\npca_XZ5Kg\n\n\npercent variance\n0.1487606\n154\npca_XZ5Kg\n\n\npercent variance\n0.1475054\n155\npca_XZ5Kg\n\n\npercent variance\n0.1468202\n156\npca_XZ5Kg\n\n\npercent variance\n0.1458028\n157\npca_XZ5Kg\n\n\npercent variance\n0.1455677\n158\npca_XZ5Kg\n\n\npercent variance\n0.1447368\n159\npca_XZ5Kg\n\n\npercent variance\n0.1428951\n160\npca_XZ5Kg\n\n\npercent variance\n0.1410578\n161\npca_XZ5Kg\n\n\npercent variance\n0.1399386\n162\npca_XZ5Kg\n\n\npercent variance\n0.1397092\n163\npca_XZ5Kg\n\n\npercent variance\n0.1395330\n164\npca_XZ5Kg\n\n\npercent variance\n0.1393550\n165\npca_XZ5Kg\n\n\npercent variance\n0.1392252\n166\npca_XZ5Kg\n\n\npercent variance\n0.1387732\n167\npca_XZ5Kg\n\n\npercent variance\n0.1383397\n168\npca_XZ5Kg\n\n\npercent variance\n0.1378161\n169\npca_XZ5Kg\n\n\npercent variance\n0.1368447\n170\npca_XZ5Kg\n\n\npercent variance\n0.1361655\n171\npca_XZ5Kg\n\n\npercent variance\n0.1358224\n172\npca_XZ5Kg\n\n\npercent variance\n0.1337013\n173\npca_XZ5Kg\n\n\npercent variance\n0.1329054\n174\npca_XZ5Kg\n\n\npercent variance\n0.1310594\n175\npca_XZ5Kg\n\n\npercent variance\n0.1302932\n176\npca_XZ5Kg\n\n\npercent variance\n0.1293243\n177\npca_XZ5Kg\n\n\npercent variance\n0.1282407\n178\npca_XZ5Kg\n\n\npercent variance\n0.1274068\n179\npca_XZ5Kg\n\n\npercent variance\n0.1268222\n180\npca_XZ5Kg\n\n\npercent variance\n0.1253217\n181\npca_XZ5Kg\n\n\npercent variance\n0.1240448\n182\npca_XZ5Kg\n\n\npercent variance\n0.1229835\n183\npca_XZ5Kg\n\n\npercent variance\n0.1216180\n184\npca_XZ5Kg\n\n\npercent variance\n0.1215063\n185\npca_XZ5Kg\n\n\npercent variance\n0.1200025\n186\npca_XZ5Kg\n\n\npercent variance\n0.1190869\n187\npca_XZ5Kg\n\n\npercent variance\n0.1188945\n188\npca_XZ5Kg\n\n\npercent variance\n0.1176042\n189\npca_XZ5Kg\n\n\npercent variance\n0.1167602\n190\npca_XZ5Kg\n\n\npercent variance\n0.1162339\n191\npca_XZ5Kg\n\n\npercent variance\n0.1147983\n192\npca_XZ5Kg\n\n\npercent variance\n0.1137035\n193\npca_XZ5Kg\n\n\npercent variance\n0.1127199\n194\npca_XZ5Kg\n\n\npercent variance\n0.1120552\n195\npca_XZ5Kg\n\n\npercent variance\n0.1094088\n196\npca_XZ5Kg\n\n\npercent variance\n0.1085456\n197\npca_XZ5Kg\n\n\npercent variance\n0.1070285\n198\npca_XZ5Kg\n\n\npercent variance\n0.1064011\n199\npca_XZ5Kg\n\n\npercent variance\n0.1054745\n200\npca_XZ5Kg\n\n\npercent variance\n0.1046285\n201\npca_XZ5Kg\n\n\npercent variance\n0.1036038\n202\npca_XZ5Kg\n\n\npercent variance\n0.1032681\n203\npca_XZ5Kg\n\n\npercent variance\n0.1020732\n204\npca_XZ5Kg\n\n\npercent variance\n0.1008248\n205\npca_XZ5Kg\n\n\npercent variance\n0.1001579\n206\npca_XZ5Kg\n\n\npercent variance\n0.0997859\n207\npca_XZ5Kg\n\n\npercent variance\n0.0991933\n208\npca_XZ5Kg\n\n\npercent variance\n0.0977275\n209\npca_XZ5Kg\n\n\npercent variance\n0.0968494\n210\npca_XZ5Kg\n\n\npercent variance\n0.0958049\n211\npca_XZ5Kg\n\n\npercent variance\n0.0956368\n212\npca_XZ5Kg\n\n\npercent variance\n0.0947728\n213\npca_XZ5Kg\n\n\npercent variance\n0.0946473\n214\npca_XZ5Kg\n\n\npercent variance\n0.0943705\n215\npca_XZ5Kg\n\n\npercent variance\n0.0926746\n216\npca_XZ5Kg\n\n\npercent variance\n0.0917402\n217\npca_XZ5Kg\n\n\npercent variance\n0.0908678\n218\npca_XZ5Kg\n\n\npercent variance\n0.0897470\n219\npca_XZ5Kg\n\n\npercent variance\n0.0891567\n220\npca_XZ5Kg\n\n\npercent variance\n0.0887811\n221\npca_XZ5Kg\n\n\npercent variance\n0.0878886\n222\npca_XZ5Kg\n\n\npercent variance\n0.0869156\n223\npca_XZ5Kg\n\n\npercent variance\n0.0862124\n224\npca_XZ5Kg\n\n\npercent variance\n0.0860558\n225\npca_XZ5Kg\n\n\npercent variance\n0.0845353\n226\npca_XZ5Kg\n\n\npercent variance\n0.0840092\n227\npca_XZ5Kg\n\n\npercent variance\n0.0835726\n228\npca_XZ5Kg\n\n\npercent variance\n0.0824022\n229\npca_XZ5Kg\n\n\npercent variance\n0.0820780\n230\npca_XZ5Kg\n\n\npercent variance\n0.0812968\n231\npca_XZ5Kg\n\n\npercent variance\n0.0810186\n232\npca_XZ5Kg\n\n\npercent variance\n0.0791971\n233\npca_XZ5Kg\n\n\npercent variance\n0.0783284\n234\npca_XZ5Kg\n\n\npercent variance\n0.0778764\n235\npca_XZ5Kg\n\n\npercent variance\n0.0771504\n236\npca_XZ5Kg\n\n\npercent variance\n0.0766372\n237\npca_XZ5Kg\n\n\npercent variance\n0.0750170\n238\npca_XZ5Kg\n\n\npercent variance\n0.0743536\n239\npca_XZ5Kg\n\n\npercent variance\n0.0738994\n240\npca_XZ5Kg\n\n\npercent variance\n0.0732431\n241\npca_XZ5Kg\n\n\npercent variance\n0.0727074\n242\npca_XZ5Kg\n\n\npercent variance\n0.0725536\n243\npca_XZ5Kg\n\n\npercent variance\n0.0716773\n244\npca_XZ5Kg\n\n\npercent variance\n0.0711041\n245\npca_XZ5Kg\n\n\npercent variance\n0.0702519\n246\npca_XZ5Kg\n\n\npercent variance\n0.0694518\n247\npca_XZ5Kg\n\n\npercent variance\n0.0694157\n248\npca_XZ5Kg\n\n\npercent variance\n0.0679467\n249\npca_XZ5Kg\n\n\npercent variance\n0.0678416\n250\npca_XZ5Kg\n\n\npercent variance\n0.0676395\n251\npca_XZ5Kg\n\n\npercent variance\n0.0671492\n252\npca_XZ5Kg\n\n\npercent variance\n0.0663286\n253\npca_XZ5Kg\n\n\npercent variance\n0.0655073\n254\npca_XZ5Kg\n\n\npercent variance\n0.0651828\n255\npca_XZ5Kg\n\n\npercent variance\n0.0644496\n256\npca_XZ5Kg\n\n\npercent variance\n0.0641337\n257\npca_XZ5Kg\n\n\npercent variance\n0.0636327\n258\npca_XZ5Kg\n\n\npercent variance\n0.0632129\n259\npca_XZ5Kg\n\n\npercent variance\n0.0627868\n260\npca_XZ5Kg\n\n\npercent variance\n0.0624060\n261\npca_XZ5Kg\n\n\npercent variance\n0.0620206\n262\npca_XZ5Kg\n\n\npercent variance\n0.0609022\n263\npca_XZ5Kg\n\n\npercent variance\n0.0605853\n264\npca_XZ5Kg\n\n\npercent variance\n0.0598580\n265\npca_XZ5Kg\n\n\npercent variance\n0.0591857\n266\npca_XZ5Kg\n\n\npercent variance\n0.0583652\n267\npca_XZ5Kg\n\n\npercent variance\n0.0580762\n268\npca_XZ5Kg\n\n\npercent variance\n0.0575234\n269\npca_XZ5Kg\n\n\npercent variance\n0.0572629\n270\npca_XZ5Kg\n\n\npercent variance\n0.0569686\n271\npca_XZ5Kg\n\n\npercent variance\n0.0563930\n272\npca_XZ5Kg\n\n\npercent variance\n0.0558807\n273\npca_XZ5Kg\n\n\npercent variance\n0.0556567\n274\npca_XZ5Kg\n\n\npercent variance\n0.0551707\n275\npca_XZ5Kg\n\n\npercent variance\n0.0550204\n276\npca_XZ5Kg\n\n\npercent variance\n0.0544644\n277\npca_XZ5Kg\n\n\npercent variance\n0.0540724\n278\npca_XZ5Kg\n\n\npercent variance\n0.0538777\n279\npca_XZ5Kg\n\n\npercent variance\n0.0532445\n280\npca_XZ5Kg\n\n\npercent variance\n0.0529544\n281\npca_XZ5Kg\n\n\npercent variance\n0.0526201\n282\npca_XZ5Kg\n\n\npercent variance\n0.0522384\n283\npca_XZ5Kg\n\n\npercent variance\n0.0518818\n284\npca_XZ5Kg\n\n\npercent variance\n0.0514805\n285\npca_XZ5Kg\n\n\npercent variance\n0.0508333\n286\npca_XZ5Kg\n\n\npercent variance\n0.0507345\n287\npca_XZ5Kg\n\n\npercent variance\n0.0503105\n288\npca_XZ5Kg\n\n\npercent variance\n0.0500768\n289\npca_XZ5Kg\n\n\npercent variance\n0.0497042\n290\npca_XZ5Kg\n\n\npercent variance\n0.0492309\n291\npca_XZ5Kg\n\n\npercent variance\n0.0482837\n292\npca_XZ5Kg\n\n\npercent variance\n0.0479555\n293\npca_XZ5Kg\n\n\npercent variance\n0.0476913\n294\npca_XZ5Kg\n\n\npercent variance\n0.0476003\n295\npca_XZ5Kg\n\n\npercent variance\n0.0471105\n296\npca_XZ5Kg\n\n\npercent variance\n0.0466000\n297\npca_XZ5Kg\n\n\npercent variance\n0.0458822\n298\npca_XZ5Kg\n\n\npercent variance\n0.0456748\n299\npca_XZ5Kg\n\n\npercent variance\n0.0449407\n300\npca_XZ5Kg\n\n\npercent variance\n0.0446582\n301\npca_XZ5Kg\n\n\npercent variance\n0.0442656\n302\npca_XZ5Kg\n\n\npercent variance\n0.0438385\n303\npca_XZ5Kg\n\n\npercent variance\n0.0438195\n304\npca_XZ5Kg\n\n\npercent variance\n0.0432325\n305\npca_XZ5Kg\n\n\npercent variance\n0.0430700\n306\npca_XZ5Kg\n\n\npercent variance\n0.0424771\n307\npca_XZ5Kg\n\n\npercent variance\n0.0421255\n308\npca_XZ5Kg\n\n\npercent variance\n0.0419234\n309\npca_XZ5Kg\n\n\npercent variance\n0.0415179\n310\npca_XZ5Kg\n\n\npercent variance\n0.0413275\n311\npca_XZ5Kg\n\n\npercent variance\n0.0410348\n312\npca_XZ5Kg\n\n\npercent variance\n0.0408452\n313\npca_XZ5Kg\n\n\npercent variance\n0.0403769\n314\npca_XZ5Kg\n\n\npercent variance\n0.0401531\n315\npca_XZ5Kg\n\n\npercent variance\n0.0398452\n316\npca_XZ5Kg\n\n\npercent variance\n0.0395260\n317\npca_XZ5Kg\n\n\npercent variance\n0.0392093\n318\npca_XZ5Kg\n\n\npercent variance\n0.0390083\n319\npca_XZ5Kg\n\n\npercent variance\n0.0386734\n320\npca_XZ5Kg\n\n\npercent variance\n0.0386207\n321\npca_XZ5Kg\n\n\npercent variance\n0.0383034\n322\npca_XZ5Kg\n\n\npercent variance\n0.0379053\n323\npca_XZ5Kg\n\n\npercent variance\n0.0376053\n324\npca_XZ5Kg\n\n\npercent variance\n0.0375230\n325\npca_XZ5Kg\n\n\npercent variance\n0.0370519\n326\npca_XZ5Kg\n\n\npercent variance\n0.0366553\n327\npca_XZ5Kg\n\n\npercent variance\n0.0363809\n328\npca_XZ5Kg\n\n\npercent variance\n0.0359468\n329\npca_XZ5Kg\n\n\npercent variance\n0.0358257\n330\npca_XZ5Kg\n\n\npercent variance\n0.0355090\n331\npca_XZ5Kg\n\n\npercent variance\n0.0353727\n332\npca_XZ5Kg\n\n\npercent variance\n0.0352500\n333\npca_XZ5Kg\n\n\npercent variance\n0.0348450\n334\npca_XZ5Kg\n\n\npercent variance\n0.0345271\n335\npca_XZ5Kg\n\n\npercent variance\n0.0341972\n336\npca_XZ5Kg\n\n\npercent variance\n0.0338996\n337\npca_XZ5Kg\n\n\npercent variance\n0.0336546\n338\npca_XZ5Kg\n\n\npercent variance\n0.0334798\n339\npca_XZ5Kg\n\n\npercent variance\n0.0332341\n340\npca_XZ5Kg\n\n\npercent variance\n0.0330096\n341\npca_XZ5Kg\n\n\npercent variance\n0.0328801\n342\npca_XZ5Kg\n\n\npercent variance\n0.0324949\n343\npca_XZ5Kg\n\n\npercent variance\n0.0323091\n344\npca_XZ5Kg\n\n\npercent variance\n0.0321550\n345\npca_XZ5Kg\n\n\npercent variance\n0.0319330\n346\npca_XZ5Kg\n\n\npercent variance\n0.0314955\n347\npca_XZ5Kg\n\n\npercent variance\n0.0311147\n348\npca_XZ5Kg\n\n\npercent variance\n0.0309910\n349\npca_XZ5Kg\n\n\npercent variance\n0.0308832\n350\npca_XZ5Kg\n\n\npercent variance\n0.0306476\n351\npca_XZ5Kg\n\n\npercent variance\n0.0304602\n352\npca_XZ5Kg\n\n\npercent variance\n0.0302166\n353\npca_XZ5Kg\n\n\npercent variance\n0.0300485\n354\npca_XZ5Kg\n\n\npercent variance\n0.0297699\n355\npca_XZ5Kg\n\n\npercent variance\n0.0296507\n356\npca_XZ5Kg\n\n\npercent variance\n0.0294382\n357\npca_XZ5Kg\n\n\npercent variance\n0.0290275\n358\npca_XZ5Kg\n\n\npercent variance\n0.0289892\n359\npca_XZ5Kg\n\n\npercent variance\n0.0287456\n360\npca_XZ5Kg\n\n\npercent variance\n0.0285534\n361\npca_XZ5Kg\n\n\npercent variance\n0.0283583\n362\npca_XZ5Kg\n\n\npercent variance\n0.0282367\n363\npca_XZ5Kg\n\n\npercent variance\n0.0280189\n364\npca_XZ5Kg\n\n\npercent variance\n0.0277317\n365\npca_XZ5Kg\n\n\npercent variance\n0.0276485\n366\npca_XZ5Kg\n\n\npercent variance\n0.0272504\n367\npca_XZ5Kg\n\n\npercent variance\n0.0270523\n368\npca_XZ5Kg\n\n\npercent variance\n0.0270038\n369\npca_XZ5Kg\n\n\npercent variance\n0.0266002\n370\npca_XZ5Kg\n\n\npercent variance\n0.0265727\n371\npca_XZ5Kg\n\n\npercent variance\n0.0262995\n372\npca_XZ5Kg\n\n\npercent variance\n0.0260022\n373\npca_XZ5Kg\n\n\npercent variance\n0.0259336\n374\npca_XZ5Kg\n\n\npercent variance\n0.0255641\n375\npca_XZ5Kg\n\n\npercent variance\n0.0253839\n376\npca_XZ5Kg\n\n\npercent variance\n0.0253071\n377\npca_XZ5Kg\n\n\npercent variance\n0.0251249\n378\npca_XZ5Kg\n\n\npercent variance\n0.0249732\n379\npca_XZ5Kg\n\n\npercent variance\n0.0248269\n380\npca_XZ5Kg\n\n\npercent variance\n0.0244425\n381\npca_XZ5Kg\n\n\npercent variance\n0.0244022\n382\npca_XZ5Kg\n\n\npercent variance\n0.0243260\n383\npca_XZ5Kg\n\n\npercent variance\n0.0242242\n384\npca_XZ5Kg\n\n\npercent variance\n0.0237773\n385\npca_XZ5Kg\n\n\npercent variance\n0.0237021\n386\npca_XZ5Kg\n\n\npercent variance\n0.0236187\n387\npca_XZ5Kg\n\n\npercent variance\n0.0234955\n388\npca_XZ5Kg\n\n\npercent variance\n0.0233115\n389\npca_XZ5Kg\n\n\npercent variance\n0.0230621\n390\npca_XZ5Kg\n\n\npercent variance\n0.0229707\n391\npca_XZ5Kg\n\n\npercent variance\n0.0229059\n392\npca_XZ5Kg\n\n\npercent variance\n0.0227135\n393\npca_XZ5Kg\n\n\npercent variance\n0.0225591\n394\npca_XZ5Kg\n\n\npercent variance\n0.0224797\n395\npca_XZ5Kg\n\n\npercent variance\n0.0224158\n396\npca_XZ5Kg\n\n\npercent variance\n0.0221744\n397\npca_XZ5Kg\n\n\npercent variance\n0.0221393\n398\npca_XZ5Kg\n\n\npercent variance\n0.0220104\n399\npca_XZ5Kg\n\n\npercent variance\n0.0218274\n400\npca_XZ5Kg\n\n\npercent variance\n0.0217179\n401\npca_XZ5Kg\n\n\npercent variance\n0.0216259\n402\npca_XZ5Kg\n\n\npercent variance\n0.0214719\n403\npca_XZ5Kg\n\n\npercent variance\n0.0213356\n404\npca_XZ5Kg\n\n\npercent variance\n0.0211567\n405\npca_XZ5Kg\n\n\npercent variance\n0.0210481\n406\npca_XZ5Kg\n\n\npercent variance\n0.0208622\n407\npca_XZ5Kg\n\n\npercent variance\n0.0207084\n408\npca_XZ5Kg\n\n\npercent variance\n0.0206225\n409\npca_XZ5Kg\n\n\npercent variance\n0.0203735\n410\npca_XZ5Kg\n\n\npercent variance\n0.0202692\n411\npca_XZ5Kg\n\n\npercent variance\n0.0199951\n412\npca_XZ5Kg\n\n\npercent variance\n0.0199834\n413\npca_XZ5Kg\n\n\npercent variance\n0.0198223\n414\npca_XZ5Kg\n\n\npercent variance\n0.0198037\n415\npca_XZ5Kg\n\n\npercent variance\n0.0196254\n416\npca_XZ5Kg\n\n\npercent variance\n0.0195397\n417\npca_XZ5Kg\n\n\npercent variance\n0.0190573\n418\npca_XZ5Kg\n\n\npercent variance\n0.0189984\n419\npca_XZ5Kg\n\n\npercent variance\n0.0189109\n420\npca_XZ5Kg\n\n\npercent variance\n0.0188644\n421\npca_XZ5Kg\n\n\npercent variance\n0.0187215\n422\npca_XZ5Kg\n\n\npercent variance\n0.0185164\n423\npca_XZ5Kg\n\n\npercent variance\n0.0184912\n424\npca_XZ5Kg\n\n\npercent variance\n0.0183366\n425\npca_XZ5Kg\n\n\npercent variance\n0.0180638\n426\npca_XZ5Kg\n\n\npercent variance\n0.0179137\n427\npca_XZ5Kg\n\n\npercent variance\n0.0179070\n428\npca_XZ5Kg\n\n\npercent variance\n0.0177557\n429\npca_XZ5Kg\n\n\npercent variance\n0.0176246\n430\npca_XZ5Kg\n\n\npercent variance\n0.0176034\n431\npca_XZ5Kg\n\n\npercent variance\n0.0175095\n432\npca_XZ5Kg\n\n\npercent variance\n0.0173225\n433\npca_XZ5Kg\n\n\npercent variance\n0.0173140\n434\npca_XZ5Kg\n\n\npercent variance\n0.0171066\n435\npca_XZ5Kg\n\n\npercent variance\n0.0170430\n436\npca_XZ5Kg\n\n\npercent variance\n0.0170103\n437\npca_XZ5Kg\n\n\npercent variance\n0.0168660\n438\npca_XZ5Kg\n\n\npercent variance\n0.0168279\n439\npca_XZ5Kg\n\n\npercent variance\n0.0166931\n440\npca_XZ5Kg\n\n\npercent variance\n0.0165987\n441\npca_XZ5Kg\n\n\npercent variance\n0.0165434\n442\npca_XZ5Kg\n\n\npercent variance\n0.0164738\n443\npca_XZ5Kg\n\n\npercent variance\n0.0163828\n444\npca_XZ5Kg\n\n\npercent variance\n0.0163646\n445\npca_XZ5Kg\n\n\npercent variance\n0.0162792\n446\npca_XZ5Kg\n\n\npercent variance\n0.0161606\n447\npca_XZ5Kg\n\n\npercent variance\n0.0160560\n448\npca_XZ5Kg\n\n\npercent variance\n0.0159098\n449\npca_XZ5Kg\n\n\npercent variance\n0.0157161\n450\npca_XZ5Kg\n\n\npercent variance\n0.0156415\n451\npca_XZ5Kg\n\n\npercent variance\n0.0155452\n452\npca_XZ5Kg\n\n\npercent variance\n0.0153885\n453\npca_XZ5Kg\n\n\npercent variance\n0.0153679\n454\npca_XZ5Kg\n\n\npercent variance\n0.0152370\n455\npca_XZ5Kg\n\n\npercent variance\n0.0151249\n456\npca_XZ5Kg\n\n\npercent variance\n0.0149945\n457\npca_XZ5Kg\n\n\npercent variance\n0.0149403\n458\npca_XZ5Kg\n\n\npercent variance\n0.0149100\n459\npca_XZ5Kg\n\n\npercent variance\n0.0147983\n460\npca_XZ5Kg\n\n\npercent variance\n0.0147274\n461\npca_XZ5Kg\n\n\npercent variance\n0.0147218\n462\npca_XZ5Kg\n\n\npercent variance\n0.0146736\n463\npca_XZ5Kg\n\n\npercent variance\n0.0144912\n464\npca_XZ5Kg\n\n\npercent variance\n0.0143608\n465\npca_XZ5Kg\n\n\npercent variance\n0.0143483\n466\npca_XZ5Kg\n\n\npercent variance\n0.0143001\n467\npca_XZ5Kg\n\n\npercent variance\n0.0141532\n468\npca_XZ5Kg\n\n\npercent variance\n0.0141228\n469\npca_XZ5Kg\n\n\npercent variance\n0.0140885\n470\npca_XZ5Kg\n\n\npercent variance\n0.0139593\n471\npca_XZ5Kg\n\n\npercent variance\n0.0139453\n472\npca_XZ5Kg\n\n\npercent variance\n0.0138260\n473\npca_XZ5Kg\n\n\npercent variance\n0.0137614\n474\npca_XZ5Kg\n\n\npercent variance\n0.0136848\n475\npca_XZ5Kg\n\n\npercent variance\n0.0136156\n476\npca_XZ5Kg\n\n\npercent variance\n0.0134200\n477\npca_XZ5Kg\n\n\npercent variance\n0.0133990\n478\npca_XZ5Kg\n\n\npercent variance\n0.0133461\n479\npca_XZ5Kg\n\n\npercent variance\n0.0131737\n480\npca_XZ5Kg\n\n\npercent variance\n0.0131280\n481\npca_XZ5Kg\n\n\npercent variance\n0.0131021\n482\npca_XZ5Kg\n\n\npercent variance\n0.0130364\n483\npca_XZ5Kg\n\n\npercent variance\n0.0129885\n484\npca_XZ5Kg\n\n\npercent variance\n0.0129369\n485\npca_XZ5Kg\n\n\npercent variance\n0.0128130\n486\npca_XZ5Kg\n\n\npercent variance\n0.0126939\n487\npca_XZ5Kg\n\n\npercent variance\n0.0126560\n488\npca_XZ5Kg\n\n\npercent variance\n0.0125734\n489\npca_XZ5Kg\n\n\npercent variance\n0.0125052\n490\npca_XZ5Kg\n\n\npercent variance\n0.0124432\n491\npca_XZ5Kg\n\n\npercent variance\n0.0123746\n492\npca_XZ5Kg\n\n\npercent variance\n0.0122811\n493\npca_XZ5Kg\n\n\npercent variance\n0.0122771\n494\npca_XZ5Kg\n\n\npercent variance\n0.0121971\n495\npca_XZ5Kg\n\n\npercent variance\n0.0121836\n496\npca_XZ5Kg\n\n\npercent variance\n0.0120348\n497\npca_XZ5Kg\n\n\npercent variance\n0.0120179\n498\npca_XZ5Kg\n\n\npercent variance\n0.0119383\n499\npca_XZ5Kg\n\n\npercent variance\n0.0118833\n500\npca_XZ5Kg\n\n\npercent variance\n0.0118056\n501\npca_XZ5Kg\n\n\npercent variance\n0.0117785\n502\npca_XZ5Kg\n\n\npercent variance\n0.0116696\n503\npca_XZ5Kg\n\n\npercent variance\n0.0115879\n504\npca_XZ5Kg\n\n\npercent variance\n0.0115146\n505\npca_XZ5Kg\n\n\npercent variance\n0.0114154\n506\npca_XZ5Kg\n\n\npercent variance\n0.0113904\n507\npca_XZ5Kg\n\n\npercent variance\n0.0113184\n508\npca_XZ5Kg\n\n\npercent variance\n0.0112500\n509\npca_XZ5Kg\n\n\npercent variance\n0.0112235\n510\npca_XZ5Kg\n\n\npercent variance\n0.0111379\n511\npca_XZ5Kg\n\n\npercent variance\n0.0110944\n512\npca_XZ5Kg\n\n\npercent variance\n0.0109817\n513\npca_XZ5Kg\n\n\npercent variance\n0.0109406\n514\npca_XZ5Kg\n\n\npercent variance\n0.0108832\n515\npca_XZ5Kg\n\n\npercent variance\n0.0108371\n516\npca_XZ5Kg\n\n\npercent variance\n0.0107845\n517\npca_XZ5Kg\n\n\npercent variance\n0.0107158\n518\npca_XZ5Kg\n\n\npercent variance\n0.0107099\n519\npca_XZ5Kg\n\n\npercent variance\n0.0106894\n520\npca_XZ5Kg\n\n\npercent variance\n0.0106350\n521\npca_XZ5Kg\n\n\npercent variance\n0.0105643\n522\npca_XZ5Kg\n\n\npercent variance\n0.0105282\n523\npca_XZ5Kg\n\n\npercent variance\n0.0104765\n524\npca_XZ5Kg\n\n\npercent variance\n0.0104117\n525\npca_XZ5Kg\n\n\npercent variance\n0.0103268\n526\npca_XZ5Kg\n\n\npercent variance\n0.0102978\n527\npca_XZ5Kg\n\n\npercent variance\n0.0102552\n528\npca_XZ5Kg\n\n\npercent variance\n0.0102220\n529\npca_XZ5Kg\n\n\npercent variance\n0.0101724\n530\npca_XZ5Kg\n\n\npercent variance\n0.0100701\n531\npca_XZ5Kg\n\n\npercent variance\n0.0100012\n532\npca_XZ5Kg\n\n\npercent variance\n0.0099818\n533\npca_XZ5Kg\n\n\npercent variance\n0.0099066\n534\npca_XZ5Kg\n\n\npercent variance\n0.0098849\n535\npca_XZ5Kg\n\n\npercent variance\n0.0098674\n536\npca_XZ5Kg\n\n\npercent variance\n0.0097393\n537\npca_XZ5Kg\n\n\npercent variance\n0.0097177\n538\npca_XZ5Kg\n\n\npercent variance\n0.0096515\n539\npca_XZ5Kg\n\n\npercent variance\n0.0096133\n540\npca_XZ5Kg\n\n\npercent variance\n0.0095635\n541\npca_XZ5Kg\n\n\npercent variance\n0.0095049\n542\npca_XZ5Kg\n\n\npercent variance\n0.0094561\n543\npca_XZ5Kg\n\n\npercent variance\n0.0094032\n544\npca_XZ5Kg\n\n\npercent variance\n0.0093878\n545\npca_XZ5Kg\n\n\npercent variance\n0.0092796\n546\npca_XZ5Kg\n\n\npercent variance\n0.0092409\n547\npca_XZ5Kg\n\n\npercent variance\n0.0092365\n548\npca_XZ5Kg\n\n\npercent variance\n0.0092122\n549\npca_XZ5Kg\n\n\npercent variance\n0.0090877\n550\npca_XZ5Kg\n\n\npercent variance\n0.0090250\n551\npca_XZ5Kg\n\n\npercent variance\n0.0089652\n552\npca_XZ5Kg\n\n\npercent variance\n0.0088776\n553\npca_XZ5Kg\n\n\npercent variance\n0.0088340\n554\npca_XZ5Kg\n\n\npercent variance\n0.0087908\n555\npca_XZ5Kg\n\n\npercent variance\n0.0087514\n556\npca_XZ5Kg\n\n\npercent variance\n0.0087479\n557\npca_XZ5Kg\n\n\npercent variance\n0.0086795\n558\npca_XZ5Kg\n\n\npercent variance\n0.0086144\n559\npca_XZ5Kg\n\n\npercent variance\n0.0085302\n560\npca_XZ5Kg\n\n\npercent variance\n0.0084786\n561\npca_XZ5Kg\n\n\npercent variance\n0.0084528\n562\npca_XZ5Kg\n\n\npercent variance\n0.0084439\n563\npca_XZ5Kg\n\n\npercent variance\n0.0084148\n564\npca_XZ5Kg\n\n\npercent variance\n0.0083726\n565\npca_XZ5Kg\n\n\npercent variance\n0.0083217\n566\npca_XZ5Kg\n\n\npercent variance\n0.0082146\n567\npca_XZ5Kg\n\n\npercent variance\n0.0082095\n568\npca_XZ5Kg\n\n\npercent variance\n0.0081660\n569\npca_XZ5Kg\n\n\npercent variance\n0.0080883\n570\npca_XZ5Kg\n\n\npercent variance\n0.0080673\n571\npca_XZ5Kg\n\n\npercent variance\n0.0080402\n572\npca_XZ5Kg\n\n\npercent variance\n0.0079882\n573\npca_XZ5Kg\n\n\npercent variance\n0.0079331\n574\npca_XZ5Kg\n\n\npercent variance\n0.0078554\n575\npca_XZ5Kg\n\n\npercent variance\n0.0078282\n576\npca_XZ5Kg\n\n\npercent variance\n0.0078138\n577\npca_XZ5Kg\n\n\npercent variance\n0.0077663\n578\npca_XZ5Kg\n\n\npercent variance\n0.0077226\n579\npca_XZ5Kg\n\n\npercent variance\n0.0076527\n580\npca_XZ5Kg\n\n\npercent variance\n0.0075866\n581\npca_XZ5Kg\n\n\npercent variance\n0.0075664\n582\npca_XZ5Kg\n\n\npercent variance\n0.0075554\n583\npca_XZ5Kg\n\n\npercent variance\n0.0074734\n584\npca_XZ5Kg\n\n\npercent variance\n0.0074591\n585\npca_XZ5Kg\n\n\npercent variance\n0.0074295\n586\npca_XZ5Kg\n\n\npercent variance\n0.0073403\n587\npca_XZ5Kg\n\n\npercent variance\n0.0073299\n588\npca_XZ5Kg\n\n\npercent variance\n0.0072605\n589\npca_XZ5Kg\n\n\npercent variance\n0.0072574\n590\npca_XZ5Kg\n\n\npercent variance\n0.0071797\n591\npca_XZ5Kg\n\n\npercent variance\n0.0071704\n592\npca_XZ5Kg\n\n\npercent variance\n0.0071195\n593\npca_XZ5Kg\n\n\npercent variance\n0.0070501\n594\npca_XZ5Kg\n\n\npercent variance\n0.0069611\n595\npca_XZ5Kg\n\n\npercent variance\n0.0069422\n596\npca_XZ5Kg\n\n\npercent variance\n0.0069089\n597\npca_XZ5Kg\n\n\npercent variance\n0.0068963\n598\npca_XZ5Kg\n\n\npercent variance\n0.0068285\n599\npca_XZ5Kg\n\n\npercent variance\n0.0067887\n600\npca_XZ5Kg\n\n\npercent variance\n0.0067567\n601\npca_XZ5Kg\n\n\npercent variance\n0.0067505\n602\npca_XZ5Kg\n\n\npercent variance\n0.0067106\n603\npca_XZ5Kg\n\n\npercent variance\n0.0066730\n604\npca_XZ5Kg\n\n\npercent variance\n0.0066524\n605\npca_XZ5Kg\n\n\npercent variance\n0.0065756\n606\npca_XZ5Kg\n\n\npercent variance\n0.0065513\n607\npca_XZ5Kg\n\n\npercent variance\n0.0065249\n608\npca_XZ5Kg\n\n\npercent variance\n0.0064736\n609\npca_XZ5Kg\n\n\npercent variance\n0.0064219\n610\npca_XZ5Kg\n\n\npercent variance\n0.0063971\n611\npca_XZ5Kg\n\n\npercent variance\n0.0063371\n612\npca_XZ5Kg\n\n\npercent variance\n0.0062915\n613\npca_XZ5Kg\n\n\npercent variance\n0.0062626\n614\npca_XZ5Kg\n\n\npercent variance\n0.0062059\n615\npca_XZ5Kg\n\n\npercent variance\n0.0061888\n616\npca_XZ5Kg\n\n\npercent variance\n0.0061607\n617\npca_XZ5Kg\n\n\npercent variance\n0.0061100\n618\npca_XZ5Kg\n\n\npercent variance\n0.0060731\n619\npca_XZ5Kg\n\n\npercent variance\n0.0060481\n620\npca_XZ5Kg\n\n\npercent variance\n0.0060123\n621\npca_XZ5Kg\n\n\npercent variance\n0.0059957\n622\npca_XZ5Kg\n\n\npercent variance\n0.0059317\n623\npca_XZ5Kg\n\n\npercent variance\n0.0059043\n624\npca_XZ5Kg\n\n\npercent variance\n0.0058706\n625\npca_XZ5Kg\n\n\npercent variance\n0.0058577\n626\npca_XZ5Kg\n\n\npercent variance\n0.0058015\n627\npca_XZ5Kg\n\n\npercent variance\n0.0057955\n628\npca_XZ5Kg\n\n\npercent variance\n0.0057276\n629\npca_XZ5Kg\n\n\npercent variance\n0.0057034\n630\npca_XZ5Kg\n\n\npercent variance\n0.0056647\n631\npca_XZ5Kg\n\n\npercent variance\n0.0056402\n632\npca_XZ5Kg\n\n\npercent variance\n0.0056266\n633\npca_XZ5Kg\n\n\npercent variance\n0.0055989\n634\npca_XZ5Kg\n\n\npercent variance\n0.0055727\n635\npca_XZ5Kg\n\n\npercent variance\n0.0055582\n636\npca_XZ5Kg\n\n\npercent variance\n0.0055052\n637\npca_XZ5Kg\n\n\npercent variance\n0.0054918\n638\npca_XZ5Kg\n\n\npercent variance\n0.0054350\n639\npca_XZ5Kg\n\n\npercent variance\n0.0053897\n640\npca_XZ5Kg\n\n\npercent variance\n0.0053489\n641\npca_XZ5Kg\n\n\npercent variance\n0.0053390\n642\npca_XZ5Kg\n\n\npercent variance\n0.0052816\n643\npca_XZ5Kg\n\n\npercent variance\n0.0052300\n644\npca_XZ5Kg\n\n\npercent variance\n0.0052000\n645\npca_XZ5Kg\n\n\npercent variance\n0.0051760\n646\npca_XZ5Kg\n\n\npercent variance\n0.0051233\n647\npca_XZ5Kg\n\n\npercent variance\n0.0050992\n648\npca_XZ5Kg\n\n\npercent variance\n0.0050471\n649\npca_XZ5Kg\n\n\npercent variance\n0.0050200\n650\npca_XZ5Kg\n\n\npercent variance\n0.0050092\n651\npca_XZ5Kg\n\n\npercent variance\n0.0049460\n652\npca_XZ5Kg\n\n\npercent variance\n0.0049375\n653\npca_XZ5Kg\n\n\npercent variance\n0.0048864\n654\npca_XZ5Kg\n\n\npercent variance\n0.0048604\n655\npca_XZ5Kg\n\n\npercent variance\n0.0048271\n656\npca_XZ5Kg\n\n\npercent variance\n0.0048150\n657\npca_XZ5Kg\n\n\npercent variance\n0.0047856\n658\npca_XZ5Kg\n\n\npercent variance\n0.0047627\n659\npca_XZ5Kg\n\n\npercent variance\n0.0047355\n660\npca_XZ5Kg\n\n\npercent variance\n0.0046490\n661\npca_XZ5Kg\n\n\npercent variance\n0.0046084\n662\npca_XZ5Kg\n\n\npercent variance\n0.0046069\n663\npca_XZ5Kg\n\n\npercent variance\n0.0045827\n664\npca_XZ5Kg\n\n\npercent variance\n0.0045280\n665\npca_XZ5Kg\n\n\npercent variance\n0.0045063\n666\npca_XZ5Kg\n\n\npercent variance\n0.0044774\n667\npca_XZ5Kg\n\n\npercent variance\n0.0044341\n668\npca_XZ5Kg\n\n\npercent variance\n0.0044011\n669\npca_XZ5Kg\n\n\npercent variance\n0.0043658\n670\npca_XZ5Kg\n\n\npercent variance\n0.0043430\n671\npca_XZ5Kg\n\n\npercent variance\n0.0043052\n672\npca_XZ5Kg\n\n\npercent variance\n0.0042687\n673\npca_XZ5Kg\n\n\npercent variance\n0.0041937\n674\npca_XZ5Kg\n\n\npercent variance\n0.0041791\n675\npca_XZ5Kg\n\n\npercent variance\n0.0041554\n676\npca_XZ5Kg\n\n\npercent variance\n0.0041048\n677\npca_XZ5Kg\n\n\npercent variance\n0.0040813\n678\npca_XZ5Kg\n\n\npercent variance\n0.0040283\n679\npca_XZ5Kg\n\n\npercent variance\n0.0039956\n680\npca_XZ5Kg\n\n\npercent variance\n0.0039847\n681\npca_XZ5Kg\n\n\npercent variance\n0.0039483\n682\npca_XZ5Kg\n\n\npercent variance\n0.0038798\n683\npca_XZ5Kg\n\n\npercent variance\n0.0038714\n684\npca_XZ5Kg\n\n\npercent variance\n0.0038274\n685\npca_XZ5Kg\n\n\npercent variance\n0.0038196\n686\npca_XZ5Kg\n\n\npercent variance\n0.0037671\n687\npca_XZ5Kg\n\n\npercent variance\n0.0037438\n688\npca_XZ5Kg\n\n\npercent variance\n0.0037044\n689\npca_XZ5Kg\n\n\npercent variance\n0.0036434\n690\npca_XZ5Kg\n\n\npercent variance\n0.0036372\n691\npca_XZ5Kg\n\n\npercent variance\n0.0035713\n692\npca_XZ5Kg\n\n\npercent variance\n0.0035447\n693\npca_XZ5Kg\n\n\npercent variance\n0.0035013\n694\npca_XZ5Kg\n\n\npercent variance\n0.0034676\n695\npca_XZ5Kg\n\n\npercent variance\n0.0034124\n696\npca_XZ5Kg\n\n\npercent variance\n0.0033782\n697\npca_XZ5Kg\n\n\npercent variance\n0.0033406\n698\npca_XZ5Kg\n\n\npercent variance\n0.0033188\n699\npca_XZ5Kg\n\n\npercent variance\n0.0032897\n700\npca_XZ5Kg\n\n\npercent variance\n0.0032546\n701\npca_XZ5Kg\n\n\npercent variance\n0.0032076\n702\npca_XZ5Kg\n\n\npercent variance\n0.0031052\n703\npca_XZ5Kg\n\n\npercent variance\n0.0030217\n704\npca_XZ5Kg\n\n\npercent variance\n0.0029985\n705\npca_XZ5Kg\n\n\npercent variance\n0.0029388\n706\npca_XZ5Kg\n\n\npercent variance\n0.0028301\n707\npca_XZ5Kg\n\n\npercent variance\n0.0026421\n708\npca_XZ5Kg\n\n\npercent variance\n0.0025553\n709\npca_XZ5Kg\n\n\npercent variance\n0.0019366\n710\npca_XZ5Kg\n\n\npercent variance\n0.0010003\n711\npca_XZ5Kg\n\n\npercent variance\n0.0009157\n712\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n713\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n714\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n715\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n716\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n717\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n718\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n719\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n720\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n721\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n722\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n723\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n724\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n725\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n726\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n727\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n728\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n729\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n730\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n731\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n732\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n733\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n734\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n735\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n736\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n737\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n738\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n739\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n740\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n741\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n742\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n743\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n744\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n745\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n746\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n747\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n748\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n749\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n750\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n751\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n752\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n753\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n754\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n755\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n756\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n757\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n758\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n759\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n760\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n761\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n762\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n763\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n764\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n765\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n766\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n767\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n768\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n769\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n770\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n771\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n772\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n773\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n774\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n775\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n776\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n777\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n778\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n779\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n780\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n781\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n782\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n783\npca_XZ5Kg\n\n\npercent variance\n0.0000000\n784\npca_XZ5Kg"
  },
  {
    "objectID": "slides/33-PCA-2.html#scree-plot",
    "href": "slides/33-PCA-2.html#scree-plot",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Scree Plot",
    "text": "Scree Plot\n\nPercent variance against number of components\n\n\n\nCode\nprepped_pca |&gt; \n  tidy(2, type = \"variance\") |&gt; \n  filter(terms == \"percent variance\") |&gt; \n  ggplot(aes(x = component, y = value)) +\n  geom_line() +\n  labs(\n    x = \"PCAs\",\n    y = \"% of variance\",\n    title = \"Scree plot\"\n  )"
  },
  {
    "objectID": "slides/33-PCA-2.html#scree-plot-1",
    "href": "slides/33-PCA-2.html#scree-plot-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Scree Plot",
    "text": "Scree Plot\n\nPercent variance against number of components\n\n\n\nCode\nprepped_pca |&gt; \n  tidy(2, type = \"variance\") |&gt; \n  filter(terms == \"percent variance\", component &lt; 40) |&gt;\n  ggplot(aes(x = component, y = value)) +\n  geom_line() +\n  labs(\n    x = \"PCAs\",\n    y = \"% of variance\",\n    title = \"Scree plot\"\n  )"
  },
  {
    "objectID": "slides/33-PCA-2.html#heuristic",
    "href": "slides/33-PCA-2.html#heuristic",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Heuristic",
    "text": "Heuristic\n\nSet number of components to be where you see a “big drop”\nChoose number of components where plot starts to level off"
  },
  {
    "objectID": "slides/33-PCA-2.html#new-scores",
    "href": "slides/33-PCA-2.html#new-scores",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "New Scores",
    "text": "New Scores\n\nmnist_pca |&gt; head() |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC01\nPC02\nPC03\nPC04\nPC05\nPC06\nPC07\nPC08\nPC09\nPC10\nPC11\nPC12\nPC13\nPC14\nPC15\nPC16\nPC17\nPC18\nPC19\nPC20\n\n\n\n\n0.9221511\n-4.814750\n0.0675593\n-8.0512622\n0.9853391\n-0.9516077\n0.3361365\n-1.3934615\n-1.0523056\n2.3241877\n-1.520368\n3.1626599\n-1.1837683\n9.2008053\n-4.4196858\n1.6432947\n-7.5116268\n5.212185\n-4.1801314\n3.8211714\n\n\n-8.7089044\n-7.754338\n-3.4478817\n-1.6683075\n-0.8347884\n5.1446213\n-0.2722768\n-3.1354301\n-0.1983909\n-1.1961859\n-3.251257\n-1.1838362\n1.6347196\n-4.4051467\n-2.9243789\n-1.3810971\n-0.3620996\n-1.307530\n-3.4556711\n0.3211000\n\n\n-2.3283699\n9.431260\n-6.1840625\n1.7250517\n-4.0924219\n-2.2913712\n18.6913830\n-3.9924399\n2.1701000\n-0.1511394\n-7.383439\n2.0300298\n-0.5865363\n0.1094830\n11.3288811\n10.3145412\n4.6287307\n-3.465988\n-5.3164712\n3.5571589\n\n\n6.5821185\n-3.746287\n3.6908205\n-0.4610368\n-5.6272777\n-3.4615154\n1.5614022\n4.6004233\n-0.6248143\n-2.6171627\n-2.046650\n3.9413188\n-0.7617708\n-3.7772303\n-0.9635426\n2.2071743\n0.6448561\n-1.710684\n-0.0514925\n-0.3227111\n\n\n5.1832080\n3.133271\n-6.2778951\n1.4596418\n1.6088289\n0.6125598\n-0.5800603\n3.2059835\n0.0129479\n-0.6990456\n4.812231\n0.1232363\n-0.7611926\n-1.9712651\n-1.4728054\n-0.8389647\n0.9075877\n-5.817357\n-0.9964415\n1.8889851\n\n\n-2.1983838\n-3.068341\n-0.2312553\n2.7525776\n1.2168772\n-5.7912779\n-0.6937827\n-0.3355727\n-0.7865303\n-1.9459007\n2.753515\n-1.6144086\n1.2155606\n0.2366206\n-4.4797497\n-0.4558847\n-2.7655543\n-2.910686\n-1.2932355\n0.7981577"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-scores",
    "href": "slides/33-PCA-2.html#visualizing-scores",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing Scores",
    "text": "Visualizing Scores\n\n\nCode\nlibrary(GGally)\n\nmnist_pca |&gt; ggpairs(columns = 1: 5, aes(color = as_factor(mnist$train$labels)),\n                     lower = list(continuous = wrap(\"points\", alpha = 0.1,    size=0.1)))"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-pcs",
    "href": "slides/33-PCA-2.html#visualizing-pcs",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing PCs",
    "text": "Visualizing PCs\n\n\nCode\nprepped_pca |&gt; tidy(2) |&gt; \n  filter(component == \"PC1\") |&gt; \n  ggplot(aes(x = terms, y = value)) +\n  geom_col() +\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-pc1",
    "href": "slides/33-PCA-2.html#visualizing-pc1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing PC1",
    "text": "Visualizing PC1\n\n\nCode\npc_matrix &lt;- prepped_pca |&gt; tidy(2) |&gt; \n  filter(component == \"PC1\") |&gt; \n  pull(value) |&gt; \n  matrix(nrow = 28, byrow=FALSE)\n\nimage(x = 1:28, y = 1:28,\n      z = pc_matrix[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-pc2",
    "href": "slides/33-PCA-2.html#visualizing-pc2",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing PC2",
    "text": "Visualizing PC2\n\n\nCode\npc_matrix &lt;- prepped_pca |&gt; tidy(2) |&gt; \n  filter(component == \"PC2\") |&gt; \n  pull(value) |&gt; \n  matrix(nrow = 28, byrow=FALSE)\n\nimage(x = 1:28, y = 1:28,\n      z = pc_matrix[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-pcs-1",
    "href": "slides/33-PCA-2.html#visualizing-pcs-1",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing PCs",
    "text": "Visualizing PCs\n\n\nCode\npc_matrix &lt;- prepped_pca |&gt; tidy(2) |&gt; \n  filter(component == \"PC3\") |&gt; \n  pull(value) |&gt; \n  matrix(nrow = 28, byrow=FALSE)\n\nimage(x = 1:28, y = 1:28,\n      z = pc_matrix[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#twos-and-sevens",
    "href": "slides/33-PCA-2.html#twos-and-sevens",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Twos and Sevens",
    "text": "Twos and Sevens\n\nmnist_train_27 &lt;- mnist_train[mnist$train$labels %in% c(2, 7),]\nprepped_pca27 &lt;- pca_recipe |&gt; prep(mnist_train_27)\nmnist27_pca &lt;- prepped_pca27 |&gt; bake(new_data = mnist_train_27)"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-pcs-2",
    "href": "slides/33-PCA-2.html#visualizing-pcs-2",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing PCs",
    "text": "Visualizing PCs\n\n\nCode\npc_matrix &lt;- prepped_pca27 |&gt; tidy(2) |&gt; \n  filter(component == \"PC1\") |&gt; \n  pull(value) |&gt; \n  matrix(nrow = 28, byrow=FALSE)\n\nimage(x = 1:28, y = 1:28,\n      z = pc_matrix[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-pcs-3",
    "href": "slides/33-PCA-2.html#visualizing-pcs-3",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing PCs",
    "text": "Visualizing PCs\n\n\nCode\npc_matrix &lt;- prepped_pca27 |&gt; tidy(2) |&gt; \n  filter(component == \"PC2\") |&gt; \n  pull(value) |&gt; \n  matrix(nrow = 28, byrow=FALSE)\n\nimage(x = 1:28, y = 1:28,\n      z = pc_matrix[,28:1],\n      col=gray((0:255)/255))"
  },
  {
    "objectID": "slides/33-PCA-2.html#visualizing-pcs-4",
    "href": "slides/33-PCA-2.html#visualizing-pcs-4",
    "title": "MATH 427: Principal Component Analysis (PCA)",
    "section": "Visualizing PCs",
    "text": "Visualizing PCs\n\n\nCode\npc_matrix &lt;- prepped_pca27 |&gt; tidy(2) |&gt; \n  filter(component == \"PC3\") |&gt; \n  pull(value) |&gt; \n  matrix(nrow = 28, byrow=FALSE)\n\nimage(x = 1:28, y = 1:28,\n      z = pc_matrix[,28:1],\n      col=gray((0:255)/255))\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/19-decision-trees.html#computational-set-up",
    "href": "slides/19-decision-trees.html#computational-set-up",
    "title": "MATH 427: Decision Trees",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/19-decision-trees.html#tree-based-methods",
    "href": "slides/19-decision-trees.html#tree-based-methods",
    "title": "MATH 427: Decision Trees",
    "section": "Tree-Based Methods",
    "text": "Tree-Based Methods\n\nIdea: stratify or segment the predictor space into simple regions\nSplitting rules used to segment the predictor space form a tree\nCan be used for both classification and regression\nSingle tree called decision tree\n\nSimple and useful for interpretation\nNot the best in terms of prediction accuracy.\n\nMuch more powerful: grow multiple trees and then combine their results\n\nbagging and boosting .\n\nMost powerful methods for tabular data are typically tree-based methods"
  },
  {
    "objectID": "slides/19-decision-trees.html#terminology-for-trees",
    "href": "slides/19-decision-trees.html#terminology-for-trees",
    "title": "MATH 427: Decision Trees",
    "section": "Terminology for Trees",
    "text": "Terminology for Trees\n\nEvery split is considered to be a node\nFirst node at the top of the tree: root node (contains all the training data)\nFinal nodes at the bottom of tree called leaves or terminal nodes\nDecision trees typically drawn with root at top and leaves at bottom\nNodes in middle of tree called internal nodes\nThe segments of the trees that connect nodes are known as branches"
  },
  {
    "objectID": "slides/19-decision-trees.html#terminology-for-trees-1",
    "href": "slides/19-decision-trees.html#terminology-for-trees-1",
    "title": "MATH 427: Decision Trees",
    "section": "Terminology for Trees",
    "text": "Terminology for Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: From Hands-On Machine Learning, Boehmke & Greenwell"
  },
  {
    "objectID": "slides/19-decision-trees.html#building-a-tree",
    "href": "slides/19-decision-trees.html#building-a-tree",
    "title": "MATH 427: Decision Trees",
    "section": "Building a Tree",
    "text": "Building a Tree\n\nSelect predictor \\(X_j\\) and cut point \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\geq s\\}\\) leads to the greatest possible reduction in performance metric (e.g. SSE)\nFor any \\(j\\) and \\(s\\), define \\[R_1 = \\{X|X_j &lt; s\\} \\ \\ \\text{and} \\ \\ R_2 = \\{X|X_j \\geq s\\}\\]\nFind \\(j\\) and \\(s\\) that minimize \\[SSE = \\displaystyle \\sum_{i \\in R_1}\\left(y_i - \\hat{y}_{R_1}\\right)^2 + \\sum_{i \\in R_2}\\left(y_i - \\hat{y}_{R_2}\\right)^2\\]\nRepeat process on each the two new regions\nContinue until a stopping criterion is reached"
  },
  {
    "objectID": "slides/19-decision-trees.html#prediction",
    "href": "slides/19-decision-trees.html#prediction",
    "title": "MATH 427: Decision Trees",
    "section": "Prediction",
    "text": "Prediction\n\nFor new observation that falls in region \\(R_j\\):\n\nRegression: the mean response of the training set observations in \\(R_j\\)\nClassificaiton: majority vote response of the training set observations in \\(R_j\\)"
  },
  {
    "objectID": "slides/19-decision-trees.html#building-a-tree-and-prediction",
    "href": "slides/19-decision-trees.html#building-a-tree-and-prediction",
    "title": "MATH 427: Decision Trees",
    "section": "Building a Tree and Prediction",
    "text": "Building a Tree and Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: From Hands-On Machine Learning, Boehmke & Greenwell"
  },
  {
    "objectID": "slides/19-decision-trees.html#building-a-tree-and-prediction-1",
    "href": "slides/19-decision-trees.html#building-a-tree-and-prediction-1",
    "title": "MATH 427: Decision Trees",
    "section": "Building a Tree and Prediction",
    "text": "Building a Tree and Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: From Hands-On Machine Learning, Boehmke & Greenwell"
  },
  {
    "objectID": "slides/19-decision-trees.html#building-a-tree-and-prediction-2",
    "href": "slides/19-decision-trees.html#building-a-tree-and-prediction-2",
    "title": "MATH 427: Decision Trees",
    "section": "Building a Tree and Prediction",
    "text": "Building a Tree and Prediction\n\nFrom ISLR"
  },
  {
    "objectID": "slides/19-decision-trees.html#building-a-tree-1",
    "href": "slides/19-decision-trees.html#building-a-tree-1",
    "title": "MATH 427: Decision Trees",
    "section": "Building a Tree",
    "text": "Building a Tree\n\nAnyone know what a greedy algorithm is?\n\n\n\nComputationally infeasible to consider every possible partition\nIdea: top-down, greedy approach known as recursive binary splitting.\n\ntop-down because it begins at the top of the tree\ngreedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step\n\nImportant for determining whether to use a ordinal encoding or not\n\nStop when each terminal node has fewer than some predetermined number of observations"
  },
  {
    "objectID": "slides/19-decision-trees.html#overfitting",
    "href": "slides/19-decision-trees.html#overfitting",
    "title": "MATH 427: Decision Trees",
    "section": "Overfitting",
    "text": "Overfitting\n\nThis process described above is likely to overfit the data\nOne solution: require each split to improve performance by some amount\n\nBad Idea: sometimes seemingly meaningless cuts early on enable really good cuts later on\n\nGood solution: pruning\n\nBuild big tree and the prune off branches that are unnecessary\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "project/project-instructions.html",
    "href": "project/project-instructions.html",
    "title": "Project Instructions",
    "section": "",
    "text": "Every fall, the College of Idaho welcomes around 300 new students to campus. Before a student steps foot on campus, they have likely traversed a “funnel”. The College starts the process by buying information on tens of thousands of high school students. The College of Idaho’s Admissions office reaches out in various ways to these prospects with the goal of eventually turning them into College of Idaho students. However, there are several steps in the funnel from Prospect to Student:\n\nProspects must apply.\nApplicants must be admitted.\nAdmitted students must then commit to the school by putting down a deposit.\nDeposited students must then enroll.\n\nYour task is to analyze the data, finding actionable insights that would help the Admissions staff to refine their processes. The class will be divided into 4 groups, each focusing on a different step in the funnel. Your groups can be found on Canvas and your tasks will be assigned next week.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#introduction",
    "href": "project/project-instructions.html#introduction",
    "title": "Project Instructions",
    "section": "",
    "text": "Every fall, the College of Idaho welcomes around 300 new students to campus. Before a student steps foot on campus, they have likely traversed a “funnel”. The College starts the process by buying information on tens of thousands of high school students. The College of Idaho’s Admissions office reaches out in various ways to these prospects with the goal of eventually turning them into College of Idaho students. However, there are several steps in the funnel from Prospect to Student:\n\nProspects must apply.\nApplicants must be admitted.\nAdmitted students must then commit to the school by putting down a deposit.\nDeposited students must then enroll.\n\nYour task is to analyze the data, finding actionable insights that would help the Admissions staff to refine their processes. The class will be divided into 4 groups, each focusing on a different step in the funnel. Your groups can be found on Canvas and your tasks will be assigned next week.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#deliverables",
    "href": "project/project-instructions.html#deliverables",
    "title": "Project Instructions",
    "section": "Deliverables",
    "text": "Deliverables\nYour deliverables are:\n\nA seven-minute non-technical talk with slides, appropriate for Admissions leaders, to be followed by a brief Q&A. Presentations are tentatively scheduled to happen during class on Monday, May 5th.\nA one-page executive summary or infographic (not a report) with bullet points of key information, and possibly one or two data visualizations, etc.\n\nAll of your work should be included and reproducible in GitHub classroom and you should fill in the README.md file to help guide any readers through your work.\n\nWhile you need not submit a data analysis report, it is expected that your work is organized enough for Dr. Friedlander to look at it and, with minimal effort, figure out what you did.\n\nA short report summarizing each team members contribution and the rough amount of time spend working on the project.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#data",
    "href": "project/project-instructions.html#data",
    "title": "Project Instructions",
    "section": "Data",
    "text": "Data\nAnonymized data is provided on over 100,000 prospects for 2022-2024, and stored in three separate datasets. Note, if you discover that you are able to identify anyone in this dataset, please contact Dr. Friedlander immediately! In order, the data attributes are as follows:\n\nProspect: Was information on this prospect purchased?\nInquiry: Did the student reach out to us first?\nApplicant: TBD\nAdmit: Was the student admitted?\nConversion Rate: Did the student convert from an admit to a deposit?\nDeposit: Did the student put down a deposit?\nYield Rate: Did the student convert from a deposit to an enrolled student?\nDrops-DP/DF: Deposits who don’t enroll or defer to a future term.\nNet Deposits: Deposits that matriculate that fall.\nSex: Sex of student.\nStudent Type: Is the student a Freshman or Transfer?\nIPEDS Classification: Race category used by the Integrated Post-secondary Education Data System (IPEDS) which is a survey conducted by the U.S. Department of Education’s National Center for Education Statistics (NCES).\nGeographic Region: region that the student lives in.\nActive County: County if from Idaho.\nFirst_Source Origin First Source Summary: Where their data was obtained from.\nApplication Souce: How the student submitted their application.\nEntry Term: What term did the student intend to matriculate?\nSchool X Type: Type of school on their application.\nSchool X Institution: Name of school on their application.\nSchool X Verified GPA: GPA from corresponding school.\nACT and SAT variables: should be self explanatory\nSport X Sport: A sport they’re interested in.\nAcademic Interest: What their stated academic interest is.\n\nPlease note that the biggest part of this project will be cleaning the data, augmenting the data, and conducting feature engineering.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project/project-instructions.html#evaluation-guide",
    "href": "project/project-instructions.html#evaluation-guide",
    "title": "Project Instructions",
    "section": "Evaluation Guide",
    "text": "Evaluation Guide\nYour project will be assessed in each of the following areas as either Outstanding (5), Good (4), Acceptable (3), Needs Work (2), Inadequate (1), Incomplete (0). The questions below will be used as a guide to determine the quality of your analysis in each area.\n\nSlides and Presentation Evaluation:\n\nIs the presentation of the appropriate length (roughly 7 minutes)?\nIs the presentation directed at the appropriate (NON-TECHNICAL) audience?\nAre results presented in a clear, logical, and engaging manner?\nAre the slides easy to read with appropriate graphics that support the narrative and analysis?\n\n\n\nContent and Analysis Evaluation:\n\nIs the analysis complex and insightful, showing evidence that the data have been analyzed in multiple different ways?\nAre the methods and assumptions explained and justified?\nAre the findings and conclusions supported by sound reasoning and/or additional research?\nAre the possible implications of the results both useful and discussed in context?\n\n\n\nProfessionalism Evaluation:\n\nIs the presentation well-prepared with minimal reliance on notes or slides?\nIs the presentation clear and well-paced and free of distracting mannerisms?\nAre all presenters engaged throughout the presentation?\nIs the responsibility for presenting the analysis shared between presenters?\nDo all presenters show mastery of the subject and answer questions with ease?\n\n\n\nExecutive Summary & Infographic:\n\nDoes the document clearly summarize the methodology, results, and recommendations?\nIs the executive summary directed at the appropriate (NON-TECHNICAL) audience?\nAre results presented in clear, logical, and engaging manner?\nDo appropriate visuals support the narrative and analysis?",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "slides/08-classification-metrics.html#tips-on-gradient-descent-hw",
    "href": "slides/08-classification-metrics.html#tips-on-gradient-descent-hw",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Tips on Gradient Descent HW",
    "text": "Tips on Gradient Descent HW\n\nMake step size small!\nMay take a while to converge\nTry adaptive step size (i.e. backtracking)\nClarification on stopping criteria\n\nSet tolerance\nStop when distance from gradient to \\((0, 0)\\) is below tolerance"
  },
  {
    "objectID": "slides/08-classification-metrics.html#computational-set-up",
    "href": "slides/08-classification-metrics.html#computational-set-up",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(janitor) # for next contingency tables\nlibrary(kableExtra)\nlibrary(ISLR2)\n\ntidymodels_prefer()"
  },
  {
    "objectID": "slides/08-classification-metrics.html#default-dataset",
    "href": "slides/08-classification-metrics.html#default-dataset",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Default Dataset",
    "text": "Default Dataset\n\n\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\nhead(Default) |&gt; kable()  # print first six observations\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n\n\nNo\nYes\n817.1804\n12106.135\n\n\nNo\nNo\n1073.5492\n31767.139\n\n\nNo\nNo\n529.2506\n35704.494\n\n\nNo\nNo\n785.6559\n38463.496\n\n\nNo\nYes\n919.5885\n7491.559\n\n\n\n\n\n\nResponse Variable: default\n\nDefault |&gt; \n  tabyl(default) |&gt;  # class frequencies\n  kable()           # Make it look nice\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n9667\n0.9667\n\n\nYes\n333\n0.0333"
  },
  {
    "objectID": "slides/08-classification-metrics.html#split-the-data",
    "href": "slides/08-classification-metrics.html#split-the-data",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Split the data",
    "text": "Split the data\n\nset.seed(427)\n\ndefault_split &lt;- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n\n&lt;Training/Testing/Total&gt;\n&lt;6000/4000/10000&gt;\n\ndefault_train &lt;- training(default_split)\ndefault_test &lt;- testing(default_split)"
  },
  {
    "objectID": "slides/08-classification-metrics.html#k-nearest-neighbors-classifier-build-model",
    "href": "slides/08-classification-metrics.html#k-nearest-neighbors-classifier-build-model",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "K-Nearest Neighbors Classifier: Build Model",
    "text": "K-Nearest Neighbors Classifier: Build Model\n\nResponse (\\(Y\\)): default\nPredictor (\\(X\\)): balance\n\n\nknnfit &lt;- nearest_neighbor(neighbors = 10) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;  \n  fit(default ~ balance, data = Default)   # fit 10-nn model"
  },
  {
    "objectID": "slides/08-classification-metrics.html#k-nearest-neighbors-classifier-predictions",
    "href": "slides/08-classification-metrics.html#k-nearest-neighbors-classifier-predictions",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "K-Nearest Neighbors Classifier: Predictions",
    "text": "K-Nearest Neighbors Classifier: Predictions\n\nClass labelsProbabilities\n\n\n\npredict(knnfit, new_data = Default, type = \"class\") |&gt; head() |&gt; kable()   # obtain predictions as classes\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\n\n\n\n\n\n\nPredicts class w/ maximum probability\n\n\npredict(knnfit, new_data = Default, type = \"prob\") |&gt; head() |&gt; kable() # obtain predictions as probabilities\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0"
  },
  {
    "objectID": "slides/08-classification-metrics.html#fitting-a-logistic-regression",
    "href": "slides/08-classification-metrics.html#fitting-a-logistic-regression",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Fitting a logistic regression",
    "text": "Fitting a logistic regression\nFitting a logistic regression model with default as the response and balance as the predictor:\n\nlogregfit &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |&gt; kable()  # obtain results\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6926385\n0.4659035\n-22.95033\n0\n\n\nbalance\n0.0055327\n0.0002841\n19.47329\n0"
  },
  {
    "objectID": "slides/08-classification-metrics.html#making-predictions-in-r",
    "href": "slides/08-classification-metrics.html#making-predictions-in-r",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Making predictions in R",
    "text": "Making predictions in R\n\nClass LabelsLog-OddsProbabilities\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |&gt; kable()   # obtain class predictions\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\n\n\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |&gt; kable()   # obtain log-odds predictions\n\n\n\n\nx\n\n\n\n\n-6.819727\n\n\n\n\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |&gt; kable()  # obtain probability predictions\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.9989092\n0.0010908"
  },
  {
    "objectID": "slides/08-classification-metrics.html#binary-classifiers",
    "href": "slides/08-classification-metrics.html#binary-classifiers",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Binary Classifiers",
    "text": "Binary Classifiers\n\nStart with binary classification scenarios\nWith binary classification, designate one category as “Success/Positive” and the other as “Failure/Negative”\n\nIf relevant to your problem: “Positive” should be the thing you’re trying to predict/care more about\nNote: “Positive” \\(\\neq\\) “Good”\nFor default: “Yes” is Positive\n\nSome metrics weight “Positives” more and viceversa"
  },
  {
    "objectID": "slides/08-classification-metrics.html#confusion-matrix",
    "href": "slides/08-classification-metrics.html#confusion-matrix",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\n\n\n\n\n\n\n\nActual Positive/Event\nActual Negative/Non-event\n\n\n\n\nPredicted Positive/Event\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nPredicted Negative/Non-event\nFalse Negative (FN)\nTrue Negative (TN)"
  },
  {
    "objectID": "slides/08-classification-metrics.html#adding-predictions-to-tibble",
    "href": "slides/08-classification-metrics.html#adding-predictions-to-tibble",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Adding predictions to tibble",
    "text": "Adding predictions to tibble\n\ndefault_test_wpreds &lt;- default_test |&gt; \n  mutate(\n    knn_preds = predict(knnfit, new_data = default_test, type = \"class\")$.pred_class,\n    logistic_preds = predict(logregfit, new_data = default_test, type = \"class\")$.pred_class\n  )\n\ndefault_test_wpreds |&gt; head() |&gt; kable()\n\n\n\n\ndefault\nstudent\nbalance\nincome\nknn_preds\nlogistic_preds\n\n\n\n\nNo\nNo\n729.5265\n44361.63\nNo\nNo\n\n\nNo\nYes\n808.6675\n17600.45\nNo\nNo\n\n\nNo\nYes\n1220.5838\n13268.56\nNo\nNo\n\n\nNo\nNo\n237.0451\n28251.70\nNo\nNo\n\n\nNo\nNo\n606.7423\n44994.56\nNo\nNo\n\n\nNo\nNo\n286.2326\n45042.41\nNo\nNo"
  },
  {
    "objectID": "slides/08-classification-metrics.html#knn-confusion-matrix",
    "href": "slides/08-classification-metrics.html#knn-confusion-matrix",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "KNN: Confusion Matrix",
    "text": "KNN: Confusion Matrix\n\ndefault_test_wpreds |&gt;\n  conf_mat(truth = default, estimate = knn_preds)\n\n          Truth\nPrediction   No  Yes\n       No  3854   80\n       Yes   17   49"
  },
  {
    "objectID": "slides/08-classification-metrics.html#knn-confusion-matrix-sexy",
    "href": "slides/08-classification-metrics.html#knn-confusion-matrix-sexy",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "KNN: Confusion Matrix (Sexy)",
    "text": "KNN: Confusion Matrix (Sexy)\n\ndefault_test_wpreds |&gt;\n  conf_mat(truth = default, estimate = knn_preds) |&gt; \n  autoplot(\"heatmap\")"
  },
  {
    "objectID": "slides/08-classification-metrics.html#logistic-regression-confusion-matrix",
    "href": "slides/08-classification-metrics.html#logistic-regression-confusion-matrix",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Logistic Regression: Confusion Matrix",
    "text": "Logistic Regression: Confusion Matrix\n\ndefault_test_wpreds |&gt;\n  conf_mat(truth = default, estimate = logistic_preds) |&gt; \n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "slides/08-classification-metrics.html#classification-metrics",
    "href": "slides/08-classification-metrics.html#classification-metrics",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Classification Metrics",
    "text": "Classification Metrics\n\nAccuracy: proportion of your classes that are correct \\[(TP + TN)/Total\\]\nRecall/Sensitivity: proportion of true positives correct (true positive rate) \\[TP/(TP+FN)\\]\nPrecision/Positive Predictive Value (PPV): proportion of predicted positive that are correct \\[TP/(TP+FP)\\]\nSpecificity: proportion of true negatives correct (true negative rate) \\[TN/(TN+FP)\\]\nNegative Predictive Value (NPV): proportion of predicted negatives that are correct \\[TN/(TN+FN)\\]"
  },
  {
    "objectID": "slides/08-classification-metrics.html#knn-performance",
    "href": "slides/08-classification-metrics.html#knn-performance",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "KNN: Performance",
    "text": "KNN: Performance\n\n\n\ndefault_test_wpreds |&gt;\n  conf_mat(truth = default, estimate = knn_preds) |&gt; \n  autoplot(\"heatmap\")\n\n\n\n\n\n\n\n\n\n\nAccuracy: \\((3854+49)/4000 = .976 = 97.6\\%\\)\nRecall/Sensitivity: \\(49/(49+80) = 0.380 = 38.0\\%\\)\nPrecision/Positive Predictive Value (PPV): \\(49/(49+17) = .742 = 74.2\\%\\)\nSpecificity: \\(3854/(3854+17) = 0.996 = 99.6\\%\\)\nNegative Predictive Value (NPV): \\(3854/(3854+80) = 98.0\\)"
  },
  {
    "objectID": "slides/08-classification-metrics.html#logistic-regression-performance",
    "href": "slides/08-classification-metrics.html#logistic-regression-performance",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Logistic Regression: Performance",
    "text": "Logistic Regression: Performance\n\n\n\ndefault_test_wpreds |&gt;\n  conf_mat(truth = default, estimate = logistic_preds) |&gt; \n  autoplot(\"heatmap\")\n\n\n\n\n\n\n\n\n\nCompute the following and write your answers on the board:\n\nAccuracy\nRecall/Sensitivity\nPrecision/Positive Predictive Value (PPV)\nSpecificity\nNegative Predictive Value (NPV)"
  },
  {
    "objectID": "slides/08-classification-metrics.html#performance-metrics-with-yardstick",
    "href": "slides/08-classification-metrics.html#performance-metrics-with-yardstick",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Performance Metrics with yardstick",
    "text": "Performance Metrics with yardstick\n\nyardstick is a package that ships with tidymodels meant for model evaluation\nTypical syntax: metricname(data, truth, estimate, ...)\n\nBind original data with predicted observations\nPut true response in for truth and predicted values in for estimate"
  },
  {
    "objectID": "slides/08-classification-metrics.html#logistic-regression-accuracy",
    "href": "slides/08-classification-metrics.html#logistic-regression-accuracy",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Logistic Regression: Accuracy",
    "text": "Logistic Regression: Accuracy\n\ndefault_test_wpreds |&gt; \n  accuracy(truth = default, estimate = logistic_preds) |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.973"
  },
  {
    "objectID": "slides/08-classification-metrics.html#two-more-metrics",
    "href": "slides/08-classification-metrics.html#two-more-metrics",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Two More Metrics",
    "text": "Two More Metrics\n\nMatthews correlation coefficient (MCC): similar to \\(R^2\\) but for classification \\[\\frac{TP\\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP+FN)(TN+FP)(TN+FN)}}\\]\n\nGood for imbalanced data\nConsiders both positives and negatives\n\nF-Measure: harmonic mean of recall and precision \\[\\frac{2}{recall^{-1} + precision^{-1}} = \\frac{2TP}{2TP+FP+FN}\\]\n\nFocuses more on positives\nbad of imbalanced data"
  },
  {
    "objectID": "slides/08-classification-metrics.html#metric-sets",
    "href": "slides/08-classification-metrics.html#metric-sets",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Metric Sets",
    "text": "Metric Sets\n\nbinary_metrics &lt;- metric_set(accuracy, recall, precision, specificity,\n                             npv, mcc, f_meas)\n\n\nCan apply this to compute a bunch of metrics"
  },
  {
    "objectID": "slides/08-classification-metrics.html#knn-performance-1",
    "href": "slides/08-classification-metrics.html#knn-performance-1",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "KNN: Performance",
    "text": "KNN: Performance\n\ndefault_test_wpreds |&gt; \n  binary_metrics(truth = default, estimate = knn_preds, event_level = \"second\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9757500\n\n\nrecall\nbinary\n0.3798450\n\n\nprecision\nbinary\n0.7424242\n\n\nspecificity\nbinary\n0.9956084\n\n\nnpv\nbinary\n0.9796645\n\n\nmcc\nbinary\n0.5206828\n\n\nf_meas\nbinary\n0.5025641"
  },
  {
    "objectID": "slides/08-classification-metrics.html#logistic-regression-performance-1",
    "href": "slides/08-classification-metrics.html#logistic-regression-performance-1",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Logistic Regression: Performance",
    "text": "Logistic Regression: Performance\n\ndefault_test_wpreds |&gt; \n  binary_metrics(truth = default, estimate = logistic_preds, event_level = \"second\") |&gt; \n  kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9730000\n\n\nrecall\nbinary\n0.3023256\n\n\nprecision\nbinary\n0.6842105\n\n\nspecificity\nbinary\n0.9953500\n\n\nnpv\nbinary\n0.9771747\n\n\nmcc\nbinary\n0.4437097\n\n\nf_meas\nbinary\n0.4193548"
  },
  {
    "objectID": "slides/08-classification-metrics.html#discussion",
    "href": "slides/08-classification-metrics.html#discussion",
    "title": "MATH 427: Evaluating Classification Models",
    "section": "Discussion",
    "text": "Discussion\n\nFor each of the following metrics, brainstorm a situation in which that metric is probably the most important:\n\nRecall\nPrecision\nAccuracy\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/16-selection-regularization.html#computational-set-up",
    "href": "slides/16-selection-regularization.html#computational-set-up",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(fivethirtyeight) # for candy rankings data\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/16-selection-regularization.html#data-candy",
    "href": "slides/16-selection-regularization.html#data-candy",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data for this lecture comes from the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g. Reese’s vs. Skittles). Click here to check out some of the match ups.\nThe data set contains 12 characteristics and win percentage from 85 candies in the experiment."
  },
  {
    "objectID": "slides/16-selection-regularization.html#data-candy-1",
    "href": "slides/16-selection-regularization.html#data-candy-1",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Data: Candy",
    "text": "Data: Candy\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "slides/16-selection-regularization.html#data-cleaning",
    "href": "slides/16-selection-regularization.html#data-cleaning",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\ncandy_rankings_clean &lt;- candy_rankings |&gt; \n  select(-competitorname) |&gt; \n  mutate(sugarpercent = sugarpercent*100, # convert proportions into percentages\n         pricepercent = pricepercent*100, # convert proportions into percentages\n         across(where(is.logical), ~ factor(.x, levels = c(\"FALSE\", \"TRUE\")))) # convert logicals into factors"
  },
  {
    "objectID": "slides/16-selection-regularization.html#data-cleaning-1",
    "href": "slides/16-selection-regularization.html#data-cleaning-1",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nglimpse(candy_rankings_clean)\n\nRows: 85\nColumns: 12\n$ chocolate        &lt;fct&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;fct&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;fct&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 73.2, 60.4, 1.1, 1.1, 90.6, 46.5, 60.4, 31.3, 90.6, 6…\n$ pricepercent     &lt;dbl&gt; 86.0, 51.1, 11.6, 51.1, 51.1, 76.7, 76.7, 51.1, 32.5,…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "slides/16-selection-regularization.html#data-splitting",
    "href": "slides/16-selection-regularization.html#data-splitting",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Data Splitting",
    "text": "Data Splitting\n\ncandy_split &lt;- initial_split(candy_rankings_clean, prop = 0.75, strata = winpercent)\ncandy_train &lt;- training(candy_split)\ncandy_test &lt;- testing(candy_split)"
  },
  {
    "objectID": "slides/16-selection-regularization.html#what-is-feature-selection",
    "href": "slides/16-selection-regularization.html#what-is-feature-selection",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "What is feature selection?",
    "text": "What is feature selection?\n\nHow do we choose what variables to include in our model?\nUp to now… include all of them… probably not the best\nAdvantage of linear regression: interpretability\nIncluding every feature decreases interpretability\nReasons for feature selection:\n\nImprove model performance\nImprove model interpretability\n\nParsimony: simpler models are called more parsimonious\nOccam’s Razor: more parsimonious models are better than less parsimonious models, holding all else constant"
  },
  {
    "objectID": "slides/16-selection-regularization.html#types-of-feature-selection",
    "href": "slides/16-selection-regularization.html#types-of-feature-selection",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Types of feature selection",
    "text": "Types of feature selection\n\nSubset selection: Forward/Backward/Best-Subset Selection\nShrinkage-based methods: LASSO and Ridge Regression\nDimension reduction: consider linear combinations of predictors"
  },
  {
    "objectID": "slides/16-selection-regularization.html#exercise",
    "href": "slides/16-selection-regularization.html#exercise",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Exercise",
    "text": "Exercise\n\nWith your group, write out the steps for the following algorithms on the board\n\nGroup 1: Forward selection\nGroup 2: Backward elimination\nGroup 3: Step-wise selection\nGroup 4: Best-subset selection"
  },
  {
    "objectID": "slides/16-selection-regularization.html#subset-selection-in-r",
    "href": "slides/16-selection-regularization.html#subset-selection-in-r",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Subset Selection in R",
    "text": "Subset Selection in R\n\ntidymodels does not have an implementation for any subset selection techniques\nregularization (shrinkage-based) methods almost always perform better\ncolino package provides tidymodels implementation\nOther options\n\ncaret package\nolsrr and blorr packages if you don’t care about cross-validation\nimplement yourself"
  },
  {
    "objectID": "slides/16-selection-regularization.html#feature-selection-in-r",
    "href": "slides/16-selection-regularization.html#feature-selection-in-r",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Feature Selection in R",
    "text": "Feature Selection in R\n\nWhen creating your recipe, don’t need to always include all variables in your recipe:\n\n\nint_recipe &lt;- recipe(resp ~ var1 + var2 + var1*var2, data = training_data) |&gt; \n  step_x(...)"
  },
  {
    "objectID": "slides/16-selection-regularization.html#re-using-recipe-but-changing-formula",
    "href": "slides/16-selection-regularization.html#re-using-recipe-but-changing-formula",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Re-using Recipe but changing formula",
    "text": "Re-using Recipe but changing formula\n\nnoint_recipe2 &lt;- new_recipe |&gt; \n  remove_formula() |&gt; \n  add_formula(resp ~ var1 + var2)"
  },
  {
    "objectID": "slides/16-selection-regularization.html#question",
    "href": "slides/16-selection-regularization.html#question",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Question",
    "text": "Question\n\nWhat criteria do we use to fit a linear regression model? Write down an expression with \\(\\hat{\\beta_i}\\)’s, \\(x_{ij}\\)’s, and \\(y_j\\)’s in it."
  },
  {
    "objectID": "slides/16-selection-regularization.html#ols",
    "href": "slides/16-selection-regularization.html#ols",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "OLS",
    "text": "OLS\n\nOrdinary Least Squares Regression:\n\n\\[\n\\begin{aligned}\n\\hat{\\beta} =\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}SSE(\\hat{\\beta}) &= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{y}_j)^2\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2\n\\end{aligned}\n\\]\n\n\\(\\hat{\\beta} = (\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p)\\) is the vector of all my coefficients\n\\(\\operatorname{argmin}\\) is a function (operator) that returns the arguments that minimize the quantity it’s being applied to"
  },
  {
    "objectID": "slides/16-selection-regularization.html#ridge-regression",
    "href": "slides/16-selection-regularization.html#ridge-regression",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\\[\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_2^2\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\n\\end{aligned}\n\\]\n\n\\(\\lambda\\) is called a “tuning parameter” and is not estimated from the data (more on this later)\nIdea: penalize large coefficients\nIf we penalize large coefficients, what’s going to happen to to our estimated coefficients? THEY SHRINK!\n\\(\\|\\cdot\\|_2\\) is called the \\(L_2\\)-norm"
  },
  {
    "objectID": "slides/16-selection-regularization.html#but-but-but-why",
    "href": "slides/16-selection-regularization.html#but-but-but-why",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "But… but… but why?!?",
    "text": "But… but… but why?!?\n\n\nRecall the Bias-Variance Trade-Off\nOur reducible error can partitioned into:\n\nBias: how much \\(\\hat{f}\\) misses \\(f\\) by on average\nVariance: how much \\(\\hat{f}\\) moves around from sample to sample\n\nRidge: increase bias a little bit in exchange for large decrease in variance\nAs we increase \\(\\lambda\\) do we increase or decrease the penalty for large coefficients?\nAs we increase \\(\\lambda\\) do we increase or decrease the flexibility of our model?"
  },
  {
    "objectID": "slides/16-selection-regularization.html#lasso",
    "href": "slides/16-selection-regularization.html#lasso",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "LASSO",
    "text": "LASSO\n\\[\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_1\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\n\\end{aligned}\n\\]\n\nLASSO: Least Absolute Shrinkage and Selection Operator\n\\(\\lambda\\) is called a “tuning parameter” and is not estimated from the data (more on this later)\nIdea: penalize large coefficients\nIf we penalize large coefficients, what’s going to happen to to our estimated coefficients THEY SHRINK!\n\\(\\|\\cdot\\|_1\\) is called the \\(L_1\\)-norm"
  },
  {
    "objectID": "slides/16-selection-regularization.html#question-1",
    "href": "slides/16-selection-regularization.html#question-1",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Question",
    "text": "Question\n\nWhat should happen to our coefficients as we increase \\(\\lambda\\)?"
  },
  {
    "objectID": "slides/16-selection-regularization.html#ridge-vs.-lasso",
    "href": "slides/16-selection-regularization.html#ridge-vs.-lasso",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Ridge vs. LASSO",
    "text": "Ridge vs. LASSO\n\n\nRidge has a closed-form solution… how might we calculate it?\nRidge has some nice linear algebra properties that makes it EXTREMELY FAST to fit\nLASSO has no closed-form solution… why?\nLASSO coefficients estimated numerically… how?\n\nGradient descent works (kind of) but something called coordinate descent is typically better\n\nMOST IMPORTANT PROPERTY OF LASSO: it induces sparsity while Ridge does not"
  },
  {
    "objectID": "slides/16-selection-regularization.html#sparsity-in-applied-math",
    "href": "slides/16-selection-regularization.html#sparsity-in-applied-math",
    "title": "MATH 427: Feature Selection and Regularization",
    "section": "Sparsity in Applied Math",
    "text": "Sparsity in Applied Math\n\nsparse typically means “most things are zero”\nExample: sparse matrices are matrices where most entries are zero\n\nfor large matrices this can provide HUGE performance gains\n\nLASSO induces sparsity by setting most of the parameter estimates to zero\n\nthis means it fits the model and does feature selection SIMULTANEOUSLY\n\nLet’s do some board work to see why this is…\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#computational-set-up",
    "href": "slides/24-classification-trees-2.html#computational-set-up",
    "title": "MATH 427: Classification Trees",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rpart.plot)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#classification-trees",
    "href": "slides/24-classification-trees-2.html#classification-trees",
    "title": "MATH 427: Classification Trees",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nPredictions:\n\nClasses: most common class at terminal node\nProbability: proportion of each class at terminal node\n\nRest of tree: same as regression tree"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#classification-trees-1",
    "href": "slides/24-classification-trees-2.html#classification-trees-1",
    "title": "MATH 427: Classification Trees",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nPredictions:\n\nClasses: most common class at terminal node\nProbability: proportion of each class at terminal node\n\nRest of tree: same as regression tree"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#fitting-classification-trees",
    "href": "slides/24-classification-trees-2.html#fitting-classification-trees",
    "title": "MATH 427: Classification Trees",
    "section": "Fitting Classification Trees",
    "text": "Fitting Classification Trees\n\nStill use recursive binary splitting to grow a classification tree\n\\(\\hat{p}_{mk}\\): proportion of training observations in the \\(m^{th}\\) region from the \\(k^{th}\\) class\n\\(SSE\\) can be replaced by\n\nclassification error rate, fraction of the training observations that do not belong to the most common class ($1 - ) \\[E = 1 - \\max_k \\left(\\hat{p}_{mk}\\right)\\]"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#alternatives-to-classification-error-rate",
    "href": "slides/24-classification-trees-2.html#alternatives-to-classification-error-rate",
    "title": "MATH 427: Classification Trees",
    "section": "Alternatives to Classification Error Rate",
    "text": "Alternatives to Classification Error Rate\n\nGini index, measure of node purity— small values indicate that a node is predominantly a single class \\[G = \\displaystyle \\sum_{k=1}^{K} \\hat{p}_{mk}\\left(1-\\hat{p}_{mk}\\right)\\]\nEntropy, measure of node purity— small values indicate that a node is predominantly a single class \\[G = \\displaystyle -\\sum_{k=1}^{K} \\hat{p}_{mk}]\\log\\left(\\hat{p}_{mk}\\right)\\]"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#general-idea",
    "href": "slides/24-classification-trees-2.html#general-idea",
    "title": "MATH 427: Classification Trees",
    "section": "General Idea",
    "text": "General Idea\n\nMany different implementations so check documentation\nISLR2 recommendation\n\nUse Gini or Cross-Entropy to split\nUse classification error rate to prune\n\nrpart uses Gini to split and classification error (plus penalty) to prune"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#data-voter-frequency",
    "href": "slides/24-classification-trees-2.html#data-voter-frequency",
    "title": "MATH 427: Classification Trees",
    "section": "Data: Voter Frequency",
    "text": "Data: Voter Frequency\n\nInfo about data\nGoal: Identify individuals who are unlikely to vote to help organization target “get out the vote” effort.\n\n\nvoter_data &lt;- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nglimpse(voter_data)\n\nRows: 5,836\nColumns: 119\n$ RespId         &lt;dbl&gt; 470001, 470002, 470003, 470007, 480008, 480009, 480010,…\n$ weight         &lt;dbl&gt; 0.7516, 1.0267, 1.0844, 0.6817, 0.9910, 1.0591, 1.1512,…\n$ Q1             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Q2_1           &lt;dbl&gt; 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1…\n$ Q2_2           &lt;dbl&gt; 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1…\n$ Q2_3           &lt;dbl&gt; 2, 2, 2, 1, -1, 3, 2, 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 1, …\n$ Q2_4           &lt;dbl&gt; 4, 3, 2, 3, 1, 4, 3, 2, 3, 1, 2, 4, 2, 3, 1, 1, 1, 1, 2…\n$ Q2_5           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1…\n$ Q2_6           &lt;dbl&gt; 4, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1, 2…\n$ Q2_7           &lt;dbl&gt; 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1…\n$ Q2_8           &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 2, 1, 1, 1, 1, 2, 1…\n$ Q2_9           &lt;dbl&gt; 4, 1, 4, 1, 1, 1, 1, 4, 3, 1, 3, 4, 4, 4, 1, 1, 2, 1, 3…\n$ Q2_10          &lt;dbl&gt; 2, 3, 3, 2, 1, 4, 3, 2, 2, 2, 3, 2, 2, 3, 4, 2, 1, 1, 1…\n$ Q3_1           &lt;dbl&gt; 1, 3, 2, 1, 4, 1, 2, 2, 1, 3, 3, 1, 4, 1, 3, 3, 4, 4, 3…\n$ Q3_2           &lt;dbl&gt; 1, 3, 2, 1, -1, 2, 3, 3, 4, 3, 3, 1, 4, 1, 4, 4, 2, 4, …\n$ Q3_3           &lt;dbl&gt; 4, 4, 3, 4, 1, -1, 3, 3, 2, 3, 2, 4, 1, 4, 4, 4, 4, 1, …\n$ Q3_4           &lt;dbl&gt; 4, 3, 3, 4, 1, 2, 2, 1, 1, 2, 2, 4, 1, 1, 1, 1, 4, 1, 2…\n$ Q3_5           &lt;dbl&gt; 3, 3, 2, 2, 2, 2, 2, 1, 2, 3, 2, 3, 1, 1, 2, 2, 3, 3, 2…\n$ Q3_6           &lt;dbl&gt; 2, 2, 2, 1, 4, 2, 2, 2, 1, 2, 3, 1, 4, 1, 1, 2, 4, 3, 2…\n$ Q4_1           &lt;dbl&gt; 2, 2, 2, 1, 1, 4, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2…\n$ Q4_2           &lt;dbl&gt; 1, 2, 2, 2, 1, 3, 1, 1, 2, 3, 2, 1, 1, 2, 2, 1, 1, 1, 1…\n$ Q4_3           &lt;dbl&gt; 2, 2, 3, 2, 1, 3, 1, 2, 2, 3, 3, 1, 1, 2, 2, 1, 1, 1, 1…\n$ Q4_4           &lt;dbl&gt; 2, 3, 3, 2, 1, 3, 2, 2, 4, 3, 3, 2, 3, 4, 4, 2, 1, 1, 2…\n$ Q4_5           &lt;dbl&gt; 2, 3, 2, 2, 1, 4, 1, 2, 3, 2, 3, 1, 2, 2, 2, 1, 1, 2, 3…\n$ Q4_6           &lt;dbl&gt; 2, 1, 3, 2, 1, 2, 1, 3, 3, 3, 3, 2, 4, 2, 1, 2, 1, 1, 3…\n$ Q5             &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1…\n$ Q6             &lt;dbl&gt; 2, 2, 1, 3, 2, 4, 1, 1, 3, 3, 3, 2, 4, 4, 3, 1, 2, 3, 2…\n$ Q7             &lt;dbl&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n$ Q8_1           &lt;dbl&gt; 3, 2, 3, 3, 1, 3, 2, 4, 3, 2, 2, 4, 1, 4, 1, 1, 4, 1, 3…\n$ Q8_2           &lt;dbl&gt; 4, 3, 2, 2, 3, 3, -1, 4, 4, 3, 2, 3, 4, 4, 3, 4, 2, 3, …\n$ Q8_3           &lt;dbl&gt; 2, 2, 1, 2, 2, 3, 2, 1, 3, 2, 2, 2, 1, 4, 2, 2, 2, 3, 2…\n$ Q8_4           &lt;dbl&gt; 1, 2, 1, 2, 3, 2, 1, 1, 2, 2, 2, 3, 3, 2, 2, 2, 1, 2, 1…\n$ Q8_5           &lt;dbl&gt; 1, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 4, 4, 2, 2, 1, 2, 2…\n$ Q8_6           &lt;dbl&gt; 1, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2…\n$ Q8_7           &lt;dbl&gt; 1, 3, 2, 2, 4, 2, 2, 4, 4, 2, 3, 1, 4, 4, 4, 4, 1, 4, 4…\n$ Q8_8           &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 3, 2, 2, 1, 1, 1, 1, 2…\n$ Q8_9           &lt;dbl&gt; 4, 2, 1, 2, 2, 2, 2, 1, 2, 2, 3, 2, 3, 2, 1, 1, 1, 2, 1…\n$ Q9_1           &lt;dbl&gt; 2, 1, 1, 1, 1, -1, 1, 1, 1, 2, 1, 2, 2, 3, 1, 1, 1, 1, …\n$ Q9_2           &lt;dbl&gt; 2, 1, 2, 2, 4, -1, 2, 2, 4, 2, 2, 2, 3, 3, 4, 4, 4, 4, …\n$ Q9_3           &lt;dbl&gt; 4, 3, 4, 4, 3, -1, 2, 3, 4, 3, 3, 4, 2, 3, 3, 3, 4, 4, …\n$ Q9_4           &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4…\n$ Q10_1          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q10_2          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q10_3          &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2…\n$ Q10_4          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q11_1          &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2…\n$ Q11_2          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q11_3          &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2…\n$ Q11_4          &lt;dbl&gt; 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2…\n$ Q11_5          &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2…\n$ Q11_6          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q14            &lt;dbl&gt; 5, 1, 5, 5, 1, -1, 1, 5, 2, 1, 1, 5, 1, 2, 1, 1, 2, 1, …\n$ Q15            &lt;dbl&gt; 1, 1, 2, 1, 5, -1, 3, 1, 4, 5, 1, 2, 5, 2, 3, 5, 1, 4, …\n$ Q16            &lt;dbl&gt; 1, 2, 1, 4, 1, -1, 3, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, …\n$ Q17_1          &lt;dbl&gt; 1, 2, 1, 1, 2, -1, 3, 1, 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, …\n$ Q17_2          &lt;dbl&gt; 1, 2, 3, 1, 2, -1, 2, 1, 1, 1, 2, 1, 4, 3, 1, 1, 1, 1, …\n$ Q17_3          &lt;dbl&gt; 1, 2, 1, 1, 4, -1, 4, 1, 2, 1, 3, 1, 4, 3, 2, 3, 1, 4, …\n$ Q17_4          &lt;dbl&gt; 3, 3, 1, 1, 4, -1, 2, 2, 4, 2, 2, 3, 2, 2, 4, 4, 1, 4, …\n$ Q18_1          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_2          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_3          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_4          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_5          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_6          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_7          &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_8          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2…\n$ Q18_9          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_10         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q19_1          &lt;dbl&gt; -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, …\n$ Q19_2          &lt;dbl&gt; -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1…\n$ Q19_3          &lt;dbl&gt; 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1…\n$ Q19_4          &lt;dbl&gt; 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -…\n$ Q19_5          &lt;dbl&gt; 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -…\n$ Q19_6          &lt;dbl&gt; 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -…\n$ Q19_7          &lt;dbl&gt; 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -…\n$ Q19_8          &lt;dbl&gt; -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, …\n$ Q19_9          &lt;dbl&gt; -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, …\n$ Q19_10         &lt;dbl&gt; -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -…\n$ Q20            &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q21            &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1…\n$ Q22            &lt;dbl&gt; NA, NA, NA, NA, NA, 7, NA, NA, NA, NA, NA, NA, 6, NA, N…\n$ Q23            &lt;dbl&gt; 2, 1, 2, 2, 1, -1, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 2, 1, …\n$ Q24            &lt;dbl&gt; 1, 3, 1, 1, 3, 4, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 1…\n$ Q25            &lt;dbl&gt; 1, 3, 2, 2, 1, 3, 1, 2, 1, 2, 3, 1, 2, 4, 1, 1, 1, 1, 1…\n$ Q26            &lt;dbl&gt; 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1…\n$ Q27_1          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_2          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_3          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_4          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_5          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_6          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q28_1          &lt;dbl&gt; 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, NA, -1, 1, -1, 1, …\n$ Q28_2          &lt;dbl&gt; 1, -1, -1, 1, 1, NA, -1, -1, -1, 1, -1, 1, NA, -1, 1, -…\n$ Q28_3          &lt;dbl&gt; 1, -1, -1, -1, 1, NA, 1, -1, 1, -1, -1, -1, NA, -1, 1, …\n$ Q28_4          &lt;dbl&gt; 1, -1, -1, 1, -1, NA, 1, -1, -1, -1, 1, 1, NA, -1, 1, -…\n$ Q28_5          &lt;dbl&gt; -1, -1, -1, -1, 1, NA, 1, -1, -1, -1, -1, 1, NA, -1, -1…\n$ Q28_6          &lt;dbl&gt; -1, 1, -1, -1, -1, NA, -1, 1, -1, -1, -1, 1, NA, -1, 1,…\n$ Q28_7          &lt;dbl&gt; 1, -1, 1, -1, 1, NA, -1, -1, 1, 1, -1, -1, NA, -1, 1, -…\n$ Q28_8          &lt;dbl&gt; -1, -1, -1, -1, -1, NA, -1, -1, -1, -1, -1, -1, NA, 1, …\n$ Q29_1          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_2          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, 1, NA, …\n$ Q29_3          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_4          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_5          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_6          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_7          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_8          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_9          &lt;dbl&gt; NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, -1, NA, …\n$ Q29_10         &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q30            &lt;dbl&gt; 2, 3, 2, 2, 1, 5, 1, 2, 1, 3, 1, 2, 1, 5, 1, 1, 2, 1, 5…\n$ Q31            &lt;dbl&gt; NA, NA, NA, NA, -1, NA, 1, NA, 1, NA, 1, NA, 2, NA, 1, …\n$ Q32            &lt;dbl&gt; 1, NA, 2, 1, NA, NA, NA, 1, NA, NA, NA, 1, NA, NA, NA, …\n$ Q33            &lt;dbl&gt; NA, 1, NA, NA, NA, -1, NA, NA, NA, 1, NA, NA, NA, 1, NA…\n$ ppage          &lt;dbl&gt; 73, 90, 53, 58, 81, 61, 80, 68, 70, 83, 43, 42, 48, 52,…\n$ educ           &lt;chr&gt; \"College\", \"College\", \"College\", \"Some college\", \"High …\n$ race           &lt;chr&gt; \"White\", \"White\", \"White\", \"Black\", \"White\", \"White\", \"…\n$ gender         &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\",…\n$ income_cat     &lt;chr&gt; \"$75-125k\", \"$125k or more\", \"$125k or more\", \"$40-75k\"…\n$ voter_category &lt;chr&gt; \"always\", \"always\", \"sporadic\", \"sporadic\", \"always\", \"…"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-the-data-straight-forward-stuff",
    "href": "slides/24-classification-trees-2.html#cleaning-the-data-straight-forward-stuff",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning the Data: Straight Forward Stuff",
    "text": "Cleaning the Data: Straight Forward Stuff\n\nvoter_clean &lt;- voter_data |&gt; \n  select(-RespId, -weight, -Q1) |&gt;\n  mutate(\n    educ = factor(educ, levels = c(\"High school or less\", \"Some college\", \"College\")),\n    income_cat = factor(income_cat, levels = c(\"Less than $40k\", \"$40-75k \",\n                                               \"$75-125k\", \"$125k or more\")),\n    voter_category = factor(voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n  )"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-data-missing-data",
    "href": "slides/24-classification-trees-2.html#cleaning-data-missing-data",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning Data: Missing Data",
    "text": "Cleaning Data: Missing Data\n\nvoter_data |&gt; \n  summarize(across(everything(), ~sum(is.na(.x)))) |&gt; \n  pivot_longer(everything()) |&gt; \n  filter(value &gt; 0) |&gt; \n  kable() |&gt; \n  kable_paper() |&gt; \n  scroll_box(width = \"1050px\", height = \"400px\")\n\n\n\n\n\nname\nvalue\n\n\n\n\nQ22\n5350\n\n\nQ28_1\n534\n\n\nQ28_2\n534\n\n\nQ28_3\n534\n\n\nQ28_4\n534\n\n\nQ28_5\n534\n\n\nQ28_6\n534\n\n\nQ28_7\n534\n\n\nQ28_8\n534\n\n\nQ29_1\n4494\n\n\nQ29_2\n4494\n\n\nQ29_3\n4494\n\n\nQ29_4\n4494\n\n\nQ29_5\n4494\n\n\nQ29_6\n4494\n\n\nQ29_7\n4494\n\n\nQ29_8\n4494\n\n\nQ29_9\n4494\n\n\nQ29_10\n4494\n\n\nQ31\n4244\n\n\nQ32\n3834\n\n\nQ33\n3594"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-the-data-q22-28-29-31-32-33",
    "href": "slides/24-classification-trees-2.html#cleaning-the-data-q22-28-29-31-32-33",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning the Data: Q22, 28, 29, 31, 32, 33",
    "text": "Cleaning the Data: Q22, 28, 29, 31, 32, 33\n\nDealing with missing values should depend on what “missing” means in the context of each variable"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-the-data-q22",
    "href": "slides/24-classification-trees-2.html#cleaning-the-data-q22",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning the Data: Q22",
    "text": "Cleaning the Data: Q22\n\nvoter_clean |&gt; \n  count(Q22) |&gt; \n  kable() |&gt; \n  kable_paper() |&gt; \n  scroll_box(width = \"1050px\", height = \"400px\")\n\n\n\n\n\nQ22\nn\n\n\n\n\n-1\n12\n\n\n1\n27\n\n\n2\n107\n\n\n3\n19\n\n\n4\n116\n\n\n5\n19\n\n\n6\n100\n\n\n7\n86\n\n\nNA\n5350"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#uh-oh--1s",
    "href": "slides/24-classification-trees-2.html#uh-oh--1s",
    "title": "MATH 427: Classification Trees",
    "section": "Uh-Oh… -1s?",
    "text": "Uh-Oh… -1s?\n\n\n\nvoter_clean |&gt; filter(Q22 == -1) |&gt; select(Q20, Q22) |&gt; kable() |&gt; \n  kable_paper() |&gt; scroll_box(width = \"1050px\", height = \"400px\")\n\n\n\n\n\nQ20\nQ22\n\n\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n2\n-1\n\n\n\n\n\n\n\n\n\nvoter_clean |&gt; filter(is.na(Q22)) |&gt; select(Q20, Q22) |&gt; kable() |&gt; \n  kable_paper() |&gt; scroll_box(width = \"1050px\", height = \"400px\")\n\n\n\n\n\nQ20\nQ22\n\n\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n-1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA\n\n\n1\nNA"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#more-cleaning--1s",
    "href": "slides/24-classification-trees-2.html#more-cleaning--1s",
    "title": "MATH 427: Classification Trees",
    "section": "More Cleaning: -1s",
    "text": "More Cleaning: -1s\n\nvoter_clean |&gt; \n  summarize(across(everything(), ~sum(.x == -1))) |&gt; \n  pivot_longer(everything()) |&gt; \n  filter(value &gt; 0)  |&gt; kable() |&gt; \n  kable_paper() |&gt; scroll_box(width = \"1050px\", height = \"400px\")\n\n\n\n\n\nname\nvalue\n\n\n\n\nQ2_1\n35\n\n\nQ2_2\n36\n\n\nQ2_3\n36\n\n\nQ2_4\n34\n\n\nQ2_5\n28\n\n\nQ2_6\n31\n\n\nQ2_7\n44\n\n\nQ2_8\n26\n\n\nQ2_9\n36\n\n\nQ2_10\n38\n\n\nQ3_1\n39\n\n\nQ3_2\n53\n\n\nQ3_3\n54\n\n\nQ3_4\n32\n\n\nQ3_5\n45\n\n\nQ3_6\n34\n\n\nQ4_1\n39\n\n\nQ4_2\n39\n\n\nQ4_3\n33\n\n\nQ4_4\n38\n\n\nQ4_5\n44\n\n\nQ4_6\n33\n\n\nQ5\n31\n\n\nQ6\n47\n\n\nQ7\n61\n\n\nQ8_1\n47\n\n\nQ8_2\n49\n\n\nQ8_3\n42\n\n\nQ8_4\n34\n\n\nQ8_5\n49\n\n\nQ8_6\n26\n\n\nQ8_7\n42\n\n\nQ8_8\n29\n\n\nQ8_9\n37\n\n\nQ9_1\n74\n\n\nQ9_2\n80\n\n\nQ9_3\n75\n\n\nQ9_4\n82\n\n\nQ10_1\n21\n\n\nQ10_2\n27\n\n\nQ10_3\n27\n\n\nQ10_4\n18\n\n\nQ11_1\n21\n\n\nQ11_2\n24\n\n\nQ11_3\n21\n\n\nQ11_4\n15\n\n\nQ11_5\n16\n\n\nQ11_6\n22\n\n\nQ14\n204\n\n\nQ15\n192\n\n\nQ16\n56\n\n\nQ17_1\n54\n\n\nQ17_2\n59\n\n\nQ17_3\n55\n\n\nQ17_4\n57\n\n\nQ18_1\n49\n\n\nQ18_2\n55\n\n\nQ18_3\n48\n\n\nQ18_4\n56\n\n\nQ18_5\n55\n\n\nQ18_6\n53\n\n\nQ18_7\n56\n\n\nQ18_8\n53\n\n\nQ18_9\n49\n\n\nQ18_10\n52\n\n\nQ19_1\n4139\n\n\nQ19_2\n2818\n\n\nQ19_3\n2777\n\n\nQ19_4\n3599\n\n\nQ19_5\n3488\n\n\nQ19_6\n2705\n\n\nQ19_7\n3648\n\n\nQ19_8\n4167\n\n\nQ19_9\n4420\n\n\nQ19_10\n5529\n\n\nQ20\n20\n\n\nQ21\n23\n\n\nQ23\n109\n\n\nQ24\n73\n\n\nQ25\n27\n\n\nQ26\n25\n\n\nQ27_1\n67\n\n\nQ27_2\n59\n\n\nQ27_3\n80\n\n\nQ27_4\n55\n\n\nQ27_5\n76\n\n\nQ27_6\n56\n\n\nQ30\n48"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#how-should-we-handle--1s",
    "href": "slides/24-classification-trees-2.html#how-should-we-handle--1s",
    "title": "MATH 427: Classification Trees",
    "section": "How should we handle -1s?",
    "text": "How should we handle -1s?\n\n\nSeems like -1 means: question asked and not answered, or answer not ranked\nSeems like NA means: question never asked\nWhat should we do?\nCouple methods:\n\nReplace -1 with “Not answered”\nReplace -1 with NA\nReplace -1 with NA and add extra columns QX_Ranked_Answered, (Yes/No)\n\nCan do this using step_indicate_na in our recipe\nFirst, deal with current NAs\nSecond, convert -1s to NA"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#dealing-with-current-nas-q22",
    "href": "slides/24-classification-trees-2.html#dealing-with-current-nas-q22",
    "title": "MATH 427: Classification Trees",
    "section": "Dealing with current NAs: Q22",
    "text": "Dealing with current NAs: Q22\n\nvoter_clean &lt;- voter_clean |&gt; \n  filter(Q22 != 5 | is.na(Q22)) |&gt; \n  mutate(Q22 = as_factor(Q22),\n         Q22 = if_else(is.na(Q22), \"Not Asked\", Q22))"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-q28",
    "href": "slides/24-classification-trees-2.html#cleaning-q28",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning Q28",
    "text": "Cleaning Q28\n\nvoter_clean |&gt; \n  count(Q28_1) |&gt; \n  kable()\n\n\n\n\nQ28_1\nn\n\n\n\n\n-1\n1248\n\n\n1\n4049\n\n\nNA\n520"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-q28-1",
    "href": "slides/24-classification-trees-2.html#cleaning-q28-1",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning Q28",
    "text": "Cleaning Q28\n\nSeems like\n\n-1 means not selected\nSeems like 1 means selected\nNA means never asked"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-q28-2",
    "href": "slides/24-classification-trees-2.html#cleaning-q28-2",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning Q28",
    "text": "Cleaning Q28\n\nvoter_clean &lt;- voter_clean |&gt; \n  mutate(across(Q28_1:Q28_8, ~if_else(.x == -1, 0, .x)),\n         across(Q28_1:Q28_8, ~ as_factor(.x)),\n         across(Q28_1:Q28_8, ~if_else(is.na(.x) , \"Not Asked\", .x))\n  )"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-q29",
    "href": "slides/24-classification-trees-2.html#cleaning-q29",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning Q29",
    "text": "Cleaning Q29\n\nvoter_clean &lt;- voter_clean |&gt; \n  mutate(across(Q29_1:Q29_10, ~if_else(.x == -1, 0, .x)),\n         across(Q29_1:Q29_10, ~ as_factor(.x)),\n         across(Q29_1:Q29_8, ~if_else(is.na(.x) , \"Not Asked\", .x))\n  )"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#cleaning-q31q33-handling-party",
    "href": "slides/24-classification-trees-2.html#cleaning-q31q33-handling-party",
    "title": "MATH 427: Classification Trees",
    "section": "Cleaning Q31:Q33: Handling Party",
    "text": "Cleaning Q31:Q33: Handling Party\n\nvoter_clean &lt;- voter_clean |&gt; \nmutate(\n  Party_ID = as_factor(case_when(\n    Q31 == 1 ~ \"Strong Republican\",\n    Q31 == 2 ~ \"Republican\",\n    Q32 == 1  ~ \"Strong Democrat\",\n    Q32 == 2 ~ \"Democrat\",\n    Q33 == 1 ~ \"Lean Republican\",\n    Q33 == 2 ~ \"Lean Democrat\",\n    TRUE ~ \"Other\"\n  )),\n  Party_ID = factor(Party_ID, levels =c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n                                          \"Other\", \"Lean Democrat\", \"Democrat\", \"Strong Democrat\")))"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#finally-dealing-with--1s",
    "href": "slides/24-classification-trees-2.html#finally-dealing-with--1s",
    "title": "MATH 427: Classification Trees",
    "section": "Finally: Dealing with -1s",
    "text": "Finally: Dealing with -1s\n\nvoter_clean &lt;- voter_clean |&gt; \n  mutate(across(!ppage, ~as_factor(if_else(.x == -1, NA, .x))))"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#split-data",
    "href": "slides/24-classification-trees-2.html#split-data",
    "title": "MATH 427: Classification Trees",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\n\nvoter_splits &lt;- initial_split(voter_clean, prop = 0.7, strata = voter_category)\nvoter_train &lt;- training(voter_splits)\nvoter_test &lt;- testing(voter_splits)"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#define-model",
    "href": "slides/24-classification-trees-2.html#define-model",
    "title": "MATH 427: Classification Trees",
    "section": "Define Model",
    "text": "Define Model\n\ntree_model &lt;- decision_tree(cost_complexity = tune(),\n                            min_n = tune(), \n                            tree_depth = tune()) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"classification\")"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#define-recipe",
    "href": "slides/24-classification-trees-2.html#define-recipe",
    "title": "MATH 427: Classification Trees",
    "section": "Define Recipe",
    "text": "Define Recipe\n\ntree_recipe &lt;- recipe(voter_category ~ . , data = voter_train) |&gt; \n  step_indicate_na(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt; \n  step_dummy(all_nominal_predictors(), one_hot = TRUE)"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#define-workflow",
    "href": "slides/24-classification-trees-2.html#define-workflow",
    "title": "MATH 427: Classification Trees",
    "section": "Define Workflow",
    "text": "Define Workflow\n\ntree_wf &lt;- workflow() |&gt; \n  add_model(tree_model) |&gt; \n  add_recipe(tree_recipe)"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#tune-model",
    "href": "slides/24-classification-trees-2.html#tune-model",
    "title": "MATH 427: Classification Trees",
    "section": "Tune Model",
    "text": "Tune Model\n\nvoter_folds &lt;- vfold_cv(voter_train, v = 5, repeats = 10)\ntuning_grid &lt;- grid_latin_hypercube(cost_complexity(range = c(-10, 1)), \n                                    min_n(range = c(1, 30)),\n                                    tree_depth(range = c(1, 30)), size = 50)\ntuned_model &lt;- tune_grid(tree_wf,\n                         resamples = voter_folds,\n                         grid = tuning_grid,\n                         metrics = metric_set(accuracy))"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#select-best-tree",
    "href": "slides/24-classification-trees-2.html#select-best-tree",
    "title": "MATH 427: Classification Trees",
    "section": "Select Best Tree",
    "text": "Select Best Tree\n\nbest_combo &lt;- select_best(tuned_model, metric = \"accuracy\")\nbest_tree &lt;-  tree_wf |&gt; \n  finalize_workflow(best_combo) |&gt; \n  fit(voter_train)\n\nbest_combo |&gt; kable()\n\n\n\n\ncost_complexity\ntree_depth\nmin_n\n.config\n\n\n\n\n0.0035352\n15\n20\nPreprocessor1_Model17"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#select-best-tree-1",
    "href": "slides/24-classification-trees-2.html#select-best-tree-1",
    "title": "MATH 427: Classification Trees",
    "section": "Select Best Tree",
    "text": "Select Best Tree\n\nbest_tree |&gt; extract_fit_engine() |&gt; rpart.plot()"
  },
  {
    "objectID": "slides/24-classification-trees-2.html#interpreting-the-tree",
    "href": "slides/24-classification-trees-2.html#interpreting-the-tree",
    "title": "MATH 427: Classification Trees",
    "section": "Interpreting the tree",
    "text": "Interpreting the tree\n\nWhat features seem to be more important?\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/29-imbalance.html#computational-set-up",
    "href": "slides/29-imbalance.html#computational-set-up",
    "title": "MATH 427: Class Imbalance",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/29-imbalance.html#exploring-with-app",
    "href": "slides/29-imbalance.html#exploring-with-app",
    "title": "MATH 427: Class Imbalance",
    "section": "Exploring with App",
    "text": "Exploring with App\n\nApp\n\nBreak into groups\nInvestigate how your performance metrics change between balanced data and unbalanced data\nAdditional Considerations:\n\nImpact of boundaries/models?\nImpact of sample size?\nImpact of noise level?\n\nPlease write down observations so we can discuss"
  },
  {
    "objectID": "slides/29-imbalance.html#class-imbalance",
    "href": "slides/29-imbalance.html#class-imbalance",
    "title": "MATH 427: Class Imbalance",
    "section": "Class-Imbalance",
    "text": "Class-Imbalance\n\nClass-imbalance occurs where your the classes in your response greatly differ in terms of how common they are\nOccurs frequently:\n\nMedicine: survival/death\nAdmissions: enrollment/non-enrollment\nFinance: repaid loan/defaulted\nTech: Clicked on ad/Didn’t click\nTech: Churn rate\nFinance: Fraud"
  },
  {
    "objectID": "slides/29-imbalance.html#data-haberman",
    "href": "slides/29-imbalance.html#data-haberman",
    "title": "MATH 427: Class Imbalance",
    "section": "Data: haberman",
    "text": "Data: haberman\nStudy conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\nGoal: predict whether a patient survived after undergoing surgery for breast cancer.\n\nhaberman &lt;- read_csv(\"../data/haberman.data\",\n                     col_names = c(\"Age\", \"OpYear\", \"AxNodes\", \"Survival\"))\nhaberman |&gt; head() |&gt; kable()\n\n\n\n\nAge\nOpYear\nAxNodes\nSurvival\n\n\n\n\n30\n64\n1\n1\n\n\n30\n62\n3\n1\n\n\n30\n65\n0\n1\n\n\n31\n59\n2\n1\n\n\n31\n65\n4\n1\n\n\n33\n58\n10\n1"
  },
  {
    "objectID": "slides/29-imbalance.html#quick-clean",
    "href": "slides/29-imbalance.html#quick-clean",
    "title": "MATH 427: Class Imbalance",
    "section": "Quick Clean",
    "text": "Quick Clean\n\nhaberman &lt;- haberman |&gt; \n  mutate(Survival = factor(if_else(Survival == 1, \"Survived\", \"Died\"),\n                           levels = c(\"Died\", \"Survived\")))\nhaberman |&gt; head() |&gt; kable()\n\n\n\n\nAge\nOpYear\nAxNodes\nSurvival\n\n\n\n\n30\n64\n1\nSurvived\n\n\n30\n62\n3\nSurvived\n\n\n30\n65\n0\nSurvived\n\n\n31\n59\n2\nSurvived\n\n\n31\n65\n4\nSurvived\n\n\n33\n58\n10\nSurvived"
  },
  {
    "objectID": "slides/29-imbalance.html#split-data",
    "href": "slides/29-imbalance.html#split-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\nhab_splits &lt;- initial_split(haberman, prop = 0.75, strata = Survival)\nhab_train &lt;- training(hab_splits)\nhab_test &lt;- testing(hab_splits)"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-response",
    "href": "slides/29-imbalance.html#visualizing-response",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Response",
    "text": "Visualizing Response\n\nhab_train |&gt; \n  ggplot(aes(y = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/29-imbalance.html#fitting-model",
    "href": "slides/29-imbalance.html#fitting-model",
    "title": "MATH 427: Class Imbalance",
    "section": "Fitting Model",
    "text": "Fitting Model\n\nlr_model &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\n\nlr_fit &lt;- lr_model |&gt; \n  fit(Survival ~ . , data = hab_train)"
  },
  {
    "objectID": "slides/29-imbalance.html#confusion-matrix",
    "href": "slides/29-imbalance.html#confusion-matrix",
    "title": "MATH 427: Class Imbalance",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  conf_mat(truth = Survival, estimate = .pred_class) |&gt; autoplot(\"heatmap\")"
  },
  {
    "objectID": "slides/29-imbalance.html#performance-metrics",
    "href": "slides/29-imbalance.html#performance-metrics",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance Metrics",
    "text": "Performance Metrics\n\nhab_metrics &lt;- metric_set(accuracy, precision, recall)\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7284879\n\n\n\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7692308\n\n\nprecision\nbinary\n0.8000000\n\n\nrecall\nbinary\n0.1904762"
  },
  {
    "objectID": "slides/29-imbalance.html#recall-is-bad",
    "href": "slides/29-imbalance.html#recall-is-bad",
    "title": "MATH 427: Class Imbalance",
    "section": "Recall is BAD!",
    "text": "Recall is BAD!\n\nSince there are so few deaths, model always predicts a low probability of death\nIdea: just because you you have a HIGHER probability of death doesn’t mean have a HIGH probability of death"
  },
  {
    "objectID": "slides/29-imbalance.html#what-do-we-do",
    "href": "slides/29-imbalance.html#what-do-we-do",
    "title": "MATH 427: Class Imbalance",
    "section": "What do we do?",
    "text": "What do we do?\n\nDepends on what your goal is…\nAsk yourself: What is most important to my problem?\n\nAccurate probabilities?\nOverall accuracy?\nEffective identification of a specific class (e.g. positives)?\nLow false-positive rate?\n\nDiscussion: Let’s think of scenarios where each one of these is the most important."
  },
  {
    "objectID": "slides/29-imbalance.html#solutions-to-class-imbalance",
    "href": "slides/29-imbalance.html#solutions-to-class-imbalance",
    "title": "MATH 427: Class Imbalance",
    "section": "Solutions to Class Imbalance",
    "text": "Solutions to Class Imbalance\n\nSampling-based solutions (done during pre-processing)\n\nOver-sample minority class\nUnder-sample majority class\nCombination of both (e.g. SMOTE)\n\nWeight class/objective function"
  },
  {
    "objectID": "slides/29-imbalance.html#over-sampling-minority-class",
    "href": "slides/29-imbalance.html#over-sampling-minority-class",
    "title": "MATH 427: Class Imbalance",
    "section": "Over-sampling minority class",
    "text": "Over-sampling minority class\n\nUpsample: think bootstrapping for final sample is larger than original\nIdea: upsample minority class until it is same size(ish) as majority class"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-data",
    "href": "slides/29-imbalance.html#visualizing-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Data",
    "text": "Visualizing Data\n\nhab_train |&gt; ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/29-imbalance.html#upsample-recipe",
    "href": "slides/29-imbalance.html#upsample-recipe",
    "title": "MATH 427: Class Imbalance",
    "section": "Upsample Recipe",
    "text": "Upsample Recipe\n\nlibrary(themis)\nupsample_recipe &lt;- recipe(Survival ~ ., data = hab_train) |&gt; \n  step_upsample(Survival, over_ratio = 1)\n\nhab_upsample &lt;- upsample_recipe |&gt; prep(hab_train) |&gt; bake(new_data = NULL)"
  },
  {
    "objectID": "slides/29-imbalance.html#upsampled-data",
    "href": "slides/29-imbalance.html#upsampled-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Upsampled Data",
    "text": "Upsampled Data\n\nhab_upsample |&gt;  ggplot(aes(x = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-upsampled-data",
    "href": "slides/29-imbalance.html#visualizing-upsampled-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Upsampled Data",
    "text": "Visualizing Upsampled Data\n\nhab_upsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-upsampled-data-no-jitter",
    "href": "slides/29-imbalance.html#visualizing-upsampled-data-no-jitter",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Upsampled Data: No Jitter",
    "text": "Visualizing Upsampled Data: No Jitter\n\nhab_upsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()"
  },
  {
    "objectID": "slides/29-imbalance.html#performance-consideration",
    "href": "slides/29-imbalance.html#performance-consideration",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance consideration",
    "text": "Performance consideration\n\nPro:\n\nPreserves all information in the data set\n\nCon:\n\nModels will probably over-align to the noise in the minority class"
  },
  {
    "objectID": "slides/29-imbalance.html#under-sampling-majority-class",
    "href": "slides/29-imbalance.html#under-sampling-majority-class",
    "title": "MATH 427: Class Imbalance",
    "section": "Under-sampling majority class",
    "text": "Under-sampling majority class\n\nDownsample: collect a random sample smaller than the original sample\nIdea: down sample majority class until it is same size(ish) as minority class"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-data-1",
    "href": "slides/29-imbalance.html#visualizing-data-1",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Data",
    "text": "Visualizing Data\n\nhab_train |&gt; ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/29-imbalance.html#downsample-recipe",
    "href": "slides/29-imbalance.html#downsample-recipe",
    "title": "MATH 427: Class Imbalance",
    "section": "Downsample Recipe",
    "text": "Downsample Recipe\n\ndownsample_recipe &lt;- recipe(Survival ~ ., data = hab_train) |&gt; \n  step_downsample(Survival, under_ratio = 1)\n\nhab_downsample &lt;- downsample_recipe |&gt; prep(hab_train) |&gt; bake(new_data = NULL)"
  },
  {
    "objectID": "slides/29-imbalance.html#downsample-data",
    "href": "slides/29-imbalance.html#downsample-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Downsample Data",
    "text": "Downsample Data\n\nhab_downsample |&gt;  ggplot(aes(x = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-downsampled-data",
    "href": "slides/29-imbalance.html#visualizing-downsampled-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Downsampled Data",
    "text": "Visualizing Downsampled Data\n\nhab_downsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-downsampled-data-no-jitter",
    "href": "slides/29-imbalance.html#visualizing-downsampled-data-no-jitter",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Downsampled Data: No Jitter",
    "text": "Visualizing Downsampled Data: No Jitter\n\nhab_downsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()"
  },
  {
    "objectID": "slides/29-imbalance.html#performance-considerations",
    "href": "slides/29-imbalance.html#performance-considerations",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance considerations",
    "text": "Performance considerations\n\nPro:\n\nModel doesn’t over-align to noise in minority class\n\nCon:\n\nLose information from majority class"
  },
  {
    "objectID": "slides/29-imbalance.html#smote",
    "href": "slides/29-imbalance.html#smote",
    "title": "MATH 427: Class Imbalance",
    "section": "SMOTE",
    "text": "SMOTE\n\nBasic idea:\n\nBoth upsample minority and downsample majority (Tidymodel implementation only upsamples)\n\nBetter Upsampling: Instead of just randomly replicating minority observations\n\nFind (minority) nearest neighbors of each minority observation\nInterpolate line between them\nUpsample by randomly generating points in interpolated lines"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-data-2",
    "href": "slides/29-imbalance.html#visualizing-data-2",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Data",
    "text": "Visualizing Data\n\nhab_train |&gt; ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/29-imbalance.html#smote-recipe",
    "href": "slides/29-imbalance.html#smote-recipe",
    "title": "MATH 427: Class Imbalance",
    "section": "SMOTE Recipe",
    "text": "SMOTE Recipe\n\nsmote_recipe &lt;- recipe(Survival ~ ., data = hab_train) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_smote(Survival, over_ratio = 1, neighbors = 5)\n\nhab_smote &lt;- smote_recipe |&gt; prep(hab_train) |&gt; bake(new_data = NULL)"
  },
  {
    "objectID": "slides/29-imbalance.html#smote-data",
    "href": "slides/29-imbalance.html#smote-data",
    "title": "MATH 427: Class Imbalance",
    "section": "SMOTE Data",
    "text": "SMOTE Data\n\nhab_smote |&gt;  ggplot(aes(x = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-smote-data",
    "href": "slides/29-imbalance.html#visualizing-smote-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing SMOTE Data",
    "text": "Visualizing SMOTE Data\n\nhab_smote |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/29-imbalance.html#visualizing-smote-data-no-jitter",
    "href": "slides/29-imbalance.html#visualizing-smote-data-no-jitter",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing SMOTE Data: No Jitter",
    "text": "Visualizing SMOTE Data: No Jitter\n\nhab_smote |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()"
  },
  {
    "objectID": "slides/29-imbalance.html#performance-considerations-1",
    "href": "slides/29-imbalance.html#performance-considerations-1",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance considerations",
    "text": "Performance considerations\n\nPro:\n\nModel doesn’t over-align (as much) to noise in minority class\nDon’t lose (as much) information from majority class\n\nCon:\n\nCreating new information out of nowhere"
  },
  {
    "objectID": "slides/29-imbalance.html#fitting-models",
    "href": "slides/29-imbalance.html#fitting-models",
    "title": "MATH 427: Class Imbalance",
    "section": "Fitting models",
    "text": "Fitting models\n\noversamp_fit &lt;- workflow() |&gt; add_recipe(upsample_recipe) |&gt; \n  add_model(lr_model) |&gt; fit(hab_train)\ndownsamp_fit &lt;- workflow() |&gt; add_recipe(downsample_recipe) |&gt; \n  add_model(lr_model)  |&gt; fit(hab_train)\nsmote_fit &lt;- workflow() |&gt; add_recipe(smote_recipe) |&gt; \n  add_model(lr_model)  |&gt; fit(hab_train)"
  },
  {
    "objectID": "slides/29-imbalance.html#evaluate-performance",
    "href": "slides/29-imbalance.html#evaluate-performance",
    "title": "MATH 427: Class Imbalance",
    "section": "Evaluate Performance",
    "text": "Evaluate Performance\n\noversamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7343358\n\n\n\n\ndownsamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7243108\n\n\n\n\nsmote_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7251462"
  },
  {
    "objectID": "slides/29-imbalance.html#evaluate-performance-1",
    "href": "slides/29-imbalance.html#evaluate-performance-1",
    "title": "MATH 427: Class Imbalance",
    "section": "Evaluate Performance",
    "text": "Evaluate Performance\n\n\noversamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7692308\n\n\nprecision\nbinary\n0.5714286\n\n\nrecall\nbinary\n0.5714286\n\n\n\n\n\n\n\ndownsamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7435897\n\n\nprecision\nbinary\n0.5217391\n\n\nrecall\nbinary\n0.5714286\n\n\n\n\n\n\n\nsmote_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7435897\n\n\nprecision\nbinary\n0.5217391\n\n\nrecall\nbinary\n0.5714286\n\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/22-classification-trees.html#computational-set-up",
    "href": "slides/22-classification-trees.html#computational-set-up",
    "title": "MATH 427: Classification Trees",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/22-classification-trees.html#decision-trees",
    "href": "slides/22-classification-trees.html#decision-trees",
    "title": "MATH 427: Classification Trees",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nAdvantages\n\nEasy to explain and interpret\nClosely mirror human decision-making\nCan be displayed graphically, and are easily interpreted by non-experts\nDoes not require standardization of predictors\nCan handle missing data directly\nCan easily capture non-linear patterns\n\nDisadvantages\n\nDo not have same level of prediction accuracy\nNot very robust"
  },
  {
    "objectID": "slides/22-classification-trees.html#last-time",
    "href": "slides/22-classification-trees.html#last-time",
    "title": "MATH 427: Classification Trees",
    "section": "Last Time",
    "text": "Last Time\n\n\nRegression Trees: Decision Trees for Regression Problems\nHow are they fit?\nWhat is pruning? Why do we do it?\nWhat tuning parameter did we talk about last time?\nToday: Classification Trees"
  },
  {
    "objectID": "slides/22-classification-trees.html#classification-trees",
    "href": "slides/22-classification-trees.html#classification-trees",
    "title": "MATH 427: Classification Trees",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nPredictions:\n\nClasses: most common class at terminal node\nProbability: proportion of each class at terminal node\n\nRest of tree: same as regression tree"
  },
  {
    "objectID": "slides/22-classification-trees.html#classification-trees-1",
    "href": "slides/22-classification-trees.html#classification-trees-1",
    "title": "MATH 427: Classification Trees",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nPredictions:\n\nClasses: most common class at terminal node\nProbability: proportion of each class at terminal node\n\nRest of tree: same as regression tree"
  },
  {
    "objectID": "slides/22-classification-trees.html#exploring-decision-trees-w-app",
    "href": "slides/22-classification-trees.html#exploring-decision-trees-w-app",
    "title": "MATH 427: Classification Trees",
    "section": "Exploring Decision Trees w/ App",
    "text": "Exploring Decision Trees w/ App\n\nDr. F will split you into four groups\nOn one of your computers connect to a tv and open this app\nDo the following based on your group number:\n\n1: Choose plane on the first screen\n2: Choose circle on the first screen\n3: Choose parabola on the first screen\n4: Choose sine curve on the first screen\n\nWe will generate data from this population… do you think KNN, logistic regression, or a decision tree will yield a better classifier? Why?"
  },
  {
    "objectID": "slides/22-classification-trees.html#exploring-decision-trees-w-app-1",
    "href": "slides/22-classification-trees.html#exploring-decision-trees-w-app-1",
    "title": "MATH 427: Classification Trees",
    "section": "Exploring Decision Trees w/ App",
    "text": "Exploring Decision Trees w/ App\n\nChoose one of the populations\nGenerate some data\nFit a decision tree to the data and see how the different hyper parameters impact the resulting model:\n\ncomplexity parameter (cp): the larger the number the more pruning\nMinimum leaf size: the minimum number of observations from the training data that must be contained in a leaf\nMax depth: the maximum number of splits before a terminal node\n\nWrite down any interesting observations"
  },
  {
    "objectID": "slides/22-classification-trees.html#exploring-decision-trees-w-app-2",
    "href": "slides/22-classification-trees.html#exploring-decision-trees-w-app-2",
    "title": "MATH 427: Classification Trees",
    "section": "Exploring Decision Trees w/ App",
    "text": "Exploring Decision Trees w/ App\n\nDr. F will split you into four groups\nOn one of your computers connect to a tv and open this app\nDo the following based on your group number:\n\n1: Choose plane on the first screen\n2: Choose circle on the first screen\n3: Choose parabola on the first screen\n4: Choose sine curve on the first screen\n\nWe will generate data from this population… do you think KNN, logistic regression, or a decision tree will yield a better classifier? Why?"
  },
  {
    "objectID": "slides/22-classification-trees.html#exploring-decision-trees-w-app-3",
    "href": "slides/22-classification-trees.html#exploring-decision-trees-w-app-3",
    "title": "MATH 427: Classification Trees",
    "section": "Exploring Decision Trees w/ App",
    "text": "Exploring Decision Trees w/ App\n\nChoose one of the populations\nGenerate some data\nFit a decision tree to the data and see how the different hyper parameters impact the resulting model:\n\ncomplexity parameter (cp): the larger the number the more pruning\nMinimum leaf size: the minimum number of observations from the training data that must be contained in a leaf\nMax depth: the maximum number of splits before a terminal node\n\nWrite down any interesting observations\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#computational-set-up",
    "href": "slides/20-decision-trees-continued.html#computational-set-up",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#building-a-tree-and-prediction",
    "href": "slides/20-decision-trees-continued.html#building-a-tree-and-prediction",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Building a Tree and Prediction",
    "text": "Building a Tree and Prediction\n\nFrom ISLR"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#building-a-tree",
    "href": "slides/20-decision-trees-continued.html#building-a-tree",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Building a Tree",
    "text": "Building a Tree\n\nAnyone know what a greedy algorithm is?\n\n\n\nComputationally infeasible to consider every possible partition\nIdea: top-down, greedy approach known as recursive binary splitting.\n\ntop-down because it begins at the top of the tree\ngreedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step\n\nImportant for determining whether to use a ordinal encoding or not\n\nStop when each terminal node has fewer than some predetermined number of observations"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#overfitting",
    "href": "slides/20-decision-trees-continued.html#overfitting",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Overfitting",
    "text": "Overfitting\n\nThis process described above is likely to overfit the data\nOne solution: require each split to improve performance by some amount\n\nBad Idea: sometimes seemingly meaningless cuts early on enable really good cuts later on\n\nGood solution: pruning\n\nBuild big tree and the prune off branches that are unnecessary"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#tree-pruning",
    "href": "slides/20-decision-trees-continued.html#tree-pruning",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Tree Pruning",
    "text": "Tree Pruning\n\nFrom Hands-On Machine Learning, Boehmke & Greenwell"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#bias-variance-trade-off",
    "href": "slides/20-decision-trees-continued.html#bias-variance-trade-off",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Bias-Variance Trade-Off",
    "text": "Bias-Variance Trade-Off\n\nHow does the bias-variance trade-off relate to the bias-variance trade-off?\n\nWould larger trees have high or lower bias?\nWhat about variance?"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#tree-pruning-1",
    "href": "slides/20-decision-trees-continued.html#tree-pruning-1",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Tree Pruning",
    "text": "Tree Pruning\n\nGrow a very large tree, and then prune it back to obtain a subtree.\nTerminology: cost complexity pruning or weakest link pruning\nConsider the following objective function \\[\n\\begin{aligned}\n&SSE(T) + \\alpha \\times |T|\\\\\n&\\quad= SSE(T) + \\alpha \\times (\\text{# of terminal nodes of }T)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#cost-complexity-pruning",
    "href": "slides/20-decision-trees-continued.html#cost-complexity-pruning",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Cost-Complexity Pruning",
    "text": "Cost-Complexity Pruning\n\nFit full tree \\(T_0\\) to minimise \\(SSE\\)\nSelect sub-tree \\(T\\subset T_0\\) which minimizes \\[\n\\begin{aligned}\n&SSE(T) + \\alpha \\times |T|\\\\\n&\\quad= SSE(T) + \\alpha \\times (\\text{# of terminal nodes of }T)\n\\end{aligned}\n\\]\nWhat should happen to the tree as we increase \\(\\alpha\\)?\nWhat should happen to the bias and variance as we increase \\(\\alpha\\)?\nHow should we choose \\(\\alpha\\)? Cross Validation"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#data-dcbikeshare",
    "href": "slides/20-decision-trees-continued.html#data-dcbikeshare",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Data: dcbikeshare",
    "text": "Data: dcbikeshare\nBike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. As of May 2018, there are about over 1600 bike-sharing programs around the world, providing more than 18 million bicycles for public use. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. Documentation\n\nglimpse(dcbikeshare)\n\nRows: 731\nColumns: 16\n$ instant    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dteday     &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05…\n$ season     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ yr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mnth       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ holiday    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ weekday    &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,…\n$ workingday &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,…\n$ weathersit &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ temp       &lt;dbl&gt; 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.20…\n$ atemp      &lt;dbl&gt; 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.23…\n$ hum        &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261,…\n$ windspeed  &lt;dbl&gt; 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.08…\n$ casual     &lt;dbl&gt; 331, 131, 120, 108, 82, 88, 148, 68, 54, 41, 43, 25, 38, 54…\n$ registered &lt;dbl&gt; 654, 670, 1229, 1454, 1518, 1518, 1362, 891, 768, 1280, 122…\n$ cnt        &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 126…"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#cleaning-the-data",
    "href": "slides/20-decision-trees-continued.html#cleaning-the-data",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\n\ndcbikeshare_clean &lt;- dcbikeshare |&gt; \n  select(-instant, -dteday, -casual, -registered, -yr) |&gt; \n  mutate(\n    season = as_factor(case_when(\n      season == 1 ~ \"winter\",\n      season == 2 ~ \"spring\",\n      season == 3 ~ \"summer\",\n      season == 4 ~ \"fall\"\n    )),\n    mnth = as_factor(mnth),\n    weekday = as_factor(weekday),\n    weathersit = as_factor(weathersit)\n  )"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#split-the-data",
    "href": "slides/20-decision-trees-continued.html#split-the-data",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Split the Data",
    "text": "Split the Data\n\nset.seed(427)\n\nbike_split &lt;- initial_split(dcbikeshare_clean, prop = 0.7, strata = cnt)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#recipe",
    "href": "slides/20-decision-trees-continued.html#recipe",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Recipe",
    "text": "Recipe\n\nbike_recipe &lt;- recipe(cnt ~ ., data = bike_train) |&gt;   # set up recipe\n  step_integer(season, mnth, weekday) |&gt;   # numeric conversion of levels of the predictors\n  step_dummy(all_nominal(), one_hot = TRUE)  # one-hot/dummy encode nominal categorical predictors"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#define-model-workflow",
    "href": "slides/20-decision-trees-continued.html#define-model-workflow",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Define Model Workflow",
    "text": "Define Model Workflow\n\ndec_tree_lowcc &lt;- decision_tree(cost_complexity = 10^(-4)) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\ndec_tree_highcc &lt;- decision_tree(cost_complexity = 0.1) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#visualize",
    "href": "slides/20-decision-trees-continued.html#visualize",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize\n\nlibrary(rpart.plot)\nworkflow() |&gt; \n  add_recipe(bike_recipe) |&gt; \n  add_model(dec_tree_lowcc) |&gt; \n  fit(bike_train) |&gt;\n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#visualize-1",
    "href": "slides/20-decision-trees-continued.html#visualize-1",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#visualize-2",
    "href": "slides/20-decision-trees-continued.html#visualize-2",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize\n\nlibrary(rpart.plot)\nworkflow() |&gt; \n  add_recipe(bike_recipe) |&gt; \n  add_model(dec_tree_highcc) |&gt; \n  fit(bike_train) |&gt;\n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#visualize-3",
    "href": "slides/20-decision-trees-continued.html#visualize-3",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#defin-model-workflow-with-tuning",
    "href": "slides/20-decision-trees-continued.html#defin-model-workflow-with-tuning",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Defin Model Workflow with Tuning",
    "text": "Defin Model Workflow with Tuning\n\ndec_tree &lt;- decision_tree(cost_complexity = tune()) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\ndt_wf &lt;- workflow() |&gt; \n  add_recipe(bike_recipe) |&gt; \n  add_model(dec_tree)"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#define-folds-and-tuning-grid",
    "href": "slides/20-decision-trees-continued.html#define-folds-and-tuning-grid",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Define Folds and Tuning Grid",
    "text": "Define Folds and Tuning Grid\n\nbike_folds &lt;- vfold_cv(bike_train, v = 5, repeats = 10)\n\ncp_grid &lt;- grid_regular(cost_complexity(range = c(-4, -1)), # I had to play around with these \n                             levels = 20)"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#tuning-cp",
    "href": "slides/20-decision-trees-continued.html#tuning-cp",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Tuning CP",
    "text": "Tuning CP\n\ntuning_cp_results &lt;- tune_grid(\n  dt_wf,\n  resamples= bike_folds,\n  grid = cp_grid\n)"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#plot-results",
    "href": "slides/20-decision-trees-continued.html#plot-results",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Plot Results",
    "text": "Plot Results\n\nautoplot(tuning_cp_results)"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#select-best-trees",
    "href": "slides/20-decision-trees-continued.html#select-best-trees",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Select Best Trees",
    "text": "Select Best Trees\n\n\n\nbest_tree &lt;- select_best(tuning_cp_results)\nbest_tree |&gt; kable()\n\n\n\n\ncost_complexity\n.config\n\n\n\n\n0.0054556\nPreprocessor1_Model12\n\n\n\n\n\n\n\nose_tree &lt;- select_by_one_std_err(tuning_cp_results, desc(cost_complexity))\nose_tree |&gt; kable()\n\n\n\n\ncost_complexity\n.config\n\n\n\n\n0.0078476\nPreprocessor1_Model13"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#fit-best-tree",
    "href": "slides/20-decision-trees-continued.html#fit-best-tree",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Fit Best Tree",
    "text": "Fit Best Tree\n\nbest_tree &lt;- select_best(tuning_cp_results)\nbest_wf &lt;- finalize_workflow(dt_wf, best_tree)\nbest_model &lt;- best_wf |&gt; fit(bike_train)\nbest_model |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#fit-ose-tree",
    "href": "slides/20-decision-trees-continued.html#fit-ose-tree",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Fit OSE Tree",
    "text": "Fit OSE Tree\n\nose_tree &lt;- select_best(tuning_cp_results)\nose_wf &lt;- finalize_workflow(dt_wf, ose_tree)\nose_model &lt;- ose_wf |&gt; fit(bike_train)\nose_model |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot()"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#questions",
    "href": "slides/20-decision-trees-continued.html#questions",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Questions",
    "text": "Questions\n\nWhy are both models the same but have different RMSE estimates from CV?\nWhat’s the difference between encoding mnth as an ordinal variable vs. a one-hot encoding?"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#decision-trees",
    "href": "slides/20-decision-trees-continued.html#decision-trees",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nAdvantages\n\nEasy to explain and interpret\nClosely mirror human decision-making\nCan be displayed graphically, and are easily interpreted by non-experts\nDoes not require standardization of predictors\nCan handle missing data directly\nCan easily capture non-linear patterns\n\nDisadvantages\n\nDo not have same level of prediction accuracy\nNot very robust"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#classification-trees",
    "href": "slides/20-decision-trees-continued.html#classification-trees",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nPredictions:\n\nClasses: most common class at terminal node\nProbability: proportion of each class at terminal node\n\nRest of tree: same as regression tree"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#exploring-decision-trees-w-app",
    "href": "slides/20-decision-trees-continued.html#exploring-decision-trees-w-app",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Exploring Decision Trees w/ App",
    "text": "Exploring Decision Trees w/ App\n\nDr. F will split you into four groups\nOn one of your computers connect to a tv and open this app\nDo the following based on your group number:\n\n1: Choose plane on the first screen\n2: Choose circle on the first screen\n3: Choose parabola on the first screen\n4: Choose sine curve on the first screen\n\nWe will generate data from this population… do you think KNN, logistic regression, or a decision tree will yield a better classifier? Why?"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#exploring-decision-trees-w-app-1",
    "href": "slides/20-decision-trees-continued.html#exploring-decision-trees-w-app-1",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Exploring Decision Trees w/ App",
    "text": "Exploring Decision Trees w/ App\n\nChoose one of the populations\nGenerate some data\nFit a decision tree to the data and see how the different hyper parameters impact the resulting model:\n\ncomplexity parameter (cp): the larger the number the more pruning\nMinimum leaf size: the minimum number of observations from the training data that must be contained in a leaf\nMax depth: the maximum number of splits before a terminal node\n\nWrite down any interesting observations"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#data-voter-frequency",
    "href": "slides/20-decision-trees-continued.html#data-voter-frequency",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Data: Voter Frequency",
    "text": "Data: Voter Frequency\nInfo about data\n\nvoter_data &lt;- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nglimpse(voter_data)\n\nRows: 5,836\nColumns: 119\n$ RespId         &lt;dbl&gt; 470001, 470002, 470003, 470007, 480008, 480009, 480010,…\n$ weight         &lt;dbl&gt; 0.7516, 1.0267, 1.0844, 0.6817, 0.9910, 1.0591, 1.1512,…\n$ Q1             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Q2_1           &lt;dbl&gt; 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1…\n$ Q2_2           &lt;dbl&gt; 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1…\n$ Q2_3           &lt;dbl&gt; 2, 2, 2, 1, -1, 3, 2, 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 1, …\n$ Q2_4           &lt;dbl&gt; 4, 3, 2, 3, 1, 4, 3, 2, 3, 1, 2, 4, 2, 3, 1, 1, 1, 1, 2…\n$ Q2_5           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1…\n$ Q2_6           &lt;dbl&gt; 4, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1, 2…\n$ Q2_7           &lt;dbl&gt; 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1…\n$ Q2_8           &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 2, 1, 1, 1, 1, 2, 1…\n$ Q2_9           &lt;dbl&gt; 4, 1, 4, 1, 1, 1, 1, 4, 3, 1, 3, 4, 4, 4, 1, 1, 2, 1, 3…\n$ Q2_10          &lt;dbl&gt; 2, 3, 3, 2, 1, 4, 3, 2, 2, 2, 3, 2, 2, 3, 4, 2, 1, 1, 1…\n$ Q3_1           &lt;dbl&gt; 1, 3, 2, 1, 4, 1, 2, 2, 1, 3, 3, 1, 4, 1, 3, 3, 4, 4, 3…\n$ Q3_2           &lt;dbl&gt; 1, 3, 2, 1, -1, 2, 3, 3, 4, 3, 3, 1, 4, 1, 4, 4, 2, 4, …\n$ Q3_3           &lt;dbl&gt; 4, 4, 3, 4, 1, -1, 3, 3, 2, 3, 2, 4, 1, 4, 4, 4, 4, 1, …\n$ Q3_4           &lt;dbl&gt; 4, 3, 3, 4, 1, 2, 2, 1, 1, 2, 2, 4, 1, 1, 1, 1, 4, 1, 2…\n$ Q3_5           &lt;dbl&gt; 3, 3, 2, 2, 2, 2, 2, 1, 2, 3, 2, 3, 1, 1, 2, 2, 3, 3, 2…\n$ Q3_6           &lt;dbl&gt; 2, 2, 2, 1, 4, 2, 2, 2, 1, 2, 3, 1, 4, 1, 1, 2, 4, 3, 2…\n$ Q4_1           &lt;dbl&gt; 2, 2, 2, 1, 1, 4, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2…\n$ Q4_2           &lt;dbl&gt; 1, 2, 2, 2, 1, 3, 1, 1, 2, 3, 2, 1, 1, 2, 2, 1, 1, 1, 1…\n$ Q4_3           &lt;dbl&gt; 2, 2, 3, 2, 1, 3, 1, 2, 2, 3, 3, 1, 1, 2, 2, 1, 1, 1, 1…\n$ Q4_4           &lt;dbl&gt; 2, 3, 3, 2, 1, 3, 2, 2, 4, 3, 3, 2, 3, 4, 4, 2, 1, 1, 2…\n$ Q4_5           &lt;dbl&gt; 2, 3, 2, 2, 1, 4, 1, 2, 3, 2, 3, 1, 2, 2, 2, 1, 1, 2, 3…\n$ Q4_6           &lt;dbl&gt; 2, 1, 3, 2, 1, 2, 1, 3, 3, 3, 3, 2, 4, 2, 1, 2, 1, 1, 3…\n$ Q5             &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1…\n$ Q6             &lt;dbl&gt; 2, 2, 1, 3, 2, 4, 1, 1, 3, 3, 3, 2, 4, 4, 3, 1, 2, 3, 2…\n$ Q7             &lt;dbl&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n$ Q8_1           &lt;dbl&gt; 3, 2, 3, 3, 1, 3, 2, 4, 3, 2, 2, 4, 1, 4, 1, 1, 4, 1, 3…\n$ Q8_2           &lt;dbl&gt; 4, 3, 2, 2, 3, 3, -1, 4, 4, 3, 2, 3, 4, 4, 3, 4, 2, 3, …\n$ Q8_3           &lt;dbl&gt; 2, 2, 1, 2, 2, 3, 2, 1, 3, 2, 2, 2, 1, 4, 2, 2, 2, 3, 2…\n$ Q8_4           &lt;dbl&gt; 1, 2, 1, 2, 3, 2, 1, 1, 2, 2, 2, 3, 3, 2, 2, 2, 1, 2, 1…\n$ Q8_5           &lt;dbl&gt; 1, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 4, 4, 2, 2, 1, 2, 2…\n$ Q8_6           &lt;dbl&gt; 1, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2…\n$ Q8_7           &lt;dbl&gt; 1, 3, 2, 2, 4, 2, 2, 4, 4, 2, 3, 1, 4, 4, 4, 4, 1, 4, 4…\n$ Q8_8           &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 3, 2, 2, 1, 1, 1, 1, 2…\n$ Q8_9           &lt;dbl&gt; 4, 2, 1, 2, 2, 2, 2, 1, 2, 2, 3, 2, 3, 2, 1, 1, 1, 2, 1…\n$ Q9_1           &lt;dbl&gt; 2, 1, 1, 1, 1, -1, 1, 1, 1, 2, 1, 2, 2, 3, 1, 1, 1, 1, …\n$ Q9_2           &lt;dbl&gt; 2, 1, 2, 2, 4, -1, 2, 2, 4, 2, 2, 2, 3, 3, 4, 4, 4, 4, …\n$ Q9_3           &lt;dbl&gt; 4, 3, 4, 4, 3, -1, 2, 3, 4, 3, 3, 4, 2, 3, 3, 3, 4, 4, …\n$ Q9_4           &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4…\n$ Q10_1          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q10_2          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q10_3          &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2…\n$ Q10_4          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q11_1          &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2…\n$ Q11_2          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q11_3          &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2…\n$ Q11_4          &lt;dbl&gt; 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2…\n$ Q11_5          &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2…\n$ Q11_6          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q14            &lt;dbl&gt; 5, 1, 5, 5, 1, -1, 1, 5, 2, 1, 1, 5, 1, 2, 1, 1, 2, 1, …\n$ Q15            &lt;dbl&gt; 1, 1, 2, 1, 5, -1, 3, 1, 4, 5, 1, 2, 5, 2, 3, 5, 1, 4, …\n$ Q16            &lt;dbl&gt; 1, 2, 1, 4, 1, -1, 3, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, …\n$ Q17_1          &lt;dbl&gt; 1, 2, 1, 1, 2, -1, 3, 1, 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, …\n$ Q17_2          &lt;dbl&gt; 1, 2, 3, 1, 2, -1, 2, 1, 1, 1, 2, 1, 4, 3, 1, 1, 1, 1, …\n$ Q17_3          &lt;dbl&gt; 1, 2, 1, 1, 4, -1, 4, 1, 2, 1, 3, 1, 4, 3, 2, 3, 1, 4, …\n$ Q17_4          &lt;dbl&gt; 3, 3, 1, 1, 4, -1, 2, 2, 4, 2, 2, 3, 2, 2, 4, 4, 1, 4, …\n$ Q18_1          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_2          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_3          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_4          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_5          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_6          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_7          &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_8          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2…\n$ Q18_9          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2…\n$ Q18_10         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ Q19_1          &lt;dbl&gt; -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, …\n$ Q19_2          &lt;dbl&gt; -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1…\n$ Q19_3          &lt;dbl&gt; 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1…\n$ Q19_4          &lt;dbl&gt; 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -…\n$ Q19_5          &lt;dbl&gt; 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -…\n$ Q19_6          &lt;dbl&gt; 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -…\n$ Q19_7          &lt;dbl&gt; 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -…\n$ Q19_8          &lt;dbl&gt; -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, …\n$ Q19_9          &lt;dbl&gt; -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, …\n$ Q19_10         &lt;dbl&gt; -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -…\n$ Q20            &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q21            &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1…\n$ Q22            &lt;dbl&gt; NA, NA, NA, NA, NA, 7, NA, NA, NA, NA, NA, NA, 6, NA, N…\n$ Q23            &lt;dbl&gt; 2, 1, 2, 2, 1, -1, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 2, 1, …\n$ Q24            &lt;dbl&gt; 1, 3, 1, 1, 3, 4, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 1…\n$ Q25            &lt;dbl&gt; 1, 3, 2, 2, 1, 3, 1, 2, 1, 2, 3, 1, 2, 4, 1, 1, 1, 1, 1…\n$ Q26            &lt;dbl&gt; 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1…\n$ Q27_1          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_2          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_3          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_4          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_5          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q27_6          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ Q28_1          &lt;dbl&gt; 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, NA, -1, 1, -1, 1, …\n$ Q28_2          &lt;dbl&gt; 1, -1, -1, 1, 1, NA, -1, -1, -1, 1, -1, 1, NA, -1, 1, -…\n$ Q28_3          &lt;dbl&gt; 1, -1, -1, -1, 1, NA, 1, -1, 1, -1, -1, -1, NA, -1, 1, …\n$ Q28_4          &lt;dbl&gt; 1, -1, -1, 1, -1, NA, 1, -1, -1, -1, 1, 1, NA, -1, 1, -…\n$ Q28_5          &lt;dbl&gt; -1, -1, -1, -1, 1, NA, 1, -1, -1, -1, -1, 1, NA, -1, -1…\n$ Q28_6          &lt;dbl&gt; -1, 1, -1, -1, -1, NA, -1, 1, -1, -1, -1, 1, NA, -1, 1,…\n$ Q28_7          &lt;dbl&gt; 1, -1, 1, -1, 1, NA, -1, -1, 1, 1, -1, -1, NA, -1, 1, -…\n$ Q28_8          &lt;dbl&gt; -1, -1, -1, -1, -1, NA, -1, -1, -1, -1, -1, -1, NA, 1, …\n$ Q29_1          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_2          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, 1, NA, …\n$ Q29_3          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_4          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_5          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_6          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_7          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_8          &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q29_9          &lt;dbl&gt; NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, -1, NA, …\n$ Q29_10         &lt;dbl&gt; NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,…\n$ Q30            &lt;dbl&gt; 2, 3, 2, 2, 1, 5, 1, 2, 1, 3, 1, 2, 1, 5, 1, 1, 2, 1, 5…\n$ Q31            &lt;dbl&gt; NA, NA, NA, NA, -1, NA, 1, NA, 1, NA, 1, NA, 2, NA, 1, …\n$ Q32            &lt;dbl&gt; 1, NA, 2, 1, NA, NA, NA, 1, NA, NA, NA, 1, NA, NA, NA, …\n$ Q33            &lt;dbl&gt; NA, 1, NA, NA, NA, -1, NA, NA, NA, 1, NA, NA, NA, 1, NA…\n$ ppage          &lt;dbl&gt; 73, 90, 53, 58, 81, 61, 80, 68, 70, 83, 43, 42, 48, 52,…\n$ educ           &lt;chr&gt; \"College\", \"College\", \"College\", \"Some college\", \"High …\n$ race           &lt;chr&gt; \"White\", \"White\", \"White\", \"Black\", \"White\", \"White\", \"…\n$ gender         &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\",…\n$ income_cat     &lt;chr&gt; \"$75-125k\", \"$125k or more\", \"$125k or more\", \"$40-75k\"…\n$ voter_category &lt;chr&gt; \"always\", \"always\", \"sporadic\", \"sporadic\", \"always\", \"…"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#cleaning-data-missing-data",
    "href": "slides/20-decision-trees-continued.html#cleaning-data-missing-data",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Cleaning Data: Missing Data",
    "text": "Cleaning Data: Missing Data\n\nvoter_data |&gt; \n  summarize(across(everything(), ~sum(is.na(.x)))) |&gt; \n  pivot_longer(everything()) |&gt; \n  filter(value &gt; 0)\n\n# A tibble: 22 × 2\n   name  value\n   &lt;chr&gt; &lt;int&gt;\n 1 Q22    5350\n 2 Q28_1   534\n 3 Q28_2   534\n 4 Q28_3   534\n 5 Q28_4   534\n 6 Q28_5   534\n 7 Q28_6   534\n 8 Q28_7   534\n 9 Q28_8   534\n10 Q29_1  4494\n# ℹ 12 more rows"
  },
  {
    "objectID": "slides/20-decision-trees-continued.html#cleaning-the-data-q28-29-31",
    "href": "slides/20-decision-trees-continued.html#cleaning-the-data-q28-29-31",
    "title": "MATH 427: Decision Trees Continued",
    "section": "Cleaning the Data: Q28, 29, 31",
    "text": "Cleaning the Data: Q28, 29, 31\n\nWhat should we do with question 28?\nWhat should we do with question 29?\nWhat should we do with question 31?\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/30-imbalance-2.html#computational-set-up",
    "href": "slides/30-imbalance-2.html#computational-set-up",
    "title": "MATH 427: Class Imbalance",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#exploring-with-app",
    "href": "slides/30-imbalance-2.html#exploring-with-app",
    "title": "MATH 427: Class Imbalance",
    "section": "Exploring with App",
    "text": "Exploring with App\n\nApp\n\nBreak into groups\nInvestigate how your performance metrics change between balanced data and unbalanced data\nAdditional Considerations:\n\nImpact of boundaries/models?\nImpact of sample size?\nImpact of noise level?\n\nPlease write down observations so we can discuss"
  },
  {
    "objectID": "slides/30-imbalance-2.html#class-imbalance",
    "href": "slides/30-imbalance-2.html#class-imbalance",
    "title": "MATH 427: Class Imbalance",
    "section": "Class-Imbalance",
    "text": "Class-Imbalance\n\nClass-imbalance occurs where your the classes in your response greatly differ in terms of how common they are\nOccurs frequently:\n\nMedicine: survival/death\nAdmissions: enrollment/non-enrollment\nFinance: repaid loan/defaulted\nTech: Clicked on ad/Didn’t click\nTech: Churn rate\nFinance: Fraud"
  },
  {
    "objectID": "slides/30-imbalance-2.html#data-haberman",
    "href": "slides/30-imbalance-2.html#data-haberman",
    "title": "MATH 427: Class Imbalance",
    "section": "Data: haberman",
    "text": "Data: haberman\nStudy conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\nGoal: predict whether a patient survived after undergoing surgery for breast cancer.\n\nhaberman &lt;- read_csv(\"../data/haberman.data\",\n                     col_names = c(\"Age\", \"OpYear\", \"AxNodes\", \"Survival\"))\nhaberman |&gt; head() |&gt; kable()\n\n\n\n\nAge\nOpYear\nAxNodes\nSurvival\n\n\n\n\n30\n64\n1\n1\n\n\n30\n62\n3\n1\n\n\n30\n65\n0\n1\n\n\n31\n59\n2\n1\n\n\n31\n65\n4\n1\n\n\n33\n58\n10\n1"
  },
  {
    "objectID": "slides/30-imbalance-2.html#quick-clean",
    "href": "slides/30-imbalance-2.html#quick-clean",
    "title": "MATH 427: Class Imbalance",
    "section": "Quick Clean",
    "text": "Quick Clean\n\nhaberman &lt;- haberman |&gt; \n  mutate(Survival = factor(if_else(Survival == 1, \"Survived\", \"Died\"),\n                           levels = c(\"Died\", \"Survived\")))\nhaberman |&gt; head() |&gt; kable()\n\n\n\n\nAge\nOpYear\nAxNodes\nSurvival\n\n\n\n\n30\n64\n1\nSurvived\n\n\n30\n62\n3\nSurvived\n\n\n30\n65\n0\nSurvived\n\n\n31\n59\n2\nSurvived\n\n\n31\n65\n4\nSurvived\n\n\n33\n58\n10\nSurvived"
  },
  {
    "objectID": "slides/30-imbalance-2.html#split-data",
    "href": "slides/30-imbalance-2.html#split-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\nhab_splits &lt;- initial_split(haberman, prop = 0.75, strata = Survival)\nhab_train &lt;- training(hab_splits)\nhab_test &lt;- testing(hab_splits)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-response",
    "href": "slides/30-imbalance-2.html#visualizing-response",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Response",
    "text": "Visualizing Response\n\nhab_train |&gt; \n  ggplot(aes(y = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#fitting-model",
    "href": "slides/30-imbalance-2.html#fitting-model",
    "title": "MATH 427: Class Imbalance",
    "section": "Fitting Model",
    "text": "Fitting Model\n\nlr_model &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\n\nlr_fit &lt;- lr_model |&gt; \n  fit(Survival ~ . , data = hab_train)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#confusion-matrix",
    "href": "slides/30-imbalance-2.html#confusion-matrix",
    "title": "MATH 427: Class Imbalance",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  conf_mat(truth = Survival, estimate = .pred_class) |&gt; autoplot(\"heatmap\")"
  },
  {
    "objectID": "slides/30-imbalance-2.html#performance-metrics",
    "href": "slides/30-imbalance-2.html#performance-metrics",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance Metrics",
    "text": "Performance Metrics\n\nhab_metrics &lt;- metric_set(accuracy, precision, recall)\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7284879\n\n\n\n\nlr_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7692308\n\n\nprecision\nbinary\n0.8000000\n\n\nrecall\nbinary\n0.1904762"
  },
  {
    "objectID": "slides/30-imbalance-2.html#recall-is-bad",
    "href": "slides/30-imbalance-2.html#recall-is-bad",
    "title": "MATH 427: Class Imbalance",
    "section": "Recall is BAD!",
    "text": "Recall is BAD!\n\nSince there are so few deaths, model always predicts a low probability of death\nIdea: just because you you have a HIGHER probability of death doesn’t mean have a HIGH probability of death"
  },
  {
    "objectID": "slides/30-imbalance-2.html#what-do-we-do",
    "href": "slides/30-imbalance-2.html#what-do-we-do",
    "title": "MATH 427: Class Imbalance",
    "section": "What do we do?",
    "text": "What do we do?\n\nDepends on what your goal is…\nAsk yourself: What is most important to my problem?\n\nAccurate probabilities?\nOverall accuracy?\nEffective identification of a specific class (e.g. positives)?\nLow false-positive rate?\n\nDiscussion: Let’s think of scenarios where each one of these is the most important."
  },
  {
    "objectID": "slides/30-imbalance-2.html#solutions-to-class-imbalance",
    "href": "slides/30-imbalance-2.html#solutions-to-class-imbalance",
    "title": "MATH 427: Class Imbalance",
    "section": "Solutions to Class Imbalance",
    "text": "Solutions to Class Imbalance\n\nAdjust probability threshold (we’ve already done this)\n\nIf you wanted to increase your recall would you increase or decrease your threshold?\n\nSampling-based solutions (done during pre-processing)\n\nOver-sample minority class\nUnder-sample majority class\nCombination of both (e.g. SMOTE)\n\nWeight class/objective function"
  },
  {
    "objectID": "slides/30-imbalance-2.html#over-sampling-minority-class",
    "href": "slides/30-imbalance-2.html#over-sampling-minority-class",
    "title": "MATH 427: Class Imbalance",
    "section": "Over-sampling minority class",
    "text": "Over-sampling minority class\n\nUpsample: think bootstrapping for final sample is larger than original\nIdea: upsample minority class until it is same size(ish) as majority class"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-data",
    "href": "slides/30-imbalance-2.html#visualizing-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Data",
    "text": "Visualizing Data\n\nhab_train |&gt; ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#upsample-recipe",
    "href": "slides/30-imbalance-2.html#upsample-recipe",
    "title": "MATH 427: Class Imbalance",
    "section": "Upsample Recipe",
    "text": "Upsample Recipe\n\nlibrary(themis)\nupsample_recipe &lt;- recipe(Survival ~ ., data = hab_train) |&gt; \n  step_upsample(Survival, over_ratio = 1)\n\nhab_upsample &lt;- upsample_recipe |&gt; prep(hab_train) |&gt; bake(new_data = NULL)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#upsampled-data",
    "href": "slides/30-imbalance-2.html#upsampled-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Upsampled Data",
    "text": "Upsampled Data\n\nhab_upsample |&gt;  ggplot(aes(x = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-upsampled-data",
    "href": "slides/30-imbalance-2.html#visualizing-upsampled-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Upsampled Data",
    "text": "Visualizing Upsampled Data\n\nhab_upsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-upsampled-data-no-jitter",
    "href": "slides/30-imbalance-2.html#visualizing-upsampled-data-no-jitter",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Upsampled Data: No Jitter",
    "text": "Visualizing Upsampled Data: No Jitter\n\nhab_upsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#performance-consideration",
    "href": "slides/30-imbalance-2.html#performance-consideration",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance consideration",
    "text": "Performance consideration\n\nPro:\n\nPreserves all information in the data set\n\nCon:\n\nModels will probably over-align to the noise in the minority class"
  },
  {
    "objectID": "slides/30-imbalance-2.html#under-sampling-majority-class",
    "href": "slides/30-imbalance-2.html#under-sampling-majority-class",
    "title": "MATH 427: Class Imbalance",
    "section": "Under-sampling majority class",
    "text": "Under-sampling majority class\n\nDownsample: collect a random sample smaller than the original sample\nIdea: down sample majority class until it is same size(ish) as minority class"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-data-1",
    "href": "slides/30-imbalance-2.html#visualizing-data-1",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Data",
    "text": "Visualizing Data\n\nhab_train |&gt; ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#downsample-recipe",
    "href": "slides/30-imbalance-2.html#downsample-recipe",
    "title": "MATH 427: Class Imbalance",
    "section": "Downsample Recipe",
    "text": "Downsample Recipe\n\ndownsample_recipe &lt;- recipe(Survival ~ ., data = hab_train) |&gt; \n  step_downsample(Survival, under_ratio = 1)\n\nhab_downsample &lt;- downsample_recipe |&gt; prep(hab_train) |&gt; bake(new_data = NULL)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#downsample-data",
    "href": "slides/30-imbalance-2.html#downsample-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Downsample Data",
    "text": "Downsample Data\n\nhab_downsample |&gt;  ggplot(aes(x = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-downsampled-data",
    "href": "slides/30-imbalance-2.html#visualizing-downsampled-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Downsampled Data",
    "text": "Visualizing Downsampled Data\n\nhab_downsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-downsampled-data-no-jitter",
    "href": "slides/30-imbalance-2.html#visualizing-downsampled-data-no-jitter",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Downsampled Data: No Jitter",
    "text": "Visualizing Downsampled Data: No Jitter\n\nhab_downsample |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#performance-considerations",
    "href": "slides/30-imbalance-2.html#performance-considerations",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance considerations",
    "text": "Performance considerations\n\nPro:\n\nModel doesn’t over-align to noise in minority class\n\nCon:\n\nLose information from majority class"
  },
  {
    "objectID": "slides/30-imbalance-2.html#smote",
    "href": "slides/30-imbalance-2.html#smote",
    "title": "MATH 427: Class Imbalance",
    "section": "SMOTE",
    "text": "SMOTE\n\nBasic idea:\n\nBoth upsample minority and downsample majority (Tidymodel implementation only upsamples)\n\nBetter Upsampling: Instead of just randomly replicating minority observations\n\nFind (minority) nearest neighbors of each minority observation\nInterpolate line between them\nUpsample by randomly generating points in interpolated lines"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-data-2",
    "href": "slides/30-imbalance-2.html#visualizing-data-2",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing Data",
    "text": "Visualizing Data\n\nhab_train |&gt; ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#smote-recipe",
    "href": "slides/30-imbalance-2.html#smote-recipe",
    "title": "MATH 427: Class Imbalance",
    "section": "SMOTE Recipe",
    "text": "SMOTE Recipe\n\nsmote_recipe &lt;- recipe(Survival ~ ., data = hab_train) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_smote(Survival, over_ratio = 1, neighbors = 5)\n\nhab_smote &lt;- smote_recipe |&gt; prep(hab_train) |&gt; bake(new_data = NULL)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#smote-data",
    "href": "slides/30-imbalance-2.html#smote-data",
    "title": "MATH 427: Class Imbalance",
    "section": "SMOTE Data",
    "text": "SMOTE Data\n\nhab_smote |&gt;  ggplot(aes(x = Survival)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-smote-data",
    "href": "slides/30-imbalance-2.html#visualizing-smote-data",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing SMOTE Data",
    "text": "Visualizing SMOTE Data\n\nhab_smote |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#visualizing-smote-data-no-jitter",
    "href": "slides/30-imbalance-2.html#visualizing-smote-data-no-jitter",
    "title": "MATH 427: Class Imbalance",
    "section": "Visualizing SMOTE Data: No Jitter",
    "text": "Visualizing SMOTE Data: No Jitter\n\nhab_smote |&gt;  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()"
  },
  {
    "objectID": "slides/30-imbalance-2.html#performance-considerations-1",
    "href": "slides/30-imbalance-2.html#performance-considerations-1",
    "title": "MATH 427: Class Imbalance",
    "section": "Performance considerations",
    "text": "Performance considerations\n\nPro:\n\nModel doesn’t over-align (as much) to noise in minority class\nDon’t lose (as much) information from majority class\n\nCon:\n\nCreating new information out of nowhere"
  },
  {
    "objectID": "slides/30-imbalance-2.html#fitting-models",
    "href": "slides/30-imbalance-2.html#fitting-models",
    "title": "MATH 427: Class Imbalance",
    "section": "Fitting models",
    "text": "Fitting models\n\noversamp_fit &lt;- workflow() |&gt; add_recipe(upsample_recipe) |&gt; \n  add_model(lr_model) |&gt; fit(hab_train)\ndownsamp_fit &lt;- workflow() |&gt; add_recipe(downsample_recipe) |&gt; \n  add_model(lr_model)  |&gt; fit(hab_train)\nsmote_fit &lt;- workflow() |&gt; add_recipe(smote_recipe) |&gt; \n  add_model(lr_model)  |&gt; fit(hab_train)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#evaluate-performance",
    "href": "slides/30-imbalance-2.html#evaluate-performance",
    "title": "MATH 427: Class Imbalance",
    "section": "Evaluate Performance",
    "text": "Evaluate Performance\n\noversamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7343358\n\n\n\n\ndownsamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7243108\n\n\n\n\nsmote_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7251462"
  },
  {
    "objectID": "slides/30-imbalance-2.html#evaluate-performance-1",
    "href": "slides/30-imbalance-2.html#evaluate-performance-1",
    "title": "MATH 427: Class Imbalance",
    "section": "Evaluate Performance",
    "text": "Evaluate Performance\n\n\noversamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7692308\n\n\nprecision\nbinary\n0.5714286\n\n\nrecall\nbinary\n0.5714286\n\n\n\n\n\n\n\ndownsamp_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7435897\n\n\nprecision\nbinary\n0.5217391\n\n\nrecall\nbinary\n0.5714286\n\n\n\n\n\n\n\nsmote_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7435897\n\n\nprecision\nbinary\n0.5217391\n\n\nrecall\nbinary\n0.5714286"
  },
  {
    "objectID": "slides/30-imbalance-2.html#creating-importance-weights",
    "href": "slides/30-imbalance-2.html#creating-importance-weights",
    "title": "MATH 427: Class Imbalance",
    "section": "Creating Importance Weights",
    "text": "Creating Importance Weights\n\nlibrary(hardhat)\nhab_train &lt;- hab_train |&gt; \n  mutate(weights = ifelse(Survival == \"Died\", 4, 1),\n         weights = importance_weights(weights))\n\nhab_train |&gt; head() |&gt; kable()\n\n\n\n\nAge\nOpYear\nAxNodes\nSurvival\nweights\n\n\n\n\n34\n59\n0\nDied\n4\n\n\n34\n66\n9\nDied\n4\n\n\n38\n69\n21\nDied\n4\n\n\n39\n66\n0\nDied\n4\n\n\n41\n67\n0\nDied\n4\n\n\n42\n69\n1\nDied\n4"
  },
  {
    "objectID": "slides/30-imbalance-2.html#weighted-workflow",
    "href": "slides/30-imbalance-2.html#weighted-workflow",
    "title": "MATH 427: Class Imbalance",
    "section": "Weighted Workflow",
    "text": "Weighted Workflow\n\nweighted_wf &lt;- workflow() |&gt; \n  add_model(lr_model) |&gt; \n  add_recipe(recipe(Survival ~ ., data = hab_train)) |&gt; \n  add_case_weights(weights)\n\nweighted_fit &lt;- weighted_wf |&gt; \n  fit(hab_train)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#model-performance",
    "href": "slides/30-imbalance-2.html#model-performance",
    "title": "MATH 427: Class Imbalance",
    "section": "Model Performance",
    "text": "Model Performance\n\nweighted_fit |&gt; augment(new_data = hab_test) |&gt; \n  conf_mat(truth = Survival, estimate = .pred_class) |&gt; autoplot(\"heatmap\")"
  },
  {
    "objectID": "slides/30-imbalance-2.html#model-performance-1",
    "href": "slides/30-imbalance-2.html#model-performance-1",
    "title": "MATH 427: Class Imbalance",
    "section": "Model Performance",
    "text": "Model Performance\n\nweighted_fit |&gt; augment(new_data = hab_test) |&gt; \n  hab_metrics(truth = Survival, estimate = .pred_class) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.5256410\n\n\nprecision\nbinary\n0.3461538\n\n\nrecall\nbinary\n0.8571429\n\n\n\n\nweighted_fit |&gt; augment(new_data = hab_test) |&gt; \n  roc_auc(truth = Survival, .pred_Died) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.7142857"
  },
  {
    "objectID": "slides/30-imbalance-2.html#model-vs-decisions",
    "href": "slides/30-imbalance-2.html#model-vs-decisions",
    "title": "MATH 427: Class Imbalance",
    "section": "Model vs Decisions",
    "text": "Model vs Decisions\n\nHelpful framework for thinking about this:\n\nDivide model predictions from decisions\nUsually, model predicts a probability, then you make a classification based on that probability\nChoosing the best model probably means (1) calibrating your probabilities correctly, then (2) making classifications/decisions to optimze your use-case"
  },
  {
    "objectID": "slides/30-imbalance-2.html#scoring-rules",
    "href": "slides/30-imbalance-2.html#scoring-rules",
    "title": "MATH 427: Class Imbalance",
    "section": "Scoring Rules",
    "text": "Scoring Rules\n\nScoring rule: metric that evaluates probabilities\nNotation:\n\n\\(\\hat{p}_{ik}\\): predicted probability observation \\(i\\) is in class \\(k\\)\n\\(y_{ik}\\): 1 if observation \\(i\\) is in class \\(k\\), 0 otherwise\n\\(K\\): number of classes\n\\(N\\): number of observations\n\nBrier Score: think MSE for probabilities\n\nBinary: \\(\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{p}_i - y_i)^2\\)\nMulticlass: \\(\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} (\\hat{p}_{ik} - y_{ik})^2\\)\n\nLogorithmic Score:\n\nBinary: \\(-\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]\\)\nMulti-class: \\(-\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{p}_{ik})\\)"
  },
  {
    "objectID": "slides/30-imbalance-2.html#scoring-our-models",
    "href": "slides/30-imbalance-2.html#scoring-our-models",
    "title": "MATH 427: Class Imbalance",
    "section": "Scoring our models",
    "text": "Scoring our models\n\nhab_scores &lt;- metric_set(brier_class, mn_log_loss, roc_auc)\nall_scores &lt;- lr_fit |&gt; augment(new_data = hab_test) |&gt; hab_scores(truth = Survival, .pred_Died) |&gt; mutate(model = \"Logistic\") |&gt; \n  bind_rows(oversamp_fit |&gt; augment(new_data = hab_test) |&gt; hab_scores(truth = Survival, .pred_Died) |&gt; mutate(model = \"Oversample\")) |&gt; \n  bind_rows(downsamp_fit |&gt; augment(new_data = hab_test) |&gt; hab_scores(truth = Survival, .pred_Died) |&gt; mutate(model = \"Undersample\")) |&gt; \n  bind_rows(smote_fit |&gt; augment(new_data = hab_test) |&gt; hab_scores(truth = Survival, .pred_Died) |&gt; mutate(model = \"SMOTE\")) |&gt; \n  bind_rows(weighted_fit |&gt; augment(new_data = hab_test) |&gt; hab_scores(truth = Survival, .pred_Died) |&gt; mutate(model = \"Weighted\"))"
  },
  {
    "objectID": "slides/30-imbalance-2.html#scores",
    "href": "slides/30-imbalance-2.html#scores",
    "title": "MATH 427: Class Imbalance",
    "section": "Scores",
    "text": "Scores\n\nall_scores |&gt;\n  select(-.estimator) |&gt; \n  pivot_wider(names_from = .metric, values_from = .estimate) |&gt; \n  kable()\n\n\n\n\nmodel\nbrier_class\nmn_log_loss\nroc_auc\n\n\n\n\nLogistic\n0.1678374\n0.5161121\n0.7284879\n\n\nOversample\n0.2115716\n0.6217420\n0.7343358\n\n\nUndersample\n0.2130135\n0.6298389\n0.7243108\n\n\nSMOTE\n0.2169736\n0.6304324\n0.7251462\n\n\nWeighted\n0.2599917\n0.7217460\n0.7142857"
  },
  {
    "objectID": "slides/30-imbalance-2.html#conclusion",
    "href": "slides/30-imbalance-2.html#conclusion",
    "title": "MATH 427: Class Imbalance",
    "section": "Conclusion",
    "text": "Conclusion\n\nMany different approaches and strategies depending on data\nFirst strategy: tresholding\nMany times method depends on model algorithm\nMake sure to ask “Is imbalance really a problem here?”\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/31-project-1.html",
    "href": "slides/31-project-1.html",
    "title": "MATH 427: Project Work Day",
    "section": "",
    "text": "Monday’s Class: Explore data, clean data, choose a step to focus on, and get ready for modeling.\n\nComplete by Wednesday’s class\n\nWednesday’s Class: Build and analyze models.\n\nComplete by Friday’s class\n\nFriday’s class: Translating models into suggestions and presentations\n\nBe ready to present by Monday"
  },
  {
    "objectID": "slides/31-project-1.html#goals-for-this-week",
    "href": "slides/31-project-1.html#goals-for-this-week",
    "title": "MATH 427: Project Work Day",
    "section": "",
    "text": "Monday’s Class: Explore data, clean data, choose a step to focus on, and get ready for modeling.\n\nComplete by Wednesday’s class\n\nWednesday’s Class: Build and analyze models.\n\nComplete by Friday’s class\n\nFriday’s class: Translating models into suggestions and presentations\n\nBe ready to present by Monday"
  },
  {
    "objectID": "slides/31-project-1.html#class-notes-on-data",
    "href": "slides/31-project-1.html#class-notes-on-data",
    "title": "MATH 427: Project Work Day",
    "section": "Class Notes on Data",
    "text": "Class Notes on Data\nWhile each group might be focusing on a slightly different question, let’s approach this data set as a team. Whenever you notice something interesting about the data, or you have a cool idea about how to filter it, put that in this shared word document. You should all have edit access with your CofI credentials."
  },
  {
    "objectID": "slides/31-project-1.html#responses-from-brian",
    "href": "slides/31-project-1.html#responses-from-brian",
    "title": "MATH 427: Project Work Day",
    "section": "Responses from Brian",
    "text": "Responses from Brian\n\nWhy is the 2024 data set so much bigger than the other two? It has about ~74k rows vs. 16k and 17k for 2022 and 23.\n\n… we imported all of our prospects into Slate during the 2024 recruitment cycle, whereas in the past they were only imported if they became an inquiry. Since we knew that we were going to be switching strategic partners, we wanted to make sure we had all the names in our system just in case.\n\nI’m still having trouble understanding when a column gets a “Y” and when it gets a “N”… For example, it sounds like a student can start as a Prospect and then move over to an Inquiry at which point they get an “N” in the Prospect column and become a “Y” in the Inquiry column. Similar with “Applicant” and “Admit”. But a student can still have a “Y” in both “Inquiry” and “Admit” right? Perhaps that report will help things make sense?\n\nI think part of the challenge with the data is that it really is used to populate information from the Application stage forward My understanding is if there is a Y in a field that leads to it being counted in the corresponding place on the report. So yes, you can have a Y in inquiry and admit.\n\nOnce a deposit is made, do we track if the student later decides not to enroll, and is that information reflected in the dataset?\n\nYes, we do track this information. It would be indicated if the student only showed up in the deposit field and not the net deposit field.\n\nWhat is the difference between the ‘Deposit’ and ‘Net Deposits’ fields? How should we interpret cases where a student has deposited but is not included in the net deposits?\n\nDeposit means they submitted an enrollment deposit. A Net Deposit means that they matriculated at the College for that term.\n\nAre there cases where a student might be listed as an applicant without ever being an inquiry? If so, what does that imply about how they entered the process?\n\nYes. We talked about this in class. They are referred to as “stealth apps”, meaning that our first engagement with them was through an application they submitted to the College.\n\nDoes the data give any hints about whether an application is complete or if parts of it are missing, such as standardized test scores or GPA information?\n\nNo, this data-set does not list out the materials that a student may be missing.\n\nHow do we distinguish between a student who is simply a prospect because we purchased their information and one who might have shown some early signs of interest? Is there any way to tell from the data which prospects are more likely to be accurate?\n\nYou wouldn’t be able to do so in this particular data set, but we do have source code information that would indicate how the student entered our system. (ex. standardized test score name purchase, group visit to campus, high school visit, etc.)\n\nWould you prefer us to disregard factors like the significance of test scores in evaluating a student’s overall performance?\n\nThe College has found that a better indicator of student success is their high school GPA, so when we review test scores, a score can only help their candidacy, not hurt it. The exception would be with language proficiency scores, such as Duolingo, TOEFL, IELTS, etc.\n\nWhat criteria do you use to calculate GPA? Is a 4.0 GPA acceptable, and is it permissible to average them with different scales?\n\nWe evaluate GPA on a 4.0 scale. In the event a school uses percentages over GPA points, there is a formula we can use to convert it to a GPA.\n\nHow do you envision us handling international students, particularly UWCers, given the fluctuating percentages?\n\nI don’t have any particular preference. Perhaps comparing the yield rates of UWC graduates vs. non-UWC graduates or even from particular UWCs?\n\nHow did the COVID-19 situation impact the admission process? Did you introduce any specific features or variables to address certain indicators?\n\nThe biggest impact that the pandemic had was that we no longer factored test score into the scholarship matrix calculation. We already had information that GPA was a better predictor anyway, but given that it was harder for students to take standardized tests at that time, we eliminated the need completely.\n\nYou mentioned that for Idahoans cost is important, what other factors do you think are important for getting yotes?\n\nI think lack of understanding among Idaho families about the difference between four-year publics, two-year publics, and private institutions is a big factor. There also is a misunderstanding of what it means to be a Liberal Arts College. In a fairly conservative state like Idaho, some families/students may perceive the College to be extremely liberal and not welcoming to their political persuasion.\n\nWhat does the drops-dp/df column mean?\n\nThis means that a student who deposited either decided not to come or deferred their enrollment to a future term.\n\nIn academic interest, there’s Undecided but also NA, what is the difference?\n\nNA would mean that the field was left blank on the application, whereas a student may have listed Undecided as their choice.\n\nHow does your team handle missing data?\n\nNot exactly sure what you mean here, but in an application there are some key things that we must have in order to make an admission decision. As long as we have them, we can proceed."
  },
  {
    "objectID": "slides/11-cross-validation.html#announcement",
    "href": "slides/11-cross-validation.html#announcement",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Announcement",
    "text": "Announcement\nOn March 5th at 10am in JAAC, Kyle Mayer will be guest lecturing to talk about his role as a data analyst at Micron. Kyle has a Bachelors in Engineering and a Masters in Physics.\nLocation: Basement of the library."
  },
  {
    "objectID": "slides/11-cross-validation.html#computational-set-up",
    "href": "slides/11-cross-validation.html#computational-set-up",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/11-cross-validation.html#data-splitting-discussion",
    "href": "slides/11-cross-validation.html#data-splitting-discussion",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Data Splitting: Discussion",
    "text": "Data Splitting: Discussion\n\n\nWhy do we split our data before fitting a model?\n\nTo prevent overfitting\n\nWhat are some pitfalls we might encounter if we compare A LOT of models on the test set? (E.g. 100 different thresholds or \\(k = 1, 2, \\ldots, 100\\) neighbors)\n\nWe open ourselves up to information leakage\n\nPotential solution… three way split (training/validation/test)\nBetter solution… cross validation and resampling"
  },
  {
    "objectID": "slides/11-cross-validation.html#resampling",
    "href": "slides/11-cross-validation.html#resampling",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Resampling",
    "text": "Resampling\n\nFigure 10.1 from TMWR"
  },
  {
    "objectID": "slides/11-cross-validation.html#cross-validation-terminology",
    "href": "slides/11-cross-validation.html#cross-validation-terminology",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Cross-Validation Terminology",
    "text": "Cross-Validation Terminology\n\nDivide training set into two sets\n\nAnalysis set: fit the model (similar to training set)\nAssessment set: evaluation the model (similar to test set)"
  },
  {
    "objectID": "slides/11-cross-validation.html#k-fold-cross-validation-cv",
    "href": "slides/11-cross-validation.html#k-fold-cross-validation-cv",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "K-Fold Cross-Validation (CV)",
    "text": "K-Fold Cross-Validation (CV)\n\n\nPartition your data into \\(K\\) randomly selected non-overlapping “folds”\n\nFolds don’t overlap and every training observation is in one fold\nEach fold contained \\(1/K\\) of the training data\n\nLooping through the folds \\(k = 1, \\ldots, K\\):\n\nTreat fold \\(k\\) as the assessment set\nTreat all folds except for \\(k\\) as the analysis set\nFit model to analysis set (use whole modeling workflow)\nCompute error metrics on assessment set\n\nAfter loop, you will have \\(K\\) copies of each error metrics\nAverage them together to get performance estimate\nCan also look at distribution of performance metrics"
  },
  {
    "objectID": "slides/11-cross-validation.html#your-turn",
    "href": "slides/11-cross-validation.html#your-turn",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Your Turn!!!",
    "text": "Your Turn!!!\nSuppose you start with a data set with \\(n=1000\\) observations. You are trying to predict a numerical variable and your target metric is MSE.\n\nYou start with an initial 70/30 training/test split. How many observations are in your training and test set?\nYou then perform 10-Fold CV on the training set. How large is each analysis set and each assessment set and how many analysis and assessment sets do you have?\nDo you analysis set’s overlap? Do your assessment sets overlap?\nHow many times will you fit your model? How many different MSE’s will you have?"
  },
  {
    "objectID": "slides/11-cross-validation.html#data-ames-housing-prices",
    "href": "slides/11-cross-validation.html#data-ames-housing-prices",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Data: Ames Housing Prices",
    "text": "Data: Ames Housing Prices\nA data set from De Cock (2011) has 82 fields were recorded for 2,930 properties in Ames IA. This version is copies from the AmesHousing package but does not include a few quality columns that appear to be outcomes rather than predictors.\nGoal: Predict Sale_Price.\n\names |&gt; glimpse()\n\nRows: 2,930\nColumns: 74\n$ MS_SubClass        &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_1946…\n$ MS_Zoning          &lt;fct&gt; Residential_Low_Density, Residential_High_Density, …\n$ Lot_Frontage       &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,…\n$ Lot_Area           &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005…\n$ Street             &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav…\n$ Alley              &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access, …\n$ Lot_Shape          &lt;fct&gt; Slightly_Irregular, Regular, Slightly_Irregular, Re…\n$ Land_Contour       &lt;fct&gt; Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L…\n$ Utilities          &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All…\n$ Lot_Config         &lt;fct&gt; Corner, Inside, Corner, Corner, Inside, Inside, Ins…\n$ Land_Slope         &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G…\n$ Neighborhood       &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gil…\n$ Condition_1        &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No…\n$ Condition_2        &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor…\n$ Bldg_Type          &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn…\n$ House_Style        &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Sto…\n$ Overall_Cond       &lt;fct&gt; Average, Above_Average, Above_Average, Average, Ave…\n$ Year_Built         &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199…\n$ Year_Remod_Add     &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199…\n$ Roof_Style         &lt;fct&gt; Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G…\n$ Roof_Matl          &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompSh…\n$ Exterior_1st       &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Exterior_2nd       &lt;fct&gt; Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Mas_Vnr_Type       &lt;fct&gt; Stone, None, BrkFace, None, None, BrkFace, None, No…\n$ Mas_Vnr_Area       &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6…\n$ Exter_Cond         &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica…\n$ Foundation         &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc…\n$ Bsmt_Cond          &lt;fct&gt; Good, Typical, Typical, Typical, Typical, Typical, …\n$ Bsmt_Exposure      &lt;fct&gt; Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,…\n$ BsmtFin_Type_1     &lt;fct&gt; BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U…\n$ BsmtFin_SF_1       &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, …\n$ BsmtFin_Type_2     &lt;fct&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U…\n$ BsmtFin_SF_2       &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0…\n$ Bsmt_Unf_SF        &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,…\n$ Total_Bsmt_SF      &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, …\n$ Heating            &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas…\n$ Heating_QC         &lt;fct&gt; Fair, Typical, Typical, Excellent, Good, Excellent,…\n$ Central_Air        &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …\n$ Electrical         &lt;fct&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB…\n$ First_Flr_SF       &lt;int&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, …\n$ Second_Flr_SF      &lt;int&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,…\n$ Gr_Liv_Area        &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616…\n$ Bsmt_Full_Bath     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, …\n$ Bsmt_Half_Bath     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Full_Bath          &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, …\n$ Half_Bath          &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ Bedroom_AbvGr      &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, …\n$ Kitchen_AbvGr      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ TotRms_AbvGrd      &lt;int&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,…\n$ Functional         &lt;fct&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T…\n$ Fireplaces         &lt;int&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, …\n$ Garage_Type        &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att…\n$ Garage_Finish      &lt;fct&gt; Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F…\n$ Garage_Cars        &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, …\n$ Garage_Area        &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4…\n$ Garage_Cond        &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica…\n$ Paved_Drive        &lt;fct&gt; Partial_Pavement, Paved, Paved, Paved, Paved, Paved…\n$ Wood_Deck_SF       &lt;int&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48…\n$ Open_Porch_SF      &lt;int&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0…\n$ Enclosed_Porch     &lt;int&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Three_season_porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Screen_Porch       &lt;int&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, …\n$ Pool_Area          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Pool_QC            &lt;fct&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo…\n$ Fence              &lt;fct&gt; No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini…\n$ Misc_Feature       &lt;fct&gt; None, None, Gar2, None, None, None, None, None, Non…\n$ Misc_Val           &lt;int&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, …\n$ Mo_Sold            &lt;int&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, …\n$ Year_Sold          &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201…\n$ Sale_Type          &lt;fct&gt; WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W…\n$ Sale_Condition     &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Nor…\n$ Sale_Price         &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213…\n$ Longitude          &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638…\n$ Latitude           &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4…"
  },
  {
    "objectID": "slides/11-cross-validation.html#initial-data-split",
    "href": "slides/11-cross-validation.html#initial-data-split",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Initial Data Split",
    "text": "Initial Data Split\n\nset.seed(427)\n\ndata_split &lt;- initial_split(ames, strata = \"Sale_Price\")\names_train &lt;- training(data_split)\names_test  &lt;- testing(data_split)"
  },
  {
    "objectID": "slides/11-cross-validation.html#define-folds",
    "href": "slides/11-cross-validation.html#define-folds",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Define Folds",
    "text": "Define Folds\n\names_folds &lt;- vfold_cv(ames_train, v = 10)\names_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [1977/220]&gt; Fold01\n 2 &lt;split [1977/220]&gt; Fold02\n 3 &lt;split [1977/220]&gt; Fold03\n 4 &lt;split [1977/220]&gt; Fold04\n 5 &lt;split [1977/220]&gt; Fold05\n 6 &lt;split [1977/220]&gt; Fold06\n 7 &lt;split [1977/220]&gt; Fold07\n 8 &lt;split [1978/219]&gt; Fold08\n 9 &lt;split [1978/219]&gt; Fold09\n10 &lt;split [1978/219]&gt; Fold10"
  },
  {
    "objectID": "slides/11-cross-validation.html#define-models",
    "href": "slides/11-cross-validation.html#define-models",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Define Model(s)",
    "text": "Define Model(s)\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine('lm')\n\nknn5_model &lt;- nearest_neighbor(neighbors = 5) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")\n\nknn10_model &lt;- nearest_neighbor(neighbors = 10) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/11-cross-validation.html#define-preprocessing-linear-regression",
    "href": "slides/11-cross-validation.html#define-preprocessing-linear-regression",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Define Preprocessing: Linear regression",
    "text": "Define Preprocessing: Linear regression\n\nlm_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; # Convert categorical data into dummy variables\n  step_zv(all_predictors()) |&gt; # remove zero-variance predictors (i.e. predictors with one value)\n  step_corr(all_predictors(), threshold = 0.5) |&gt; # remove highly correlated predictors\n  step_lincomb(all_predictors()) # remove variables that have exact linear combinations"
  },
  {
    "objectID": "slides/11-cross-validation.html#define-preprocessing-knn",
    "href": "slides/11-cross-validation.html#define-preprocessing-knn",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Define Preprocessing: KNN",
    "text": "Define Preprocessing: KNN\n\nknn_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; # only uses ames_train for data types\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt; # Convert categorical data into dummy variables\n  step_zv(all_predictors()) |&gt; # remove zero-variance predictors (i.e. predictors with one value)\n  step_normalize(all_predictors())"
  },
  {
    "objectID": "slides/11-cross-validation.html#define-workflows",
    "href": "slides/11-cross-validation.html#define-workflows",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Define Workflows",
    "text": "Define Workflows\n\nlm_wf &lt;- workflow() |&gt; add_model(lm_model) |&gt; add_recipe(lm_preproc)\nknn5_wf &lt;- workflow() |&gt; add_model(knn5_model) |&gt; add_recipe(knn_preproc)\nknn10_wf &lt;- workflow() |&gt; add_model(knn10_model) |&gt; add_recipe(knn_preproc)"
  },
  {
    "objectID": "slides/11-cross-validation.html#define-metrics",
    "href": "slides/11-cross-validation.html#define-metrics",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Define Metrics",
    "text": "Define Metrics\n\names_metrics &lt;- metric_set(rmse, rsq)"
  },
  {
    "objectID": "slides/11-cross-validation.html#fit-and-assess-models",
    "href": "slides/11-cross-validation.html#fit-and-assess-models",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Fit and Assess Models",
    "text": "Fit and Assess Models\n\nlm_results &lt;- lm_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn5_results &lt;- knn5_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn10_results &lt;- knn10_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)"
  },
  {
    "objectID": "slides/11-cross-validation.html#what-does-this-create",
    "href": "slides/11-cross-validation.html#what-does-this-create",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "What does this create",
    "text": "What does this create\n\nlm_results\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics         .notes          \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [1977/220]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [1977/220]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [1977/220]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [1977/220]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [1977/220]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [1977/220]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [1977/220]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [1978/219]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [1978/219]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [1978/219]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "slides/11-cross-validation.html#collecting-metrics",
    "href": "slides/11-cross-validation.html#collecting-metrics",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Collecting Metrics",
    "text": "Collecting Metrics\n\ncollect_metrics(lm_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.214504e+04\n10\n1501.9928830\nPreprocessor1_Model1\n\n\nrsq\nstandard\n8.413166e-01\n10\n0.0070533\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn5_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n39821.01836\n10\n2031.0935396\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.75573\n10\n0.0200233\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn10_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.809082e+04\n10\n1842.876508\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.814575e-01\n10\n0.014159\nPreprocessor1_Model1"
  },
  {
    "objectID": "slides/11-cross-validation.html#questions",
    "href": "slides/11-cross-validation.html#questions",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Questions",
    "text": "Questions\n\nHow many times did we train a model?\nWhich model formulation was the best?"
  },
  {
    "objectID": "slides/11-cross-validation.html#final-model",
    "href": "slides/11-cross-validation.html#final-model",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Final Model",
    "text": "Final Model\n\nAfter choosing best model/workflow, fit on full training set and assess on test set\n\n\nfinal_fit &lt;- lm_wf |&gt; fit(data = ames_train)\nfinal_fit |&gt; \n  predict(new_data = ames_test) |&gt; \n  bind_cols(ames_test) |&gt; \n  ames_metrics(truth = Sale_Price, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard   39804.   \n2 rsq     standard       0.761"
  },
  {
    "objectID": "slides/11-cross-validation.html#leave-one-out-cross-validation-loocv",
    "href": "slides/11-cross-validation.html#leave-one-out-cross-validation-loocv",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\\(n\\)-Fold cross validation\nIterate through training set\n\nTreat observation \\(i\\) has assessment set\nTreat other \\(n-1\\) observations as analysis set\n\n\nSuppose you implement LOOCV on a dataset with \\(n=100\\) observations.\n\nWhat is the size (number of observations) of each analysis set?\nWhat is the size of each assessment set?\nHow many times must you fit a model to complete the overall LOOCV process?"
  },
  {
    "objectID": "slides/11-cross-validation.html#loocv-vs.-k-fold-cv",
    "href": "slides/11-cross-validation.html#loocv-vs.-k-fold-cv",
    "title": "MATH 427: Cross Validation and Resampling",
    "section": "LOOCV vs. K-Fold CV",
    "text": "LOOCV vs. K-Fold CV\n\nTypically we use 5-Fold or 10-Fold CV\n\\(K\\)-Fold is much less computationally expensive\n\\(K\\) actually gives better estimates of your test error… Why?\nBias-Variance Trade-Off\nLOOCV has the lowest level of bias but highest level of variance\n5- and 10-Fold CV have medium levels of bias but lower variance\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/07-classification-logistic.html#classification-problems",
    "href": "slides/07-classification-logistic.html#classification-problems",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Classification Problems",
    "text": "Classification Problems\n\nResponse \\(Y\\) is qualitative (categorical).\nObjective: build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\)\n\nassigns class label to a future unlabeled (unseen) observations\nunderstand the relationship between the predictors and response\n\nTwo ways to make predictions\n\nClass probabilities\nClass labels"
  },
  {
    "objectID": "slides/07-classification-logistic.html#default-dataset",
    "href": "slides/07-classification-logistic.html#default-dataset",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Default Dataset",
    "text": "Default Dataset\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\nlibrary(ISLR2)   # load library\nhead(Default) |&gt; kable()  # print first six observations\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n\n\nNo\nYes\n817.1804\n12106.135\n\n\nNo\nNo\n1073.5492\n31767.139\n\n\nNo\nNo\n529.2506\n35704.494\n\n\nNo\nNo\n785.6559\n38463.496\n\n\nNo\nYes\n919.5885\n7491.559\n\n\n\n\n\nWe will consider default as the response variable."
  },
  {
    "objectID": "slides/07-classification-logistic.html#split-the-data",
    "href": "slides/07-classification-logistic.html#split-the-data",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Split the data",
    "text": "Split the data\n\nset.seed(427)\n\ndefault_split &lt;- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n\n&lt;Training/Testing/Total&gt;\n&lt;6000/4000/10000&gt;\n\ndefault_train &lt;- training(default_split)\ndefault_test &lt;- testing(default_split)"
  },
  {
    "objectID": "slides/07-classification-logistic.html#summarizing-our-response-variable",
    "href": "slides/07-classification-logistic.html#summarizing-our-response-variable",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Summarizing our response variable",
    "text": "Summarizing our response variable\n\nlibrary(janitor)\nDefault |&gt; tabyl(default) |&gt; kable()  # class frequencies\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n9667\n0.9667\n\n\nYes\n333\n0.0333\n\n\n\n\ndefault_train |&gt; tabyl(default) |&gt; kable()\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n5796\n0.966\n\n\nYes\n204\n0.034\n\n\n\n\ndefault_test |&gt; tabyl(default) |&gt; kable()\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n3871\n0.96775\n\n\nYes\n129\n0.03225"
  },
  {
    "objectID": "slides/07-classification-logistic.html#data-types-in-r",
    "href": "slides/07-classification-logistic.html#data-types-in-r",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Data Types in R",
    "text": "Data Types in R\n\nDefault |&gt; glimpse()\n\nRows: 10,000\nColumns: 4\n$ default &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ student &lt;fct&gt; No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, N…\n$ balance &lt;dbl&gt; 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8…\n$ income  &lt;dbl&gt; 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55…\n\n\n\nfct = factor which is the data type you want to use for categorical data\nas_factor will typically transform things (including numbers) into factors for you\nchr can also be used but factors are better because they store all possible levels for your categorical data\nfactors are helpful for plotting because you can reorder the levels to help you plot things"
  },
  {
    "objectID": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier",
    "href": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier",
    "text": "K-Nearest Neighbors Classifier\n\nGiven a value for \\(K\\) and a test data point \\(x_0\\): \\[P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)\\] where \\(\\mathcal{N}_0\\) is the set of the \\(K\\) “closest” neighbors.\nFor classification: neighbors “vote” for class (unlike in regression where predictions are obtained by averaging) \\[P(Y=j | X=x_0)=\\text{Proportion of neighbors in class }j\\]"
  },
  {
    "objectID": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-build-model",
    "href": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-build-model",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Build Model",
    "text": "K-Nearest Neighbors Classifier: Build Model\n\nknn_default_fit &lt;- nearest_neighbor(neighbors = 10) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\") |&gt;\n  fit(default ~ balance, data = default_train)   # fit 10-nn model\n\n\nWhy don’t I need to worry about centering and scaling?"
  },
  {
    "objectID": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-predictions",
    "href": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-predictions",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Predictions",
    "text": "K-Nearest Neighbors Classifier: Predictions\n\npredict with a categorical response: documentation\nTwo different ways of making predictions"
  },
  {
    "objectID": "slides/07-classification-logistic.html#predicting-a-class",
    "href": "slides/07-classification-logistic.html#predicting-a-class",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Predicting a class",
    "text": "Predicting a class\n\nknn_class_preds &lt;- predict(knn_default_fit, new_data = default_test, type = \"class\")   # obtain default class label predictions\n\nknn_class_preds |&gt; head() |&gt; kable()\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo"
  },
  {
    "objectID": "slides/07-classification-logistic.html#predicting-a-probability",
    "href": "slides/07-classification-logistic.html#predicting-a-probability",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Predicting a probability",
    "text": "Predicting a probability\n\nCan anyone pick-out what’s wrong here? Hint: \\(k = 10\\)\n\n\nknn_prob_preds &lt;- predict(knn_default_fit, new_data = default_test, type = \"prob\")   # obtain predictions as probabilities\nknn_prob_preds |&gt; filter(.pred_No*.pred_Yes &gt;0) |&gt; head() |&gt; kable()\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.8685\n0.1315\n\n\n0.8685\n0.1315\n\n\n0.8685\n0.1315\n\n\n0.8505\n0.1495\n\n\n0.7105\n0.2895\n\n\n0.4775\n0.5225"
  },
  {
    "objectID": "slides/07-classification-logistic.html#ive-been-lying-to-you",
    "href": "slides/07-classification-logistic.html#ive-been-lying-to-you",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "I’ve been lying to you",
    "text": "I’ve been lying to you\n\nkknn actually takes a weighted average of the nearest neighbors\n\nI.e. closer observations get more weight\n\nTo use unweighted KNN need weight_func = \"rectangular\""
  },
  {
    "objectID": "slides/07-classification-logistic.html#unweighted-knn",
    "href": "slides/07-classification-logistic.html#unweighted-knn",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Unweighted KNN",
    "text": "Unweighted KNN\n\nknn_default_unw_fit &lt;- nearest_neighbor(neighbors = 10, weight_fun = \"rectangular\") |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\") |&gt;\n  fit(default ~ balance, data = default_train)   # fit 10-nn model\n\nknn_uw_prob_preds &lt;- predict(knn_default_unw_fit, new_data = default_test, type = \"prob\")   # obtain predictions as probabilities\nknn_uw_prob_preds |&gt; filter(.pred_No*.pred_Yes &gt;0) |&gt; head() |&gt; kable()\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.9\n0.1\n\n\n0.9\n0.1\n\n\n0.9\n0.1\n\n\n0.9\n0.1\n\n\n0.7\n0.3\n\n\n0.5\n0.5"
  },
  {
    "objectID": "slides/07-classification-logistic.html#why-not-linear-regression",
    "href": "slides/07-classification-logistic.html#why-not-linear-regression",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nDefault_lr &lt;- default_train |&gt; \n  mutate(default_0_1 = if_else(default == \"Yes\", 1, 0))\n\nlrfit &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(default_0_1 ~ balance, data = Default_lr)   # fit SLR\n\nlrfit |&gt; predict(new_data = default_train) |&gt; head() |&gt; kable()\n\n\n\n\n.pred\n\n\n\n\n0.0316011\n\n\n0.0655518\n\n\n-0.0065293\n\n\n0.0274263\n\n\n0.0451629\n\n\n0.0327046"
  },
  {
    "objectID": "slides/07-classification-logistic.html#why-not-linear-regression-1",
    "href": "slides/07-classification-logistic.html#why-not-linear-regression-1",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\n\nLinear regression: does not model probabilities well\n\nmight produce probabilities less than zero or bigger than one\ntreats increase from 0.41 to 0.5 as same as 0.01 to 0.1 (bad)"
  },
  {
    "objectID": "slides/07-classification-logistic.html#why-not-linear-regression-2",
    "href": "slides/07-classification-logistic.html#why-not-linear-regression-2",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\nSuppose we have a response, \\[Y=\\begin{cases}\n1 & \\text{if stroke} \\\\\n2 & \\text{if drug overdose} \\\\\n3  & \\text{if epileptic seizure}\n\\end{cases}\\]\n\nLinear regression suggests an ordering, and in fact implies that the differences between classes have meaning\n\ne.g. drug overdose \\(-\\) stroke \\(= 1\\)? 🤔"
  },
  {
    "objectID": "slides/07-classification-logistic.html#logistic-regression-1",
    "href": "slides/07-classification-logistic.html#logistic-regression-1",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nConsider a one-dimensional binary classification problem:\n\nTransform the linear model \\(\\beta_0 + \\beta_1 \\ X\\) so that the output is a probability\nUse logistic function: \\[g(t)=\\dfrac{e^t}{1+e^t} \\ \\ \\ \\text{for} \\ t \\in \\mathcal{R}\\]\nThen: \\[p(X)=P(Y=1|X)=g\\left(\\beta_0 + \\beta_1 \\ X\\right)=\\dfrac{e^{\\beta_0 + \\beta_1 \\ X}}{1+e^{\\beta_0 + \\beta_1 \\ X}}\\]"
  },
  {
    "objectID": "slides/07-classification-logistic.html#other-important-quantities",
    "href": "slides/07-classification-logistic.html#other-important-quantities",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Other important quantities",
    "text": "Other important quantities\n\nOdds: \\(\\dfrac{p(x)}{1-p(x)}\\)\nLog-Odds (logit): \\(\\log\\left(\\dfrac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1 \\ X\\)\n\nLinear function of predictors"
  },
  {
    "objectID": "slides/07-classification-logistic.html#logistic-regression-2",
    "href": "slides/07-classification-logistic.html#logistic-regression-2",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression"
  },
  {
    "objectID": "slides/07-classification-logistic.html#fitting-the-model",
    "href": "slides/07-classification-logistic.html#fitting-the-model",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Fitting the model",
    "text": "Fitting the model\nFitting a logistic regression model with default as the response and balance as the predictor:\n\nlogregfit &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |&gt; kable()  # obtain results\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6926385\n0.4659035\n-22.95033\n0\n\n\nbalance\n0.0055327\n0.0002841\n19.47329\n0"
  },
  {
    "objectID": "slides/07-classification-logistic.html#interpreting-coefficients",
    "href": "slides/07-classification-logistic.html#interpreting-coefficients",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\nAs \\(X\\) increases by 1, the log-odds increase by \\(\\hat{\\beta}_1\\)\n\nI.e. probability of default increases but NOT linearly\nChange in the probability of default due to a one-unit change in balance depends on the current balance value"
  },
  {
    "objectID": "slides/07-classification-logistic.html#interpreting-coefficients-1",
    "href": "slides/07-classification-logistic.html#interpreting-coefficients-1",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients"
  },
  {
    "objectID": "slides/07-classification-logistic.html#making-predictions-theory",
    "href": "slides/07-classification-logistic.html#making-predictions-theory",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Making predictions: Theory",
    "text": "Making predictions: Theory\nFor balance=$700,\n\n\n\\[\\hat{p}(X)=\\dfrac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}=\\dfrac{e^{-10.69 + (0.005533 \\times 700)}}{1+e^{-10.69 + (0.005533 \\times 700)}}=0.0011\\]\n\\[\\textbf{Odds}(X) = \\dfrac{\\hat{p}(X)}{1-\\hat{p}(X)} = \\dfrac{0.0011}{1-0.0011}\\approx 0.0011\\]\n\\[\\textbf{Log-Odds}(X)=\\log\\left(\\dfrac{\\hat{p}(X)}{1-\\hat{p}(X)}\\right) = \\log(0.0011) = -6.80\\]"
  },
  {
    "objectID": "slides/07-classification-logistic.html#making-predictions-in-r",
    "href": "slides/07-classification-logistic.html#making-predictions-in-r",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Making predictions in R",
    "text": "Making predictions in R\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |&gt; kable()   # obtain class predictions\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |&gt; kable()   # obtain log-odds predictions\n\n\n\n\nx\n\n\n\n\n-6.819727\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |&gt; kable()  # obtain probability predictions\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.9989092\n0.0010908\n\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/25-bagging.html#computational-set-up",
    "href": "slides/25-bagging.html#computational-set-up",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rpart.plot)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/25-bagging.html#ensemble-methods",
    "href": "slides/25-bagging.html#ensemble-methods",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Ensemble Methods",
    "text": "Ensemble Methods\n\nSingle regression or classification trees usually have poor predictive performance.\nEnsemble Methods: use a collection of models (in this case, decision trees) to improve the predictive performance\n\nDownside: Interpretability\n\nToday:\n\nBagging\nRandom Forests\nBoosting"
  },
  {
    "objectID": "slides/25-bagging.html#flexibility-vs.-interpretability",
    "href": "slides/25-bagging.html#flexibility-vs.-interpretability",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nAdapted from ISLR, James et al."
  },
  {
    "objectID": "slides/25-bagging.html#bagging",
    "href": "slides/25-bagging.html#bagging",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Bagging",
    "text": "Bagging\n\nBootstrap aggregation or bagging is a general-purpose procedure for reducing the variance of a statistical learning method.\nIdea: Build multiple trees and average their results.\nResult: Given a set of \\(n\\) independent observations (random variables) \\(Z_1, \\ldots, Z_n\\), each with variance \\(\\sigma^2\\), the variance of the mean/average \\(\\bar{Z} = \\displaystyle \\dfrac{Z_1 + Z_2 + \\cdots + Z_n}{n}\\) of the observations is \\(\\sigma^2/n\\).\n\nIn other words, averaging a set of observations reduces variance.\n\nIn reality, we do not have multiple training datasets."
  },
  {
    "objectID": "slides/25-bagging.html#bootstrapping",
    "href": "slides/25-bagging.html#bootstrapping",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nAdapted from ISLR, James et al."
  },
  {
    "objectID": "slides/25-bagging.html#bagging-1",
    "href": "slides/25-bagging.html#bagging-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Bagging",
    "text": "Bagging\n\nTake repeated bootstrap samples (say \\(B\\)) from the original dataset.\nBuild tree on each bootstrap sample and obtain predictions \\(\\hat{f}^{*b}(x), \\ b=1, 2, \\ldots, B\\).\nAverage all the predictions: \\[\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{i=1}^B\\hat{f}^{*b}(x)\\]\nTrees not pruned: They have high variance, but low bias.\nClassification: majority vote the overall prediction is the most commonly occurring class among the \\(B\\) predictions"
  },
  {
    "objectID": "slides/25-bagging.html#out-of-bag-error-estimation",
    "href": "slides/25-bagging.html#out-of-bag-error-estimation",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Out-of-Bag Error Estimation",
    "text": "Out-of-Bag Error Estimation\n\nBagging \\(\\Rightarrow\\) fitting lots of models \\(\\Rightarrow\\) computationally taxing\nBagging + Cross-validation \\(\\Rightarrow\\) EXTREMELY COMPUTATIONALLY TAXING\nOn average, each bagged tree (constructed on each bootstrap sample) makes use of around two-thirds of the observations.\nRemaining third observations referred to as out-of-bag (OOB) observations\nFor \\(i^{th}\\) observation, use the trees in which that observation was OOB. This will yield around \\(B/3\\) predictions for the \\(i^{th}\\) observation. Take their average to obtain a single prediction\nEquivalent to LOOCV if \\(B\\) is large"
  },
  {
    "objectID": "slides/25-bagging.html#variable-importance-measures",
    "href": "slides/25-bagging.html#variable-importance-measures",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Variable Importance Measures",
    "text": "Variable Importance Measures\n\nBagging improves prediction accuracy at expense of interpretability\nCan still obtain overall summary of importance of each predictor\n\nReduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated\nA single variable could be used multiple times in a tree\nTotal reduction in the loss function across all splits by a variable are summed up and used as the total feature importance\nA large value indicates an important predictor."
  },
  {
    "objectID": "slides/25-bagging.html#data-voter-frequency",
    "href": "slides/25-bagging.html#data-voter-frequency",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Data: Voter Frequency",
    "text": "Data: Voter Frequency\n\nInfo about data\nGoal: Identify individuals who are unlikely to vote to help organization target “get out the vote” effort.\n\n\nvoter_data &lt;- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nvoter_clean &lt;- voter_data |&gt; \n  select(-RespId, -weight, -Q1) |&gt;\n  mutate(\n    educ = factor(educ, levels = c(\"High school or less\", \"Some college\", \"College\")),\n    income_cat = factor(income_cat, levels = c(\"Less than $40k\", \"$40-75k \",\n                                               \"$75-125k\", \"$125k or more\")),\n    voter_category = factor(voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n  ) |&gt; \n  filter(Q22 != 5 | is.na(Q22)) |&gt; \n  mutate(Q22 = as_factor(Q22),\n         Q22 = if_else(is.na(Q22), \"Not Asked\", Q22),\n         across(Q28_1:Q28_8, ~if_else(.x == -1, 0, .x)),\n         across(Q28_1:Q28_8, ~ as_factor(.x)),\n         across(Q28_1:Q28_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n         across(Q29_1:Q29_10, ~if_else(.x == -1, 0, .x)),\n         across(Q29_1:Q29_10, ~ as_factor(.x)),\n         across(Q29_1:Q29_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n        Party_ID = as_factor(case_when(\n          Q31 == 1 ~ \"Strong Republican\",\n          Q31 == 2 ~ \"Republican\",\n          Q32 == 1  ~ \"Strong Democrat\",\n          Q32 == 2 ~ \"Democrat\",\n          Q33 == 1 ~ \"Lean Republican\",\n          Q33 == 2 ~ \"Lean Democrat\",\n          TRUE ~ \"Other\"\n        )),\n        Party_ID = factor(Party_ID, levels =c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n                                                \"Other\", \"Lean Democrat\", \"Democrat\", \"Strong Democrat\")),\n        across(!ppage, ~as_factor(if_else(.x == -1, NA, .x))))"
  },
  {
    "objectID": "slides/25-bagging.html#split-data",
    "href": "slides/25-bagging.html#split-data",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\n\nvoter_splits &lt;- initial_split(voter_clean, prop = 0.7, strata = voter_category)\nvoter_train &lt;- training(voter_splits)\nvoter_test &lt;- testing(voter_splits)"
  },
  {
    "objectID": "slides/25-bagging.html#define-model",
    "href": "slides/25-bagging.html#define-model",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Define Model",
    "text": "Define Model\n\ntrees: kind of a tuning parameter… want to select value that is large enough but doesn’t matter past that\n\n\nbagged_model &lt;- rand_forest(trees = 500, mtry = .cols()) |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"classification\")"
  },
  {
    "objectID": "slides/25-bagging.html#define-recipe",
    "href": "slides/25-bagging.html#define-recipe",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Define Recipe",
    "text": "Define Recipe\n\nbag_recipe &lt;- recipe(voter_category ~ . , data = voter_train) |&gt; \n  step_indicate_na(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt; \n  step_dummy(all_nominal_predictors(), one_hot = TRUE)"
  },
  {
    "objectID": "slides/25-bagging.html#define-workflow-and-fit",
    "href": "slides/25-bagging.html#define-workflow-and-fit",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Define Workflow and Fit",
    "text": "Define Workflow and Fit\n\n# need to install ranger\nbag_wf &lt;- workflow() |&gt; \n  add_model(bagged_model) |&gt; \n  add_recipe(bag_recipe)\n\nbagged_fit &lt;- bag_wf |&gt; \n  fit(voter_train)"
  },
  {
    "objectID": "slides/25-bagging.html#evaluate-performance",
    "href": "slides/25-bagging.html#evaluate-performance",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Evaluate performance",
    "text": "Evaluate performance\n\naugment(bagged_fit, new_data = voter_test) |&gt; \n  accuracy(truth = voter_category, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.648"
  },
  {
    "objectID": "slides/25-bagging.html#variable-importance",
    "href": "slides/25-bagging.html#variable-importance",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Variable Importance",
    "text": "Variable Importance\n\nlibrary(vip)\nvip(bagged_fit)"
  },
  {
    "objectID": "slides/25-bagging.html#bagging-disadvantages",
    "href": "slides/25-bagging.html#bagging-disadvantages",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Bagging: Disadvantages",
    "text": "Bagging: Disadvantages\n\nimproves prediction performance but reduces interpretability\ntrees in bagging not completely independent of each other since all the original features are considered at every split of every tree\ntree correlation: trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree)\n\nprevents bagging from further reducing the variance of the individual models\n\nRandom forests extend and improve upon bagged decision trees by reducing this correlation and thereby improving the accuracy of the overall ensemble."
  },
  {
    "objectID": "slides/25-bagging.html#bagging-disadvantages-1",
    "href": "slides/25-bagging.html#bagging-disadvantages-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Bagging: Disadvantages",
    "text": "Bagging: Disadvantages\n\nAdapted from Hands-On Machine Learning, Boehmke & Greenwell"
  },
  {
    "objectID": "slides/25-bagging.html#random-forests-1",
    "href": "slides/25-bagging.html#random-forests-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Random Forests",
    "text": "Random Forests\n\nDe-correlates bagged trees \\(\\Rightarrow\\) reducing variance\nAs in bagging, we build a number of decision trees on bootstrapped training samples.\nAlgorithm: do the following to build each tree\n\nGenerate bootstrapped sample of training data\nRandomly select \\(m\\) predictors\nPick best variable/split-oint from these \\(m\\)\nSplit node into two child nodes\nStop when typical stopped criteria is hit (not pruning)\n\nNote: A fresh sample of \\(m\\) predictors is taken at each split\nTypically \\(m = p/3\\) for regression and \\(m = \\sqrt{p}\\) for classification but this should be considered a tuning parameter"
  },
  {
    "objectID": "slides/25-bagging.html#define-model-1",
    "href": "slides/25-bagging.html#define-model-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Define Model",
    "text": "Define Model\n\ntrees: kind of a tuning parameter… want to select value that is large enough but doesn’t matter past that\n\n\nrf_model &lt;- rand_forest(trees = 100, mtry = tune()) |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"classification\")"
  },
  {
    "objectID": "slides/25-bagging.html#define-recipe-1",
    "href": "slides/25-bagging.html#define-recipe-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Define Recipe",
    "text": "Define Recipe\n\nrf_recipe &lt;- recipe(voter_category ~ . , data = voter_train) |&gt; \n  step_indicate_na(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |&gt; \n  step_impute_median(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt; \n  step_dummy(all_nominal_predictors(), one_hot = TRUE)"
  },
  {
    "objectID": "slides/25-bagging.html#define-workflow-and-fit-1",
    "href": "slides/25-bagging.html#define-workflow-and-fit-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Define Workflow and Fit",
    "text": "Define Workflow and Fit\n\n# need to install ranger\nrf_wf &lt;- workflow() |&gt; \n  add_model(rf_model) |&gt; \n  add_recipe(rf_recipe)"
  },
  {
    "objectID": "slides/25-bagging.html#upper-limit-for-mtry",
    "href": "slides/25-bagging.html#upper-limit-for-mtry",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Upper Limit for mtry",
    "text": "Upper Limit for mtry\n\nrf_model |&gt; extract_parameter_set_dials()\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[?]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information."
  },
  {
    "objectID": "slides/25-bagging.html#extract-number-of-features",
    "href": "slides/25-bagging.html#extract-number-of-features",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Extract Number of Features",
    "text": "Extract Number of Features\n\nrf_param &lt;- rf_model |&gt; extract_parameter_set_dials() |&gt; finalize(voter_train)\nrf_param\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[+]"
  },
  {
    "objectID": "slides/25-bagging.html#tune-mtry",
    "href": "slides/25-bagging.html#tune-mtry",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Tune mtry",
    "text": "Tune mtry\n\nvoter_folds &lt;- vfold_cv(voter_train, v = 5, repeats = 10, strata = voter_category)\n\nmtry_grid &lt;- grid_regular(rf_param, levels = 10)\n\ntuning_results &lt;- tune_grid(\n  rf_wf,\n  resamples= voter_folds,\n  grid = mtry_grid\n)"
  },
  {
    "objectID": "slides/25-bagging.html#results",
    "href": "slides/25-bagging.html#results",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Results",
    "text": "Results\n\nautoplot(tuning_results)"
  },
  {
    "objectID": "slides/25-bagging.html#evaluate-performance-1",
    "href": "slides/25-bagging.html#evaluate-performance-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Evaluate performance",
    "text": "Evaluate performance\n\nrf_fit &lt;-  rf_wf |&gt; \n  finalize_workflow(select_best(tuning_results)) |&gt; \n  fit(voter_train)\n\naugment(rf_fit, new_data = voter_test) |&gt; \n  accuracy(truth = voter_category, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.658"
  },
  {
    "objectID": "slides/25-bagging.html#variable-importance-1",
    "href": "slides/25-bagging.html#variable-importance-1",
    "title": "MATH 427: Bagging and Boosting",
    "section": "Variable Importance",
    "text": "Variable Importance\n\nlibrary(vip)\nvip(rf_fit)\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/14-CoverLetter-Resume-PeerReview.html#peer-review",
    "href": "slides/14-CoverLetter-Resume-PeerReview.html#peer-review",
    "title": "MATH 427: Cover Letter & Resume Peer-Review",
    "section": "Peer Review",
    "text": "Peer Review\nYou will be placed in groups of two or three:\n\nSpend about 10-15 minutes reviewing each of our partner’s documents\n\nPoint out anything that is unclear\nPoint out any typos or grammatical errors. If there are A LOT, then just say “PROOF READ”\nFill out the rubric you have\nPretend to be an employer: Write down a few questions you have and would want to learn more about based on the documents you have.\n\nAfter your group is done reviewing, discuss each persons document as a group.\n\nTalk about strengths\nTalk about weaknesses\nBE KIND AND SUPPORTIVE!\n\nIf you’re getting feedback, take it graciously!\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/26-boosting.html#computational-set-up",
    "href": "slides/26-boosting.html#computational-set-up",
    "title": "MATH 427: Boosting",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rpart.plot)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/26-boosting.html#exploring-bagging-using-app",
    "href": "slides/26-boosting.html#exploring-bagging-using-app",
    "title": "MATH 427: Boosting",
    "section": "Exploring Bagging Using App",
    "text": "Exploring Bagging Using App\n\nApp"
  },
  {
    "objectID": "slides/26-boosting.html#ensemble-methods",
    "href": "slides/26-boosting.html#ensemble-methods",
    "title": "MATH 427: Boosting",
    "section": "Ensemble Methods",
    "text": "Ensemble Methods\n\nSingle regression or classification trees usually have poor predictive performance.\nEnsemble Methods: use a collection of models (in this case, decision trees) to improve the predictive performance\n\nDownside: Interpretability\n\nLast Time:\n\nBagging\nRandom Forests\n\nThis Time:\n\nBoosting"
  },
  {
    "objectID": "slides/26-boosting.html#flexibility-vs.-interpretability",
    "href": "slides/26-boosting.html#flexibility-vs.-interpretability",
    "title": "MATH 427: Boosting",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nAdapted from ISLR, James et al."
  },
  {
    "objectID": "slides/26-boosting.html#boosting-1",
    "href": "slides/26-boosting.html#boosting-1",
    "title": "MATH 427: Boosting",
    "section": "Boosting",
    "text": "Boosting\n\nStill an ensemble method in that it creates a bunch of models (trees in this case)\nTrees are created sequentially, and fit to the residuals of the previous trees\nIdea: each additional tree to improve upon the previous trees by focusing on places where the previous trees perform poorly\nEach model in the process is a weak model, referred to as a base learner\nlearning slowly: as more trees are fit, the overall ensemble gradually improves\n\n\n\nRemind me: What is a residual?"
  },
  {
    "objectID": "slides/26-boosting.html#boosting-algorithm",
    "href": "slides/26-boosting.html#boosting-algorithm",
    "title": "MATH 427: Boosting",
    "section": "Boosting Algorithm",
    "text": "Boosting Algorithm\nLet \\(B\\) be the number of trees you want to fit and \\(d\\) the maximum number of splits\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\) for all training set\nFor \\(b = 1,\\ldots, B\\):\n\nFit tree \\(\\hat{f}_b\\) with \\(d\\) splits (\\(d+1\\) leaves) using psuedo-residuals \\(r\\) as response and features \\(X\\) as predictors.\nUpdate full model \\(\\hat{f}\\): \\(\\hat{f} = \\hat{f} + \\lambda\\hat{f}_b\\)\nUpdate pseudo-residuals: \\(r_i = r_i - \\lambda\\hat{f}_b(x_i)\\)\n\nOutput final model: \\[\\hat{f}(x) = \\displaystyle \\sum_{b=1}^{B} \\lambda \\ \\hat{f}_b(x)\\]\n\n\nIt may be hard to see, but this is similar to gradient descent and so is called gradient boosting"
  },
  {
    "objectID": "slides/26-boosting.html#gradient-boosting",
    "href": "slides/26-boosting.html#gradient-boosting",
    "title": "MATH 427: Boosting",
    "section": "Gradient Boosting",
    "text": "Gradient Boosting"
  },
  {
    "objectID": "slides/26-boosting.html#many-implementations-of-boosting",
    "href": "slides/26-boosting.html#many-implementations-of-boosting",
    "title": "MATH 427: Boosting",
    "section": "Many Implementations of Boosting",
    "text": "Many Implementations of Boosting\n\nAny implementation of boosting you use will like be a tweak of this\n\nAdaBoost\nCatboost\nLightGBM\nXGBoost"
  },
  {
    "objectID": "slides/26-boosting.html#gradient-boosting-tuning-parameters",
    "href": "slides/26-boosting.html#gradient-boosting-tuning-parameters",
    "title": "MATH 427: Boosting",
    "section": "Gradient Boosting Tuning Parameters",
    "text": "Gradient Boosting Tuning Parameters\n\nDocumentation"
  },
  {
    "objectID": "slides/26-boosting.html#data-voter-frequency",
    "href": "slides/26-boosting.html#data-voter-frequency",
    "title": "MATH 427: Boosting",
    "section": "Data: Voter Frequency",
    "text": "Data: Voter Frequency\n\nInfo about data\nGoal: Identify individuals who are unlikely to vote to help organization target “get out the vote” effort.\n\n\nvoter_data &lt;- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nvoter_clean &lt;- voter_data |&gt; \n  select(-RespId, -weight, -Q1) |&gt;\n  mutate(\n    educ = factor(educ, levels = c(\"High school or less\", \"Some college\", \"College\")),\n    income_cat = factor(income_cat, levels = c(\"Less than $40k\", \"$40-75k \",\n                                               \"$75-125k\", \"$125k or more\")),\n    voter_category = factor(voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n  ) |&gt; \n  filter(Q22 != 5 | is.na(Q22)) |&gt; \n  mutate(Q22 = as_factor(Q22),\n         Q22 = if_else(is.na(Q22), \"Not Asked\", Q22),\n         across(Q28_1:Q28_8, ~if_else(.x == -1, 0, .x)),\n         across(Q28_1:Q28_8, ~ as_factor(.x)),\n         across(Q28_1:Q28_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n         across(Q29_1:Q29_10, ~if_else(.x == -1, 0, .x)),\n         across(Q29_1:Q29_10, ~ as_factor(.x)),\n         across(Q29_1:Q29_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n        Party_ID = as_factor(case_when(\n          Q31 == 1 ~ \"Strong Republican\",\n          Q31 == 2 ~ \"Republican\",\n          Q32 == 1  ~ \"Strong Democrat\",\n          Q32 == 2 ~ \"Democrat\",\n          Q33 == 1 ~ \"Lean Republican\",\n          Q33 == 2 ~ \"Lean Democrat\",\n          TRUE ~ \"Other\"\n        )),\n        Party_ID = factor(Party_ID, levels =c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n                                                \"Other\", \"Lean Democrat\", \"Democrat\", \"Strong Democrat\")),\n        across(!ppage, ~as_factor(if_else(.x == -1, NA, .x))))"
  },
  {
    "objectID": "slides/26-boosting.html#split-data",
    "href": "slides/26-boosting.html#split-data",
    "title": "MATH 427: Boosting",
    "section": "Split Data",
    "text": "Split Data\n\nset.seed(427)\n\nvoter_splits &lt;- initial_split(voter_clean, prop = 0.7, strata = voter_category)\nvoter_train &lt;- training(voter_splits)\nvoter_test &lt;- testing(voter_splits)"
  },
  {
    "objectID": "slides/26-boosting.html#define-model",
    "href": "slides/26-boosting.html#define-model",
    "title": "MATH 427: Boosting",
    "section": "Define Model",
    "text": "Define Model\n\ntrees: VERY MUCH A TUNING PARAMETER NOW!\n\n\ngbm_model &lt;- boost_tree(trees = tune(), learn_rate = tune(), tree_depth = tune(),\n                        min_n = tune()) |&gt;\n  set_engine(\"xgboost\") |&gt; # dont need importance\n  set_mode(\"classification\")"
  },
  {
    "objectID": "slides/26-boosting.html#define-recipe",
    "href": "slides/26-boosting.html#define-recipe",
    "title": "MATH 427: Boosting",
    "section": "Define Recipe",
    "text": "Define Recipe\n\ngbm_recipe &lt;- recipe(voter_category ~ . , data = voter_train) |&gt;\n  step_indicate_na(all_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |&gt;\n  step_impute_median(all_numeric_predictors()) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)"
  },
  {
    "objectID": "slides/26-boosting.html#define-workflow-and-fit",
    "href": "slides/26-boosting.html#define-workflow-and-fit",
    "title": "MATH 427: Boosting",
    "section": "Define Workflow and Fit",
    "text": "Define Workflow and Fit\n\ngbm_wf &lt;- workflow() |&gt;\n  add_model(gbm_model) |&gt;\n  add_recipe(gbm_recipe)"
  },
  {
    "objectID": "slides/26-boosting.html#creating-hyperparameter-grid",
    "href": "slides/26-boosting.html#creating-hyperparameter-grid",
    "title": "MATH 427: Boosting",
    "section": "Creating Hyperparameter Grid",
    "text": "Creating Hyperparameter Grid\n\ngbm_grid &lt;- grid_latin_hypercube(trees(range = c(20, 200)),\n                                  tree_depth(range = c(1, 15)),\n                                  learn_rate(range = c(-10,-1)),\n                                  min_n(range = c(2, 40)), size = 50)\nvoter_folds = vfold_cv(voter_train, v = 5, repeats = 10, strata = voter_category)"
  },
  {
    "objectID": "slides/26-boosting.html#tuning-hyperparameters-in-parallel",
    "href": "slides/26-boosting.html#tuning-hyperparameters-in-parallel",
    "title": "MATH 427: Boosting",
    "section": "Tuning Hyperparameters in Parallel",
    "text": "Tuning Hyperparameters in Parallel\n\nCheck out Chapter 10.4 of TMWR\n\nImplementation depends on operating system\n\n\n\nlibrary(doParallel)\n\ncl &lt;- makePSOCKcluster(9)\nregisterDoParallel(cl)\n\ntuning_results &lt;- tune_grid(\n  gbm_wf,\n  resamples= voter_folds,\n  grid = gbm_grid\n)\n\nstopCluster(cl)"
  },
  {
    "objectID": "slides/26-boosting.html#results",
    "href": "slides/26-boosting.html#results",
    "title": "MATH 427: Boosting",
    "section": "Results",
    "text": "Results\n\nautoplot(tuning_results)"
  },
  {
    "objectID": "slides/26-boosting.html#which-one-was-best",
    "href": "slides/26-boosting.html#which-one-was-best",
    "title": "MATH 427: Boosting",
    "section": "Which one was best?",
    "text": "Which one was best?\n\nselect_best(tuning_results, metric = \"accuracy\") |&gt; kable()\n\n\n\n\ntrees\nmin_n\ntree_depth\nlearn_rate\n.config\n\n\n\n\n107\n25\n7\n0.057429\nPreprocessor1_Model24"
  },
  {
    "objectID": "slides/26-boosting.html#evaluate-performance",
    "href": "slides/26-boosting.html#evaluate-performance",
    "title": "MATH 427: Boosting",
    "section": "Evaluate performance",
    "text": "Evaluate performance\n\ngbm_fit &lt;-  gbm_wf |&gt;\n  finalize_workflow(select_best(tuning_results, metric = \"accuracy\")) |&gt;\n  fit(voter_train)\n\naugment(gbm_fit, new_data = voter_test) |&gt;\n  accuracy(truth = voter_category, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.653"
  },
  {
    "objectID": "slides/26-boosting.html#variable-importance",
    "href": "slides/26-boosting.html#variable-importance",
    "title": "MATH 427: Boosting",
    "section": "Variable Importance",
    "text": "Variable Importance\n\nlibrary(vip)\nvip(gbm_fit)\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#computational-setup",
    "href": "slides/04-MultipleRegression.html#computational-setup",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Computational Setup",
    "text": "Computational Setup\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(readODS)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#question",
    "href": "slides/04-MultipleRegression.html#question",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Question!!!",
    "text": "Question!!!\nAs a model’s flexibility increases:\n\nQuestionsAnswers\n\n\n\nits variance (increases/decreases)\nits bias (increases/decreases)\nits training MSE (increases/decreases)\nits test MSE (describe)\n\n\n\n\nits variance (increases)\nits bias (decreases)\nits training MSE (decreases)\nits test MSE (decreases at first, then increases and the model starts to overfit, U-shaped)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#outlet-data",
    "href": "slides/04-MultipleRegression.html#outlet-data",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Outlet Data",
    "text": "Outlet Data\nSuppose the CEO of a restaurant franchise is considering opening new outlets in different cities. They would like to expand their business to cities that give them higher profits with the assumption that highly populated cities will probably yield higher profits.\nThey have data on the population (in 100,000) and profit (in $1,000) at 97 cities where they currently have outlets.\n\noutlets &lt;- readRDS(\"../data/outlets.rds\")   # load dataset\nhead(outlets) |&gt; kable() # first six observations of the dataset\n\n\n\n\npopulation\nprofit\n\n\n\n\n6.1101\n17.5920\n\n\n5.5277\n9.1302\n\n\n8.5186\n13.6620\n\n\n7.0032\n11.8540\n\n\n5.8598\n6.8233\n\n\n8.3829\n11.8860"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#r-as-an-open-source-language",
    "href": "slides/04-MultipleRegression.html#r-as-an-open-source-language",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "R as an open-source language",
    "text": "R as an open-source language\n\nR is an open source language\n\nAdvantages:\n\nPackages for almost anything you want\n“Cutting edge” methods rolled out quickly and early\n\nDisadvantages\n\nMany packages (especially new ones) may have bugs\nLots of syntactical diversity\nSyntax is frequently dependent on the needs of the person who wrote the package and conventions at the time the package was created"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#enter-tidyverse",
    "href": "slides/04-MultipleRegression.html#enter-tidyverse",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Enter tidyverse",
    "text": "Enter tidyverse\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\n\ntidyverse is for manipulating and visualizing data\nthe tidyverse is a meta-package meaning it is a collection of a bunch of other packages"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#enter-tidymodels",
    "href": "slides/04-MultipleRegression.html#enter-tidymodels",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Enter tidymodels",
    "text": "Enter tidymodels\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\ntidymodels creates a unified framework for building models in R\nEric’s opinion: similar idea to scikit-learn in Python"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#back-to-linear-regression",
    "href": "slides/04-MultipleRegression.html#back-to-linear-regression",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Back to linear regression",
    "text": "Back to linear regression\n\nggplot(data = outlets) +\n  geom_point(mapping = aes(x = population, y = profit)) +    # create scatterplot\n  geom_smooth(mapping = aes(x = population, y = profit), \n              method = \"lm\", se = FALSE)   # add the regression line"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#classic-linear-regression-in-r",
    "href": "slides/04-MultipleRegression.html#classic-linear-regression-in-r",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Classic Linear Regression in R",
    "text": "Classic Linear Regression in R\n\n\n\noutlets_model &lt;- lm(profit ~ population, data = outlets)\noutlets_model\n\n\nCall:\nlm(formula = profit ~ population, data = outlets)\n\nCoefficients:\n(Intercept)   population  \n     -3.896        1.193  \n\n\n\nThis corresponds to the model:\n\\[\n\\begin{aligned}\n\\text{Profit}  &= -3.90 + 1.19\\times\\text{Population}\\\\\n\\hat{Y}_i &= -3.90 + 1.19X_i\n\\end{aligned}\n\\] i.e. \\(\\hat{\\beta}_0 = -3.90\\) and \\(\\hat{\\beta}_1 = 1.19\\)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#modeling-with-tidymodels",
    "href": "slides/04-MultipleRegression.html#modeling-with-tidymodels",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Modeling with tidymodels",
    "text": "Modeling with tidymodels\n\nSpecify mathematical structure of model (e.g. linear regression, logistic regression)\nSpecify the engine for fitting the model. (e.g. lm, stan, glmnet).\nWhen required, declare the mode of the model (i.e. regression or classification)."
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels",
    "href": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression with tidymodels",
    "text": "Linear Regression with tidymodels\n\n# Usually put these at the top\nlibrary(tidymodels) # load tidymodels package\ntidymodels_prefer() # avoid common conflicts\n\nlm_model &lt;- linear_reg() |&gt; # Step 1\n  set_engine(\"lm\") # Step 2\n\n# Step 3 not required since linear regression can't be used for classification\n\n# Fit the model\nlm_model_fit &lt;- lm_model |&gt; \n  fit(profit ~ population, data = outlets)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels-1",
    "href": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression with tidymodels",
    "text": "Linear Regression with tidymodels\n\nlm_model_fit |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.895781\n0.7194828\n-5.414696\n5e-07\n\n\npopulation\n1.193034\n0.0797439\n14.960806\n0e+00\n\n\n\n\n\nSame model as before:\n\\[\n\\begin{aligned}\n\\text{Profit}  &= -3.90 + 1.19\\times\\text{Population}\\\\\n\\hat{Y}_i &= -3.90 + 1.19X_i\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-in-r-prediction",
    "href": "slides/04-MultipleRegression.html#linear-regression-in-r-prediction",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression in R: Prediction",
    "text": "Linear Regression in R: Prediction\n\nnew_cities &lt;- tibble(population = rnorm(100, 7, 3))\n\nlm_model_fit |&gt; \npredict(new_data = new_cities) |&gt; \n  kable()\n\n\n\n\n.pred\n\n\n\n\n4.2987846\n\n\n4.3494028\n\n\n1.6358812\n\n\n2.8371943\n\n\n8.6964415\n\n\n1.2610114\n\n\n4.4093826\n\n\n7.0132467\n\n\n8.9081681\n\n\n-3.7261162\n\n\n-0.3152552\n\n\n-0.0578742\n\n\n3.1103344\n\n\n-1.4511631\n\n\n3.2056127\n\n\n7.2094157\n\n\n5.2694317\n\n\n9.8741950\n\n\n5.3986612\n\n\n5.7894116\n\n\n6.5817409\n\n\n6.4489003\n\n\n1.5635853\n\n\n-5.8145557\n\n\n0.3096059\n\n\n4.7217335\n\n\n8.0782299\n\n\n6.4065191\n\n\n7.0495725\n\n\n10.5119132\n\n\n3.2281542\n\n\n5.3756384\n\n\n10.5424872\n\n\n2.6444235\n\n\n3.1483294\n\n\n5.7659904\n\n\n3.3551593\n\n\n-1.1774409\n\n\n9.5361487\n\n\n-0.3336581\n\n\n4.1063193\n\n\n2.7105932\n\n\n6.1253040\n\n\n3.6662178\n\n\n6.1309169\n\n\n9.2088409\n\n\n-2.1660582\n\n\n4.9800410\n\n\n5.5140171\n\n\n3.3714057\n\n\n5.0758817\n\n\n2.9767839\n\n\n4.4891785\n\n\n-2.2982990\n\n\n-2.0242922\n\n\n4.4403419\n\n\n4.0335077\n\n\n6.8410161\n\n\n3.6835670\n\n\n-0.6670421\n\n\n6.1270222\n\n\n7.1611647\n\n\n3.2563676\n\n\n11.3135720\n\n\n-0.8720586\n\n\n7.4979481\n\n\n5.4139803\n\n\n7.3006650\n\n\n3.3803037\n\n\n5.1760916\n\n\n4.7255493\n\n\n5.3377241\n\n\n8.0237716\n\n\n5.0616701\n\n\n-1.7427836\n\n\n5.5563838\n\n\n9.1823534\n\n\n3.3094120\n\n\n4.7223157\n\n\n3.3083360\n\n\n4.0882384\n\n\n-0.3834378\n\n\n0.9586050\n\n\n6.1677633\n\n\n-0.9134436\n\n\n6.9813206\n\n\n-1.8743103\n\n\n10.6155073\n\n\n1.7708313\n\n\n10.9242269\n\n\n4.7897426\n\n\n5.2228631\n\n\n5.1455980\n\n\n-0.9701960\n\n\n5.7970445\n\n\n0.3254730\n\n\n1.5828592\n\n\n-1.7630027\n\n\n-3.9350027\n\n\n8.2198551\n\n\n\n\n\n\nnew_cities &lt;- tibble(population = rnorm(100, 7, 3))\n\nnew_cities &lt;- new_cities |&gt; \n  bind_cols(predict(lm_model_fit, new_data = new_cities, type = \"pred_int\")) |&gt; \n  kable()\n\nNote: New data must be a data frame with the same columns names as the training data"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-1",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nResponse: \\(Y\\)\nPredictor Variables: \\(X_1, X_2, \\ldots, X_p\\)\nAssume true relationship:\n\n\\[\n\\begin{aligned}\nY&=f(\\mathbf{X}) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\n\\end{aligned}\n\\] where \\(\\beta_j\\) quantifies the association between the \\(j^{th}\\) predictor and the response."
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Estimating Parameters",
    "text": "Multiple Linear Regression: Estimating Parameters\n\nSuppose \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) are estimates of \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)\nTraining Data:\n\nObserved response: \\(y_i\\) for \\(i=1,\\ldots,n\\)\nObserved predictors: \\(x_{1i}, x_{2i}, \\ldots x_{pi}\\) for \\(i=1,\\ldots, n\\)\n\nPredicted response: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\ldots + \\hat{\\beta}_px_{pi} \\text{ for } i=1, \\ldots, n\\]\nResiduals: \\(e_i = \\hat{y}_i - y_i\\) for \\(i=1, \\ldots, n\\)\nMean Squared Error (MSE): \\(MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}\\)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters-1",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Estimating Parameters",
    "text": "Multiple Linear Regression: Estimating Parameters\n\nGoal: Use training data to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimizes MSE\n\n\\(\\hat{\\beta}_i\\)’s called least-squares estimators\nSince minimizing MSE \\(\\implies\\) MSE is called cost/loss function\n\nCan use calculus or gradient descent to find \\(\\hat{\\beta}_i\\)’s"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#house-prices-dataset",
    "href": "slides/04-MultipleRegression.html#house-prices-dataset",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "House Prices dataset",
    "text": "House Prices dataset\n\nsize is in square feet\nnum_bedrooms is a count\nprice is in $1,000’s\n\n\nhouse_prices &lt;- readRDS(\"../data/house_prices.rds\")   # load dataset\nhead(house_prices, 6) |&gt; kable()  # print first 6 observations\n\n\n\n\nsize\nnum_bedrooms\nprice\n\n\n\n\n2104\n3\n399.9\n\n\n1600\n3\n329.9\n\n\n2400\n3\n369.0\n\n\n1416\n2\n232.0\n\n\n3000\n4\n539.9\n\n\n1985\n4\n299.9"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-2",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-2",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nSome Exploratory Data Analysis (EDA)\n\nlibrary(GGally)\nggpairs(data = house_prices)   # correlation plot"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-in-r",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-in-r",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression in R",
    "text": "Multiple Linear Regression in R\n\nmlr_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n\nhouse_price_mlr &lt;- mlr_model |&gt; \n  fit(price ~ size + num_bedrooms, data = house_prices)   # fit the model\n\nhouse_price_mlr |&gt; \n  tidy() |&gt;   # produce result summaries of the model\n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n89.5977660\n41.7674230\n2.1451591\n0.0374991\n\n\nsize\n0.1392106\n0.0147951\n9.4092391\n0.0000000\n\n\nnum_bedrooms\n-8.7379154\n15.4506975\n-0.5655353\n0.5745825"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Interpreting Parameters",
    "text": "Multiple Linear Regression: Interpreting Parameters\n\n\\(\\hat{\\beta}_0=89.5978\\): The intercept \\(\\implies\\) a house with 0 square feet and 0 bedrooms would cost approximately $89,598.80. Is this meaningful in context? Not really\n\\(\\hat{\\beta}_1=0.1392\\): With num_bedrooms remaining fixed, an additional 1 square foot of size leads to an increase in price by approximately $139.20.\n\\(\\hat{\\beta}_2=-8.7379\\): With size remaining fixed, an additional bedroom leads to an decrease in price of approximately $8,737.90.\n\n\n\nHmm…. that’s a little weird…\nSimpson’s Paradox: when relationship between two variables disappears or reverses when controlling for a third, confounding variable"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters-1",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Interpreting Parameters",
    "text": "Multiple Linear Regression: Interpreting Parameters\n\n\nWrite down our model in mathematical notation\n\\(\\text{price} = 89.5978 + 0.1392\\times\\text{size} - 8.7379\\times\\text{num_bedrooms}\\)\n\\(Y = 89.5978 + 0.1392X_1 - 8.7379X_2\\)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-prediction",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-prediction",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Prediction",
    "text": "Multiple Linear Regression: Prediction\n\nPrediction of price when size is 2000 square feet for a house with 3 bedrooms\n\\(\\text{sales} = 89.5978 + 0.1392\\times2000 - 8.7379\\times3 = 341.7841\\)\n\n\n\npredict(house_price_mlr, new_data = tibble(size = 2000, num_bedrooms = 3))   # obtain prediction\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  342.\n\n\n\nWhy don’t these match exactly? rounding"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-comparing-models",
    "href": "slides/04-MultipleRegression.html#linear-regression-comparing-models",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression: Comparing Models",
    "text": "Linear Regression: Comparing Models\n\n\n\nMany methods for comparing regression models from your regression course\nToday: Data splitting\nFirst: New Data\n\n\n\n\n\names housing data\n\nMany variables\nFocus on:\n\nSale_Price: in dollars\nGr_Liv_Area: size in square feet\nBedroom_AbvGr: number of bedrooms above grade\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computational-setup",
    "href": "slides/06-knn-workflows.html#computational-setup",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Computational Setup",
    "text": "Computational Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(modeldata) # contains ames dataset\n\ntidymodels_prefer()"
  },
  {
    "objectID": "slides/06-knn-workflows.html#comparing-models-data-splitting-with-tidymodels",
    "href": "slides/06-knn-workflows.html#comparing-models-data-splitting-with-tidymodels",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Comparing Models: Data Splitting with tidymodels",
    "text": "Comparing Models: Data Splitting with tidymodels\n\nset.seed(427) # Why?\n\names_split &lt;- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2049/881/2930&gt;\n\names_train &lt;- training(ames_split) # get training data\names_test &lt;- testing(ames_split) # get test data"
  },
  {
    "objectID": "slides/06-knn-workflows.html#k-nearest-neighbors-regression-multiple-predictors",
    "href": "slides/06-knn-workflows.html#k-nearest-neighbors-regression-multiple-predictors",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "K-Nearest Neighbors Regression (multiple predictors)",
    "text": "K-Nearest Neighbors Regression (multiple predictors)\n\names |&gt;\n  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |&gt;\n  head() |&gt; \n  kable()\n\n\n\n\nSale_Price\nGr_Liv_Area\nBedroom_AbvGr\n\n\n\n\n215000\n1656\n3\n\n\n105000\n896\n2\n\n\n172000\n1329\n3\n\n\n244000\n2110\n3\n\n\n189900\n1629\n3\n\n\n195500\n1604\n3\n\n\n\n\n\n\n\nShould 1 square foot count the same as 1 bedroom?\nNeed to center and scale (freq. just say scale)\n\nsubtract mean from each predictor\ndivide by standard deviation of each predictor\ncompares apples-to-apples"
  },
  {
    "objectID": "slides/06-knn-workflows.html#new-observation",
    "href": "slides/06-knn-workflows.html#new-observation",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "New observation",
    "text": "New observation\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\")\n\n\n\n\nWhere do you think the 10 nearest neighbors should be?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computing-distances",
    "href": "slides/06-knn-workflows.html#computing-distances",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Computing Distances",
    "text": "Computing Distances\n\names_dist &lt;- ames_train |&gt; \n  mutate(dist = sqrt((Gr_Liv_Area - 2000)^2 + (Bedroom_AbvGr-2)^2)) |&gt; \n  arrange(dist)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#nearest-neighbors",
    "href": "slides/06-knn-workflows.html#nearest-neighbors",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "10 “Nearest Neighbors”",
    "text": "10 “Nearest Neighbors”\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  geom_point(data = ames_dist |&gt; slice(1:10), color = \"green\")\n\n\n\n\nAre they where you thought they should be?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#looking-at-distances",
    "href": "slides/06-knn-workflows.html#looking-at-distances",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Looking at distances",
    "text": "Looking at distances\n\names_dist |&gt;\n  select(Gr_Liv_Area, Bedroom_AbvGr, dist) |&gt; \n  kable()\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\ndist\n\n\n\n\n2000\n3\n1.000000\n\n\n2000\n3\n1.000000\n\n\n2004\n4\n4.472136\n\n\n1995\n4\n5.385165\n\n\n2006\n2\n6.000000\n\n\n2007\n3\n7.071068\n\n\n2007\n3\n7.071068\n\n\n1992\n3\n8.062258\n\n\n2008\n3\n8.062258\n\n\n1991\n3\n9.055385\n\n\n1991\n4\n9.219544\n\n\n2009\n4\n9.219544\n\n\n2009\n4\n9.219544\n\n\n1988\n4\n12.165525\n\n\n1987\n2\n13.000000\n\n\n2014\n2\n14.000000\n\n\n1986\n3\n14.035669\n\n\n2014\n3\n14.035669\n\n\n1986\n4\n14.142136\n\n\n2014\n5\n14.317821\n\n\n2016\n4\n16.124516\n\n\n1984\n5\n16.278821\n\n\n1983\n3\n17.029386\n\n\n2018\n3\n18.027756\n\n\n1982\n3\n18.027756\n\n\n2019\n3\n19.026298\n\n\n1981\n4\n19.104973\n\n\n1981\n4\n19.104973\n\n\n2020\n3\n20.024984\n\n\n2020\n3\n20.024984\n\n\n2020\n3\n20.024984\n\n\n1980\n3\n20.024984\n\n\n2021\n3\n21.023796\n\n\n1978\n2\n22.000000\n\n\n1978\n4\n22.090722\n\n\n2022\n4\n22.090722\n\n\n1977\n3\n23.021729\n\n\n1976\n2\n24.000000\n\n\n1973\n3\n27.018512\n\n\n2028\n2\n28.000000\n\n\n1971\n3\n29.017236\n\n\n1970\n3\n30.016662\n\n\n2031\n3\n31.016125\n\n\n2031\n3\n31.016125\n\n\n1968\n4\n32.062439\n\n\n1968\n4\n32.062439\n\n\n2034\n2\n34.000000\n\n\n2034\n3\n34.014703\n\n\n1966\n1\n34.014703\n\n\n2035\n3\n35.014283\n\n\n2036\n3\n36.013886\n\n\n1964\n3\n36.013886\n\n\n1964\n3\n36.013886\n\n\n1964\n4\n36.055513\n\n\n1964\n4\n36.055513\n\n\n2037\n3\n37.013511\n\n\n1962\n3\n38.013156\n\n\n1962\n4\n38.052595\n\n\n1960\n3\n40.012498\n\n\n1960\n4\n40.049969\n\n\n1959\n4\n41.048752\n\n\n1959\n5\n41.109610\n\n\n2042\n3\n42.011903\n\n\n1958\n3\n42.011903\n\n\n2042\n3\n42.011903\n\n\n1958\n3\n42.011903\n\n\n1958\n4\n42.047592\n\n\n1955\n4\n45.044423\n\n\n2046\n2\n46.000000\n\n\n2046\n3\n46.010868\n\n\n2046\n3\n46.010868\n\n\n1954\n3\n46.010868\n\n\n1953\n3\n47.010637\n\n\n2048\n3\n48.010416\n\n\n1952\n4\n48.041649\n\n\n2048\n5\n48.093659\n\n\n2049\n4\n49.040799\n\n\n2050\n4\n50.039984\n\n\n2052\n3\n52.009614\n\n\n1948\n4\n52.038447\n\n\n1947\n3\n53.009433\n\n\n2054\n3\n54.009259\n\n\n1944\n3\n56.008928\n\n\n1944\n3\n56.008928\n\n\n1944\n4\n56.035703\n\n\n2057\n3\n57.008771\n\n\n2058\n3\n58.008620\n\n\n1940\n3\n60.008333\n\n\n2060\n3\n60.008333\n\n\n2060\n4\n60.033324\n\n\n2061\n3\n61.008196\n\n\n1939\n3\n61.008196\n\n\n1939\n3\n61.008196\n\n\n2061\n4\n61.032778\n\n\n1938\n4\n62.032250\n\n\n2064\n3\n64.007812\n\n\n1936\n3\n64.007812\n\n\n2064\n4\n64.031242\n\n\n1935\n3\n65.007692\n\n\n1935\n3\n65.007692\n\n\n1934\n3\n66.007575\n\n\n2067\n3\n67.007462\n\n\n1933\n4\n67.029844\n\n\n1932\n2\n68.000000\n\n\n2068\n3\n68.007352\n\n\n1932\n3\n68.007352\n\n\n2069\n4\n69.028979\n\n\n1930\n4\n70.028566\n\n\n1929\n3\n71.007042\n\n\n2071\n4\n71.028163\n\n\n1928\n4\n72.027772\n\n\n1928\n4\n72.027772\n\n\n2073\n3\n73.006849\n\n\n1923\n3\n77.006493\n\n\n1922\n2\n78.000000\n\n\n2078\n3\n78.006410\n\n\n1922\n3\n78.006410\n\n\n1922\n4\n78.025637\n\n\n1921\n4\n79.025312\n\n\n1920\n4\n80.024996\n\n\n1920\n4\n80.024996\n\n\n1920\n4\n80.024996\n\n\n1916\n3\n84.005952\n\n\n1915\n3\n85.005882\n\n\n1914\n3\n86.005814\n\n\n1913\n3\n87.005747\n\n\n1912\n3\n88.005682\n\n\n2088\n4\n88.022724\n\n\n1911\n3\n89.005618\n\n\n2090\n3\n90.005555\n\n\n2090\n3\n90.005555\n\n\n1909\n4\n91.021975\n\n\n1908\n4\n92.021737\n\n\n1908\n4\n92.021737\n\n\n2093\n3\n93.005376\n\n\n1906\n3\n94.005319\n\n\n1905\n3\n95.005263\n\n\n1904\n3\n96.005208\n\n\n1904\n3\n96.005208\n\n\n2097\n3\n97.005155\n\n\n2097\n1\n97.005155\n\n\n2097\n3\n97.005155\n\n\n2098\n3\n98.005102\n\n\n1902\n4\n98.020406\n\n\n1902\n4\n98.020406\n\n\n2098\n4\n98.020406\n\n\n2100\n3\n100.005000\n\n\n1898\n3\n102.004902\n\n\n2104\n5\n104.043260\n\n\n1895\n3\n105.004762\n\n\n1894\n4\n106.018866\n\n\n2108\n4\n108.018517\n\n\n1891\n3\n109.004587\n\n\n2110\n3\n110.004545\n\n\n1889\n4\n111.018017\n\n\n1886\n4\n114.017543\n\n\n1884\n2\n116.000000\n\n\n1884\n2\n116.000000\n\n\n1884\n4\n116.017240\n\n\n1882\n4\n118.016948\n\n\n1880\n3\n120.004167\n\n\n1879\n3\n121.004132\n\n\n2121\n3\n121.004132\n\n\n2122\n4\n122.016392\n\n\n1877\n4\n123.016259\n\n\n1876\n4\n124.016128\n\n\n2125\n4\n125.015999\n\n\n2126\n3\n126.003968\n\n\n1874\n3\n126.003968\n\n\n2127\n3\n127.003937\n\n\n1873\n3\n127.003937\n\n\n1873\n4\n127.015747\n\n\n1872\n3\n128.003906\n\n\n2128\n4\n128.015624\n\n\n1872\n4\n128.015624\n\n\n1869\n2\n131.000000\n\n\n1869\n2\n131.000000\n\n\n1868\n3\n132.003788\n\n\n2132\n1\n132.003788\n\n\n1868\n4\n132.015151\n\n\n1867\n2\n133.000000\n\n\n2133\n4\n133.015037\n\n\n1866\n2\n134.000000\n\n\n1866\n4\n134.014925\n\n\n2134\n5\n134.033578\n\n\n1865\n3\n135.003704\n\n\n2136\n4\n136.014705\n\n\n1864\n6\n136.058811\n\n\n1863\n4\n137.014598\n\n\n1861\n3\n139.003597\n\n\n2140\n3\n140.003571\n\n\n2140\n4\n140.014285\n\n\n2142\n4\n142.014084\n\n\n1856\n1\n144.003472\n\n\n2144\n4\n144.013888\n\n\n1852\n3\n148.003378\n\n\n2149\n1\n149.003356\n\n\n1848\n3\n152.003289\n\n\n2153\n3\n153.003268\n\n\n1846\n3\n154.003247\n\n\n1846\n3\n154.003247\n\n\n1845\n3\n155.003226\n\n\n1844\n2\n156.000000\n\n\n1844\n3\n156.003205\n\n\n1844\n3\n156.003205\n\n\n2156\n3\n156.003205\n\n\n1844\n3\n156.003205\n\n\n2157\n4\n157.012738\n\n\n1842\n3\n158.003164\n\n\n2158\n4\n158.012658\n\n\n1840\n3\n160.003125\n\n\n2161\n3\n161.003106\n\n\n1839\n4\n161.012422\n\n\n1839\n4\n161.012422\n\n\n1838\n3\n162.003086\n\n\n1838\n3\n162.003086\n\n\n1836\n3\n164.003049\n\n\n1836\n3\n164.003049\n\n\n1836\n4\n164.012195\n\n\n2167\n3\n167.002994\n\n\n1832\n4\n168.011904\n\n\n2168\n5\n168.026784\n\n\n2169\n3\n169.002959\n\n\n1830\n3\n170.002941\n\n\n1829\n4\n171.011696\n\n\n1828\n3\n172.002907\n\n\n1828\n3\n172.002907\n\n\n1828\n3\n172.002907\n\n\n1828\n3\n172.002907\n\n\n2172\n4\n172.011628\n\n\n1827\n2\n173.000000\n\n\n1826\n3\n174.002873\n\n\n1824\n3\n176.002841\n\n\n1824\n4\n176.011363\n\n\n1824\n5\n176.025566\n\n\n1823\n3\n177.002825\n\n\n2177\n5\n177.025422\n\n\n1822\n3\n178.002809\n\n\n1822\n4\n178.011236\n\n\n1820\n3\n180.002778\n\n\n1820\n3\n180.002778\n\n\n2180\n3\n180.002778\n\n\n1820\n4\n180.011111\n\n\n1820\n4\n180.011111\n\n\n1818\n3\n182.002747\n\n\n1818\n4\n182.010989\n\n\n2183\n4\n183.010929\n\n\n2184\n3\n184.002717\n\n\n1816\n3\n184.002717\n\n\n2184\n4\n184.010869\n\n\n1812\n3\n188.002660\n\n\n1812\n3\n188.002660\n\n\n1812\n4\n188.010638\n\n\n1811\n2\n189.000000\n\n\n1809\n3\n191.002618\n\n\n2192\n4\n192.010416\n\n\n1806\n4\n194.010309\n\n\n1804\n3\n196.002551\n\n\n2196\n3\n196.002551\n\n\n1803\n3\n197.002538\n\n\n1803\n3\n197.002538\n\n\n2197\n4\n197.010152\n\n\n1802\n3\n198.002525\n\n\n1802\n3\n198.002525\n\n\n1802\n4\n198.010101\n\n\n2198\n4\n198.010101\n\n\n1801\n3\n199.002512\n\n\n1801\n3\n199.002512\n\n\n1800\n2\n200.000000\n\n\n2200\n3\n200.002500\n\n\n1800\n3\n200.002500\n\n\n1798\n4\n202.009901\n\n\n2202\n4\n202.009901\n\n\n1797\n3\n203.002463\n\n\n1797\n3\n203.002463\n\n\n1797\n3\n203.002463\n\n\n1796\n3\n204.002451\n\n\n1795\n3\n205.002439\n\n\n1795\n3\n205.002439\n\n\n1795\n4\n205.009756\n\n\n2207\n3\n207.002415\n\n\n1792\n2\n208.000000\n\n\n1792\n3\n208.002404\n\n\n1792\n3\n208.002404\n\n\n1792\n4\n208.009615\n\n\n1790\n4\n210.009524\n\n\n1789\n3\n211.002370\n\n\n1788\n3\n212.002359\n\n\n2212\n3\n212.002359\n\n\n1788\n5\n212.021225\n\n\n1787\n3\n213.002347\n\n\n1786\n3\n214.002336\n\n\n1786\n3\n214.002336\n\n\n1786\n3\n214.002336\n\n\n1786\n3\n214.002336\n\n\n2214\n3\n214.002336\n\n\n1784\n4\n216.009259\n\n\n2217\n4\n217.009216\n\n\n1782\n3\n218.002294\n\n\n1782\n3\n218.002294\n\n\n1780\n3\n220.002273\n\n\n1779\n3\n221.002262\n\n\n1779\n3\n221.002262\n\n\n1778\n2\n222.000000\n\n\n2223\n3\n223.002242\n\n\n1776\n3\n224.002232\n\n\n1776\n3\n224.002232\n\n\n1776\n4\n224.008928\n\n\n2224\n4\n224.008928\n\n\n1775\n3\n225.002222\n\n\n1775\n3\n225.002222\n\n\n2225\n4\n225.008889\n\n\n1774\n4\n226.008849\n\n\n1773\n3\n227.002203\n\n\n1772\n3\n228.002193\n\n\n2228\n6\n228.035085\n\n\n1771\n3\n229.002183\n\n\n1771\n3\n229.002183\n\n\n2229\n5\n229.019650\n\n\n2230\n5\n230.019564\n\n\n1768\n2\n232.000000\n\n\n1768\n3\n232.002155\n\n\n1768\n3\n232.002155\n\n\n1768\n3\n232.002155\n\n\n1768\n4\n232.008621\n\n\n1767\n2\n233.000000\n\n\n2233\n5\n233.019313\n\n\n1766\n3\n234.002137\n\n\n2234\n3\n234.002137\n\n\n1764\n3\n236.002119\n\n\n1764\n4\n236.008474\n\n\n2236\n4\n236.008474\n\n\n1762\n3\n238.002101\n\n\n1762\n4\n238.008403\n\n\n1760\n4\n240.008333\n\n\n2240\n6\n240.033331\n\n\n2243\n4\n243.008230\n\n\n1756\n3\n244.002049\n\n\n1755\n3\n245.002041\n\n\n1755\n3\n245.002041\n\n\n1755\n4\n245.008163\n\n\n1752\n4\n248.008064\n\n\n1750\n3\n250.002000\n\n\n2250\n3\n250.002000\n\n\n1750\n4\n250.008000\n\n\n1749\n3\n251.001992\n\n\n2253\n3\n253.001976\n\n\n1746\n3\n254.001969\n\n\n1746\n3\n254.001969\n\n\n1744\n3\n256.001953\n\n\n2256\n4\n256.007812\n\n\n1743\n3\n257.001945\n\n\n1743\n3\n257.001945\n\n\n1743\n0\n257.007782\n\n\n1742\n2\n258.000000\n\n\n1742\n4\n258.007752\n\n\n2259\n3\n259.001931\n\n\n1740\n2\n260.000000\n\n\n1740\n2\n260.000000\n\n\n2260\n3\n260.001923\n\n\n1740\n3\n260.001923\n\n\n1740\n4\n260.007692\n\n\n1740\n4\n260.007692\n\n\n1738\n4\n262.007633\n\n\n2262\n4\n262.007633\n\n\n1737\n3\n263.001901\n\n\n2263\n3\n263.001901\n\n\n1734\n2\n266.000000\n\n\n1734\n3\n266.001880\n\n\n1734\n3\n266.001880\n\n\n1734\n3\n266.001880\n\n\n1733\n3\n267.001873\n\n\n2267\n3\n267.001873\n\n\n1733\n4\n267.007491\n\n\n2267\n4\n267.007491\n\n\n2268\n3\n268.001866\n\n\n1730\n3\n270.001852\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n4\n272.007353\n\n\n1728\n4\n272.007353\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1726\n3\n274.001825\n\n\n2274\n5\n274.016423\n\n\n1725\n3\n275.001818\n\n\n1724\n3\n276.001812\n\n\n1724\n3\n276.001812\n\n\n1724\n3\n276.001812\n\n\n2276\n3\n276.001812\n\n\n1721\n3\n279.001792\n\n\n1721\n3\n279.001792\n\n\n1721\n3\n279.001792\n\n\n1721\n4\n279.007168\n\n\n2279\n4\n279.007168\n\n\n1720\n3\n280.001786\n\n\n1720\n3\n280.001786\n\n\n1719\n1\n281.001779\n\n\n1718\n3\n282.001773\n\n\n1718\n3\n282.001773\n\n\n1718\n3\n282.001773\n\n\n1717\n2\n283.000000\n\n\n1717\n3\n283.001767\n\n\n1717\n3\n283.001767\n\n\n2283\n4\n283.007067\n\n\n1716\n2\n284.000000\n\n\n1716\n3\n284.001761\n\n\n1716\n3\n284.001761\n\n\n1716\n4\n284.007042\n\n\n2285\n4\n285.007018\n\n\n1714\n3\n286.001748\n\n\n1714\n3\n286.001748\n\n\n1712\n4\n288.006944\n\n\n2288\n4\n288.006944\n\n\n2290\n2\n290.000000\n\n\n1710\n3\n290.001724\n\n\n2290\n4\n290.006896\n\n\n2290\n4\n290.006896\n\n\n1709\n2\n291.000000\n\n\n1709\n3\n291.001718\n\n\n1709\n3\n291.001718\n\n\n1709\n3\n291.001718\n\n\n1709\n3\n291.001718\n\n\n2291\n4\n291.006873\n\n\n2291\n4\n291.006873\n\n\n1708\n3\n292.001712\n\n\n1707\n2\n293.000000\n\n\n2294\n5\n294.015306\n\n\n2295\n4\n295.006780\n\n\n1704\n3\n296.001689\n\n\n2296\n3\n296.001689\n\n\n1702\n1\n298.001678\n\n\n1701\n3\n299.001672\n\n\n1701\n4\n299.006689\n\n\n1700\n2\n300.000000\n\n\n1700\n4\n300.006667\n\n\n1699\n3\n301.001661\n\n\n1698\n3\n302.001656\n\n\n1696\n3\n304.001645\n\n\n1696\n3\n304.001645\n\n\n1696\n3\n304.001645\n\n\n1694\n2\n306.000000\n\n\n1694\n3\n306.001634\n\n\n1694\n3\n306.001634\n\n\n1694\n3\n306.001634\n\n\n1694\n4\n306.006536\n\n\n1692\n3\n308.001623\n\n\n1691\n2\n309.000000\n\n\n1691\n3\n309.001618\n\n\n1690\n3\n310.001613\n\n\n1690\n3\n310.001613\n\n\n1690\n4\n310.006452\n\n\n1689\n3\n311.001608\n\n\n1689\n3\n311.001608\n\n\n1689\n3\n311.001608\n\n\n1688\n2\n312.000000\n\n\n1688\n4\n312.006410\n\n\n1687\n3\n313.001597\n\n\n1686\n3\n314.001592\n\n\n1686\n1\n314.001592\n\n\n2314\n3\n314.001592\n\n\n1685\n2\n315.000000\n\n\n2315\n4\n315.006349\n\n\n1684\n2\n316.000000\n\n\n1683\n3\n317.001577\n\n\n1682\n3\n318.001572\n\n\n1682\n1\n318.001572\n\n\n2318\n3\n318.001572\n\n\n1680\n2\n320.000000\n\n\n2320\n2\n320.000000\n\n\n1680\n3\n320.001562\n\n\n1680\n3\n320.001562\n\n\n2320\n3\n320.001562\n\n\n1680\n4\n320.006250\n\n\n1680\n4\n320.006250\n\n\n1680\n4\n320.006250\n\n\n2322\n4\n322.006211\n\n\n1678\n6\n322.024844\n\n\n1677\n3\n323.001548\n\n\n2324\n4\n324.006173\n\n\n1675\n3\n325.001538\n\n\n1675\n3\n325.001538\n\n\n1674\n3\n326.001534\n\n\n1673\n3\n327.001529\n\n\n2327\n4\n327.006116\n\n\n1671\n3\n329.001520\n\n\n1671\n3\n329.001520\n\n\n2329\n4\n329.006079\n\n\n1671\n4\n329.006079\n\n\n1670\n2\n330.000000\n\n\n1670\n3\n330.001515\n\n\n1670\n4\n330.006061\n\n\n2331\n3\n331.001511\n\n\n1669\n4\n331.006042\n\n\n1668\n3\n332.001506\n\n\n1668\n3\n332.001506\n\n\n1668\n3\n332.001506\n\n\n2332\n3\n332.001506\n\n\n1668\n3\n332.001506\n\n\n2332\n4\n332.006024\n\n\n1666\n2\n334.000000\n\n\n1666\n3\n334.001497\n\n\n1666\n3\n334.001497\n\n\n2334\n3\n334.001497\n\n\n1665\n3\n335.001492\n\n\n1664\n3\n336.001488\n\n\n1664\n3\n336.001488\n\n\n1664\n4\n336.005952\n\n\n1664\n4\n336.005952\n\n\n1663\n3\n337.001484\n\n\n1662\n3\n338.001479\n\n\n2338\n4\n338.005917\n\n\n1661\n3\n339.001475\n\n\n1660\n3\n340.001471\n\n\n1660\n3\n340.001471\n\n\n2340\n4\n340.005882\n\n\n1659\n2\n341.000000\n\n\n1659\n3\n341.001466\n\n\n1658\n3\n342.001462\n\n\n1657\n3\n343.001458\n\n\n1657\n3\n343.001458\n\n\n1656\n2\n344.000000\n\n\n1656\n3\n344.001454\n\n\n1654\n3\n346.001445\n\n\n1654\n3\n346.001445\n\n\n1654\n4\n346.005780\n\n\n1653\n3\n347.001441\n\n\n1652\n2\n348.000000\n\n\n1652\n2\n348.000000\n\n\n1652\n3\n348.001437\n\n\n1652\n3\n348.001437\n\n\n1652\n3\n348.001437\n\n\n1652\n4\n348.005747\n\n\n1652\n4\n348.005747\n\n\n1651\n4\n349.005731\n\n\n1648\n2\n352.000000\n\n\n1647\n3\n353.001416\n\n\n1647\n3\n353.001416\n\n\n2353\n4\n353.005666\n\n\n1646\n2\n354.000000\n\n\n1646\n2\n354.000000\n\n\n1646\n2\n354.000000\n\n\n1646\n2\n354.000000\n\n\n1646\n3\n354.001412\n\n\n1646\n3\n354.001412\n\n\n1644\n3\n356.001404\n\n\n1642\n3\n358.001397\n\n\n2358\n4\n358.005586\n\n\n1642\n4\n358.005586\n\n\n2358\n4\n358.005586\n\n\n1641\n3\n359.001393\n\n\n1641\n3\n359.001393\n\n\n1640\n3\n360.001389\n\n\n1640\n3\n360.001389\n\n\n1640\n3\n360.001389\n\n\n1640\n3\n360.001389\n\n\n1639\n3\n361.001385\n\n\n1638\n2\n362.000000\n\n\n1638\n3\n362.001381\n\n\n2362\n3\n362.001381\n\n\n2364\n2\n364.000000\n\n\n1636\n3\n364.001374\n\n\n1636\n3\n364.001374\n\n\n1635\n2\n365.000000\n\n\n1635\n3\n365.001370\n\n\n2365\n3\n365.001370\n\n\n1634\n3\n366.001366\n\n\n1633\n3\n367.001362\n\n\n1632\n3\n368.001359\n\n\n1632\n4\n368.005435\n\n\n1632\n4\n368.005435\n\n\n1632\n4\n368.005435\n\n\n1630\n3\n370.001351\n\n\n1630\n3\n370.001351\n\n\n1630\n3\n370.001351\n\n\n1629\n3\n371.001348\n\n\n1629\n3\n371.001348\n\n\n1629\n3\n371.001348\n\n\n1628\n3\n372.001344\n\n\n2372\n4\n372.005376\n\n\n1627\n4\n373.005362\n\n\n1626\n2\n374.000000\n\n\n1626\n2\n374.000000\n\n\n1626\n2\n374.000000\n\n\n1626\n3\n374.001337\n\n\n1626\n3\n374.001337\n\n\n2374\n4\n374.005348\n\n\n1625\n2\n375.000000\n\n\n1625\n2\n375.000000\n\n\n1624\n2\n376.000000\n\n\n1624\n2\n376.000000\n\n\n2376\n4\n376.005319\n\n\n1621\n3\n379.001319\n\n\n1621\n3\n379.001319\n\n\n1620\n2\n380.000000\n\n\n2380\n3\n380.001316\n\n\n1620\n4\n380.005263\n\n\n1620\n4\n380.005263\n\n\n1618\n2\n382.000000\n\n\n1616\n2\n384.000000\n\n\n1616\n3\n384.001302\n\n\n1615\n3\n385.001299\n\n\n2385\n3\n385.001299\n\n\n1614\n3\n386.001295\n\n\n1614\n3\n386.001295\n\n\n1614\n3\n386.001295\n\n\n1612\n2\n388.000000\n\n\n1612\n3\n388.001289\n\n\n1612\n3\n388.001289\n\n\n1611\n3\n389.001285\n\n\n1611\n4\n389.005141\n\n\n1610\n3\n390.001282\n\n\n1610\n4\n390.005128\n\n\n1609\n3\n391.001279\n\n\n1609\n3\n391.001279\n\n\n1609\n5\n391.011509\n\n\n1608\n3\n392.001276\n\n\n1608\n3\n392.001276\n\n\n2392\n3\n392.001276\n\n\n2392\n3\n392.001276\n\n\n1606\n3\n394.001269\n\n\n1605\n3\n395.001266\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1603\n4\n397.005038\n\n\n1602\n3\n398.001256\n\n\n2398\n3\n398.001256\n\n\n1601\n3\n399.001253\n\n\n1601\n3\n399.001253\n\n\n1600\n2\n400.000000\n\n\n1600\n3\n400.001250\n\n\n1600\n3\n400.001250\n\n\n2400\n4\n400.005000\n\n\n1598\n3\n402.001244\n\n\n2403\n4\n403.004963\n\n\n1596\n3\n404.001238\n\n\n1595\n2\n405.000000\n\n\n1595\n2\n405.000000\n\n\n1594\n2\n406.000000\n\n\n1594\n3\n406.001232\n\n\n1594\n3\n406.001232\n\n\n1593\n3\n407.001229\n\n\n1593\n3\n407.001229\n\n\n1592\n3\n408.001225\n\n\n1590\n3\n410.001219\n\n\n1589\n2\n411.000000\n\n\n2411\n4\n411.004866\n\n\n1588\n3\n412.001214\n\n\n1588\n3\n412.001214\n\n\n1588\n3\n412.001214\n\n\n1588\n5\n412.010922\n\n\n1587\n3\n413.001211\n\n\n2414\n3\n414.001208\n\n\n1586\n3\n414.001208\n\n\n1585\n3\n415.001205\n\n\n1584\n4\n416.004808\n\n\n2417\n4\n417.004796\n\n\n1582\n3\n418.001196\n\n\n1582\n3\n418.001196\n\n\n1582\n3\n418.001196\n\n\n2418\n3\n418.001196\n\n\n1582\n4\n418.004785\n\n\n1580\n3\n420.001191\n\n\n1580\n3\n420.001191\n\n\n1578\n3\n422.001185\n\n\n1578\n3\n422.001185\n\n\n1578\n3\n422.001185\n\n\n2422\n4\n422.004739\n\n\n1577\n2\n423.000000\n\n\n1577\n3\n423.001182\n\n\n1576\n2\n424.000000\n\n\n1576\n3\n424.001179\n\n\n1576\n3\n424.001179\n\n\n1576\n3\n424.001179\n\n\n1575\n2\n425.000000\n\n\n1575\n2\n425.000000\n\n\n1574\n3\n426.001174\n\n\n1574\n3\n426.001174\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1572\n3\n428.001168\n\n\n1572\n3\n428.001168\n\n\n1571\n3\n429.001166\n\n\n1571\n3\n429.001166\n\n\n1569\n1\n431.001160\n\n\n1568\n3\n432.001157\n\n\n1568\n3\n432.001157\n\n\n2432\n4\n432.004630\n\n\n1567\n1\n433.001155\n\n\n1566\n5\n434.010369\n\n\n1565\n2\n435.000000\n\n\n1565\n2\n435.000000\n\n\n1564\n2\n436.000000\n\n\n1564\n3\n436.001147\n\n\n1564\n3\n436.001147\n\n\n1561\n2\n439.000000\n\n\n1561\n3\n439.001139\n\n\n1560\n3\n440.001136\n\n\n1560\n3\n440.001136\n\n\n1560\n3\n440.001136\n\n\n1560\n4\n440.004545\n\n\n1560\n4\n440.004545\n\n\n1559\n2\n441.000000\n\n\n1558\n3\n442.001131\n\n\n1556\n3\n444.001126\n\n\n1556\n3\n444.001126\n\n\n1556\n4\n444.004504\n\n\n1556\n4\n444.004504\n\n\n1555\n2\n445.000000\n\n\n1554\n2\n446.000000\n\n\n1553\n3\n447.001119\n\n\n2447\n4\n447.004474\n\n\n1552\n3\n448.001116\n\n\n1552\n3\n448.001116\n\n\n2448\n3\n448.001116\n\n\n2448\n4\n448.004464\n\n\n1551\n3\n449.001114\n\n\n1550\n3\n450.001111\n\n\n1549\n3\n451.001109\n\n\n1548\n2\n452.000000\n\n\n1548\n3\n452.001106\n\n\n1548\n3\n452.001106\n\n\n2452\n3\n452.001106\n\n\n2454\n3\n454.001101\n\n\n1546\n3\n454.001101\n\n\n1544\n3\n456.001097\n\n\n1541\n3\n459.001089\n\n\n1540\n3\n460.001087\n\n\n1540\n3\n460.001087\n\n\n1540\n4\n460.004348\n\n\n1539\n3\n461.001085\n\n\n1539\n3\n461.001085\n\n\n1538\n3\n462.001082\n\n\n2462\n4\n462.004329\n\n\n1537\n3\n463.001080\n\n\n1536\n3\n464.001078\n\n\n1536\n3\n464.001078\n\n\n1536\n3\n464.001078\n\n\n1536\n4\n464.004310\n\n\n2464\n4\n464.004310\n\n\n1535\n3\n465.001075\n\n\n1535\n4\n465.004301\n\n\n1535\n4\n465.004301\n\n\n1534\n3\n466.001073\n\n\n1533\n2\n467.000000\n\n\n2468\n4\n468.004274\n\n\n1531\n2\n469.000000\n\n\n1531\n3\n469.001066\n\n\n1530\n3\n470.001064\n\n\n2470\n1\n470.001064\n\n\n1530\n3\n470.001064\n\n\n1528\n3\n472.001059\n\n\n2473\n4\n473.004228\n\n\n2473\n4\n473.004228\n\n\n1526\n2\n474.000000\n\n\n1526\n4\n474.004219\n\n\n1525\n3\n475.001053\n\n\n1525\n3\n475.001053\n\n\n1525\n3\n475.001053\n\n\n2475\n4\n475.004210\n\n\n1524\n2\n476.000000\n\n\n1524\n3\n476.001050\n\n\n1523\n3\n477.001048\n\n\n1522\n4\n478.004184\n\n\n1521\n3\n479.001044\n\n\n1521\n4\n479.004175\n\n\n1520\n3\n480.001042\n\n\n1520\n3\n480.001042\n\n\n2480\n5\n480.009375\n\n\n2482\n4\n482.004149\n\n\n1517\n3\n483.001035\n\n\n1516\n3\n484.001033\n\n\n1515\n3\n485.001031\n\n\n2486\n5\n486.009259\n\n\n2486\n5\n486.009259\n\n\n1513\n2\n487.000000\n\n\n1513\n4\n487.004107\n\n\n1513\n4\n487.004107\n\n\n1512\n2\n488.000000\n\n\n1512\n2\n488.000000\n\n\n1512\n2\n488.000000\n\n\n1512\n3\n488.001025\n\n\n1511\n2\n489.000000\n\n\n1511\n3\n489.001022\n\n\n2490\n2\n490.000000\n\n\n1510\n3\n490.001020\n\n\n1510\n3\n490.001020\n\n\n1510\n4\n490.004082\n\n\n1509\n3\n491.001018\n\n\n1508\n1\n492.001016\n\n\n1507\n4\n493.004057\n\n\n1506\n2\n494.000000\n\n\n1506\n3\n494.001012\n\n\n2494\n4\n494.004049\n\n\n1505\n2\n495.000000\n\n\n1505\n3\n495.001010\n\n\n2495\n4\n495.004040\n\n\n2495\n5\n495.009091\n\n\n1504\n2\n496.000000\n\n\n1504\n1\n496.001008\n\n\n1504\n3\n496.001008\n\n\n1504\n3\n496.001008\n\n\n2497\n2\n497.000000\n\n\n1502\n3\n498.001004\n\n\n1502\n3\n498.001004\n\n\n1502\n1\n498.001004\n\n\n1501\n2\n499.000000\n\n\n1501\n2\n499.000000\n\n\n1501\n3\n499.001002\n\n\n1501\n3\n499.001002\n\n\n1501\n3\n499.001002\n\n\n2499\n4\n499.004008\n\n\n1500\n2\n500.000000\n\n\n1500\n3\n500.001000\n\n\n1500\n3\n500.001000\n\n\n2500\n5\n500.009000\n\n\n1499\n3\n501.000998\n\n\n1499\n3\n501.000998\n\n\n2501\n4\n501.003992\n\n\n1498\n2\n502.000000\n\n\n1498\n3\n502.000996\n\n\n1497\n3\n503.000994\n\n\n2503\n3\n503.000994\n\n\n1496\n3\n504.000992\n\n\n1496\n3\n504.000992\n\n\n1496\n3\n504.000992\n\n\n1496\n3\n504.000992\n\n\n1495\n3\n505.000990\n\n\n1495\n3\n505.000990\n\n\n1494\n2\n506.000000\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1493\n3\n507.000986\n\n\n1492\n3\n508.000984\n\n\n1492\n3\n508.000984\n\n\n1492\n3\n508.000984\n\n\n1491\n3\n509.000982\n\n\n1491\n3\n509.000982\n\n\n1490\n3\n510.000980\n\n\n1490\n3\n510.000980\n\n\n1489\n3\n511.000978\n\n\n1489\n3\n511.000978\n\n\n1489\n3\n511.000978\n\n\n1489\n3\n511.000978\n\n\n1488\n2\n512.000000\n\n\n1488\n3\n512.000977\n\n\n1488\n3\n512.000977\n\n\n1488\n3\n512.000977\n\n\n1487\n3\n513.000975\n\n\n1486\n3\n514.000973\n\n\n1486\n3\n514.000973\n\n\n2514\n4\n514.003891\n\n\n2515\n4\n515.003884\n\n\n1484\n3\n516.000969\n\n\n1484\n3\n516.000969\n\n\n1484\n3\n516.000969\n\n\n1482\n3\n518.000965\n\n\n1482\n4\n518.003861\n\n\n1481\n3\n519.000963\n\n\n1480\n3\n520.000962\n\n\n1480\n4\n520.003846\n\n\n2520\n4\n520.003846\n\n\n2520\n5\n520.008654\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n5\n521.008637\n\n\n1478\n2\n522.000000\n\n\n1478\n3\n522.000958\n\n\n1478\n3\n522.000958\n\n\n1476\n3\n524.000954\n\n\n1476\n3\n524.000954\n\n\n2524\n4\n524.003817\n\n\n1475\n4\n525.003809\n\n\n1474\n3\n526.000951\n\n\n2526\n4\n526.003802\n\n\n1473\n1\n527.000949\n\n\n1472\n2\n528.000000\n\n\n1472\n3\n528.000947\n\n\n1472\n3\n528.000947\n\n\n1471\n3\n529.000945\n\n\n1470\n2\n530.000000\n\n\n1470\n3\n530.000943\n\n\n1470\n3\n530.000943\n\n\n1470\n4\n530.003774\n\n\n1468\n2\n532.000000\n\n\n1468\n3\n532.000940\n\n\n1466\n3\n534.000936\n\n\n1466\n3\n534.000936\n\n\n1465\n3\n535.000935\n\n\n1465\n3\n535.000935\n\n\n1464\n3\n536.000933\n\n\n1464\n3\n536.000933\n\n\n1464\n4\n536.003731\n\n\n1463\n3\n537.000931\n\n\n1458\n2\n542.000000\n\n\n1458\n3\n542.000923\n\n\n1458\n3\n542.000923\n\n\n1458\n3\n542.000923\n\n\n1456\n2\n544.000000\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n4\n544.003676\n\n\n1456\n4\n544.003676\n\n\n2544\n6\n544.014706\n\n\n1455\n2\n545.000000\n\n\n1453\n2\n547.000000\n\n\n1453\n3\n547.000914\n\n\n1452\n2\n548.000000\n\n\n1452\n2\n548.000000\n\n\n1450\n2\n550.000000\n\n\n1450\n3\n550.000909\n\n\n2550\n4\n550.003636\n\n\n1446\n3\n554.000903\n\n\n1445\n3\n555.000901\n\n\n1445\n3\n555.000901\n\n\n1444\n2\n556.000000\n\n\n1444\n3\n556.000899\n\n\n1444\n3\n556.000899\n\n\n1442\n2\n558.000000\n\n\n1442\n3\n558.000896\n\n\n1442\n3\n558.000896\n\n\n1442\n4\n558.003584\n\n\n1441\n2\n559.000000\n\n\n1441\n3\n559.000894\n\n\n2559\n4\n559.003578\n\n\n1440\n2\n560.000000\n\n\n1440\n2\n560.000000\n\n\n1440\n3\n560.000893\n\n\n1440\n3\n560.000893\n\n\n1440\n3\n560.000893\n\n\n1440\n4\n560.003571\n\n\n1440\n4\n560.003571\n\n\n1437\n3\n563.000888\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1435\n3\n565.000885\n\n\n1434\n3\n566.000883\n\n\n1434\n4\n566.003534\n\n\n1432\n2\n568.000000\n\n\n1432\n3\n568.000880\n\n\n1432\n3\n568.000880\n\n\n1432\n3\n568.000880\n\n\n1432\n3\n568.000880\n\n\n1431\n3\n569.000879\n\n\n1431\n3\n569.000879\n\n\n1430\n3\n570.000877\n\n\n1430\n3\n570.000877\n\n\n1430\n3\n570.000877\n\n\n1430\n3\n570.000877\n\n\n1429\n3\n571.000876\n\n\n1428\n3\n572.000874\n\n\n1428\n4\n572.003496\n\n\n1427\n4\n573.003490\n\n\n1426\n3\n574.000871\n\n\n2574\n3\n574.000871\n\n\n1425\n3\n575.000870\n\n\n1425\n3\n575.000870\n\n\n1424\n2\n576.000000\n\n\n1424\n3\n576.000868\n\n\n2576\n4\n576.003472\n\n\n1422\n3\n578.000865\n\n\n1422\n3\n578.000865\n\n\n1422\n3\n578.000865\n\n\n1419\n2\n581.000000\n\n\n1419\n2\n581.000000\n\n\n1419\n2\n581.000000\n\n\n1419\n3\n581.000861\n\n\n1416\n3\n584.000856\n\n\n1416\n3\n584.000856\n\n\n1414\n2\n586.000000\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1412\n3\n588.000850\n\n\n1412\n4\n588.003401\n\n\n1411\n3\n589.000849\n\n\n1409\n3\n591.000846\n\n\n1409\n1\n591.000846\n\n\n2592\n6\n592.013513\n\n\n1406\n3\n594.000842\n\n\n1405\n2\n595.000000\n\n\n1404\n3\n596.000839\n\n\n1404\n3\n596.000839\n\n\n2599\n4\n599.003339\n\n\n1400\n3\n600.000833\n\n\n1400\n3\n600.000833\n\n\n2601\n4\n601.003328\n\n\n1398\n2\n602.000000\n\n\n1396\n3\n604.000828\n\n\n1396\n4\n604.003311\n\n\n1395\n3\n605.000826\n\n\n1394\n3\n606.000825\n\n\n1393\n3\n607.000824\n\n\n1392\n2\n608.000000\n\n\n1392\n2\n608.000000\n\n\n1392\n2\n608.000000\n\n\n1392\n3\n608.000822\n\n\n1392\n3\n608.000822\n\n\n1392\n3\n608.000822\n\n\n1392\n3\n608.000822\n\n\n1391\n2\n609.000000\n\n\n1390\n1\n610.000820\n\n\n2610\n4\n610.003279\n\n\n1389\n2\n611.000000\n\n\n1389\n3\n611.000818\n\n\n1387\n3\n613.000816\n\n\n1386\n3\n614.000814\n\n\n2614\n4\n614.003257\n\n\n1383\n2\n617.000000\n\n\n1383\n3\n617.000810\n\n\n1383\n3\n617.000810\n\n\n1382\n1\n618.000809\n\n\n1382\n3\n618.000809\n\n\n1382\n3\n618.000809\n\n\n1382\n3\n618.000809\n\n\n1381\n3\n619.000808\n\n\n2620\n4\n620.003226\n\n\n1378\n3\n622.000804\n\n\n2622\n3\n622.000804\n\n\n1377\n3\n623.000803\n\n\n1376\n2\n624.000000\n\n\n1376\n3\n624.000801\n\n\n1376\n3\n624.000801\n\n\n2624\n4\n624.003205\n\n\n1375\n3\n625.000800\n\n\n1374\n2\n626.000000\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1373\n3\n627.000797\n\n\n1372\n3\n628.000796\n\n\n1372\n3\n628.000796\n\n\n1370\n2\n630.000000\n\n\n1370\n2\n630.000000\n\n\n1370\n3\n630.000794\n\n\n1370\n3\n630.000794\n\n\n2630\n4\n630.003175\n\n\n1369\n3\n631.000792\n\n\n1368\n2\n632.000000\n\n\n1368\n3\n632.000791\n\n\n1367\n2\n633.000000\n\n\n1367\n3\n633.000790\n\n\n2634\n6\n634.012618\n\n\n1365\n2\n635.000000\n\n\n1365\n3\n635.000787\n\n\n1365\n3\n635.000787\n\n\n1364\n2\n636.000000\n\n\n1364\n2\n636.000000\n\n\n1363\n2\n637.000000\n\n\n1363\n2\n637.000000\n\n\n1363\n3\n637.000785\n\n\n1362\n2\n638.000000\n\n\n1362\n2\n638.000000\n\n\n1362\n3\n638.000784\n\n\n1362\n3\n638.000784\n\n\n1362\n4\n638.003135\n\n\n1361\n2\n639.000000\n\n\n1361\n3\n639.000783\n\n\n1360\n2\n640.000000\n\n\n1360\n2\n640.000000\n\n\n1360\n3\n640.000781\n\n\n1360\n1\n640.000781\n\n\n2640\n3\n640.000781\n\n\n2640\n4\n640.003125\n\n\n2640\n5\n640.007031\n\n\n1358\n2\n642.000000\n\n\n1358\n2\n642.000000\n\n\n1358\n2\n642.000000\n\n\n1358\n2\n642.000000\n\n\n1358\n3\n642.000779\n\n\n1358\n1\n642.000779\n\n\n1357\n2\n643.000000\n\n\n2643\n3\n643.000778\n\n\n1356\n3\n644.000776\n\n\n1356\n3\n644.000776\n\n\n1355\n3\n645.000775\n\n\n1355\n4\n645.003101\n\n\n2646\n3\n646.000774\n\n\n2646\n4\n646.003096\n\n\n1353\n2\n647.000000\n\n\n1352\n2\n648.000000\n\n\n1352\n2\n648.000000\n\n\n1352\n3\n648.000772\n\n\n1352\n4\n648.003086\n\n\n1352\n4\n648.003086\n\n\n1351\n3\n649.000770\n\n\n2649\n4\n649.003082\n\n\n1350\n2\n650.000000\n\n\n1350\n3\n650.000769\n\n\n1350\n3\n650.000769\n\n\n2650\n6\n650.012308\n\n\n1348\n3\n652.000767\n\n\n1347\n3\n653.000766\n\n\n1346\n2\n654.000000\n\n\n1346\n3\n654.000764\n\n\n2654\n4\n654.003058\n\n\n2654\n4\n654.003058\n\n\n1344\n2\n656.000000\n\n\n1344\n2\n656.000000\n\n\n1344\n2\n656.000000\n\n\n1344\n3\n656.000762\n\n\n1344\n3\n656.000762\n\n\n1344\n3\n656.000762\n\n\n1344\n4\n656.003049\n\n\n1343\n3\n657.000761\n\n\n1342\n2\n658.000000\n\n\n1342\n4\n658.003039\n\n\n1341\n2\n659.000000\n\n\n1341\n1\n659.000759\n\n\n1340\n3\n660.000758\n\n\n1340\n3\n660.000758\n\n\n1338\n2\n662.000000\n\n\n1338\n3\n662.000755\n\n\n1338\n3\n662.000755\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n3\n663.000754\n\n\n1337\n3\n663.000754\n\n\n1337\n3\n663.000754\n\n\n1336\n2\n664.000000\n\n\n1336\n3\n664.000753\n\n\n1336\n3\n664.000753\n\n\n1334\n2\n666.000000\n\n\n1334\n3\n666.000751\n\n\n1332\n2\n668.000000\n\n\n2668\n3\n668.000748\n\n\n1330\n3\n670.000746\n\n\n1330\n3\n670.000746\n\n\n1329\n3\n671.000745\n\n\n1329\n3\n671.000745\n\n\n1328\n2\n672.000000\n\n\n1328\n3\n672.000744\n\n\n1328\n3\n672.000744\n\n\n1327\n3\n673.000743\n\n\n2674\n2\n674.000000\n\n\n1326\n3\n674.000742\n\n\n1324\n2\n676.000000\n\n\n1324\n2\n676.000000\n\n\n1324\n3\n676.000740\n\n\n1324\n3\n676.000740\n\n\n1324\n3\n676.000740\n\n\n1324\n3\n676.000740\n\n\n1323\n3\n677.000739\n\n\n1322\n4\n678.002950\n\n\n1321\n1\n679.000736\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1317\n3\n683.000732\n\n\n2683\n4\n683.002928\n\n\n1316\n2\n684.000000\n\n\n1316\n3\n684.000731\n\n\n1316\n3\n684.000731\n\n\n1316\n3\n684.000731\n\n\n1316\n4\n684.002924\n\n\n1315\n3\n685.000730\n\n\n1314\n2\n686.000000\n\n\n1314\n3\n686.000729\n\n\n1314\n3\n686.000729\n\n\n1314\n3\n686.000729\n\n\n1313\n3\n687.000728\n\n\n2687\n4\n687.002911\n\n\n2687\n4\n687.002911\n\n\n1312\n3\n688.000727\n\n\n1312\n3\n688.000727\n\n\n1310\n2\n690.000000\n\n\n1310\n3\n690.000725\n\n\n2690\n3\n690.000725\n\n\n1310\n1\n690.000725\n\n\n2690\n4\n690.002899\n\n\n1309\n3\n691.000724\n\n\n1308\n2\n692.000000\n\n\n1308\n2\n692.000000\n\n\n1308\n3\n692.000723\n\n\n1308\n3\n692.000723\n\n\n1306\n2\n694.000000\n\n\n1306\n3\n694.000721\n\n\n1306\n3\n694.000721\n\n\n1306\n1\n694.000721\n\n\n1306\n1\n694.000721\n\n\n1304\n3\n696.000718\n\n\n1304\n3\n696.000718\n\n\n2696\n3\n696.000718\n\n\n2696\n4\n696.002874\n\n\n1302\n2\n698.000000\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n1\n698.000716\n\n\n1302\n3\n698.000716\n\n\n2698\n4\n698.002865\n\n\n1301\n2\n699.000000\n\n\n1299\n2\n701.000000\n\n\n1299\n3\n701.000713\n\n\n1298\n2\n702.000000\n\n\n1298\n3\n702.000712\n\n\n1298\n3\n702.000712\n\n\n1298\n3\n702.000712\n\n\n1297\n3\n703.000711\n\n\n1296\n2\n704.000000\n\n\n1296\n2\n704.000000\n\n\n1296\n3\n704.000710\n\n\n2704\n4\n704.002841\n\n\n1295\n2\n705.000000\n\n\n1295\n3\n705.000709\n\n\n1295\n1\n705.000709\n\n\n1294\n2\n706.000000\n\n\n1293\n2\n707.000000\n\n\n1292\n3\n708.000706\n\n\n1290\n2\n710.000000\n\n\n1290\n3\n710.000704\n\n\n1288\n3\n712.000702\n\n\n1288\n4\n712.002809\n\n\n1287\n2\n713.000000\n\n\n1287\n3\n713.000701\n\n\n1287\n3\n713.000701\n\n\n2713\n3\n713.000701\n\n\n1285\n2\n715.000000\n\n\n1285\n3\n715.000699\n\n\n1285\n3\n715.000699\n\n\n2715\n4\n715.002797\n\n\n1283\n3\n717.000697\n\n\n1282\n2\n718.000000\n\n\n1281\n1\n719.000695\n\n\n1280\n2\n720.000000\n\n\n1280\n2\n720.000000\n\n\n1278\n2\n722.000000\n\n\n1277\n2\n723.000000\n\n\n2726\n2\n726.000000\n\n\n1273\n2\n727.000000\n\n\n1273\n3\n727.000688\n\n\n2727\n3\n727.000688\n\n\n2728\n4\n728.002747\n\n\n1271\n4\n729.002743\n\n\n1270\n2\n730.000000\n\n\n2730\n4\n730.002740\n\n\n1269\n3\n731.000684\n\n\n1269\n3\n731.000684\n\n\n1268\n2\n732.000000\n\n\n1268\n3\n732.000683\n\n\n1268\n3\n732.000683\n\n\n1268\n3\n732.000683\n\n\n1266\n2\n734.000000\n\n\n1266\n2\n734.000000\n\n\n1265\n3\n735.000680\n\n\n1264\n2\n736.000000\n\n\n1264\n3\n736.000679\n\n\n1262\n2\n738.000000\n\n\n1261\n3\n739.000677\n\n\n1260\n3\n740.000676\n\n\n1258\n2\n742.000000\n\n\n1258\n3\n742.000674\n\n\n1258\n3\n742.000674\n\n\n1258\n0\n742.002695\n\n\n1256\n3\n744.000672\n\n\n1256\n1\n744.000672\n\n\n1256\n3\n744.000672\n\n\n1254\n3\n746.000670\n\n\n1253\n2\n747.000000\n\n\n1252\n2\n748.000000\n\n\n1252\n3\n748.000668\n\n\n1252\n3\n748.000668\n\n\n1252\n3\n748.000668\n\n\n1252\n1\n748.000668\n\n\n1251\n3\n749.000668\n\n\n1250\n2\n750.000000\n\n\n1250\n2\n750.000000\n\n\n1248\n2\n752.000000\n\n\n1248\n2\n752.000000\n\n\n1248\n2\n752.000000\n\n\n1248\n3\n752.000665\n\n\n1247\n1\n753.000664\n\n\n1246\n2\n754.000000\n\n\n1246\n3\n754.000663\n\n\n1246\n3\n754.000663\n\n\n1245\n3\n755.000662\n\n\n1245\n1\n755.000662\n\n\n1244\n3\n756.000661\n\n\n1242\n2\n758.000000\n\n\n1242\n3\n758.000660\n\n\n1242\n3\n758.000660\n\n\n2758\n4\n758.002638\n\n\n1241\n1\n759.000659\n\n\n1240\n2\n760.000000\n\n\n1240\n3\n760.000658\n\n\n1239\n1\n761.000657\n\n\n1236\n2\n764.000000\n\n\n1236\n2\n764.000000\n\n\n1236\n2\n764.000000\n\n\n1236\n3\n764.000654\n\n\n1235\n2\n765.000000\n\n\n1235\n1\n765.000654\n\n\n1232\n3\n768.000651\n\n\n1232\n3\n768.000651\n\n\n1232\n3\n768.000651\n\n\n1232\n3\n768.000651\n\n\n1230\n3\n770.000649\n\n\n1229\n2\n771.000000\n\n\n1229\n2\n771.000000\n\n\n1229\n2\n771.000000\n\n\n1229\n3\n771.000649\n\n\n1228\n2\n772.000000\n\n\n1228\n3\n772.000648\n\n\n1228\n3\n772.000648\n\n\n2772\n4\n772.002591\n\n\n1226\n2\n774.000000\n\n\n1226\n3\n774.000646\n\n\n1226\n1\n774.000646\n\n\n1226\n1\n774.000646\n\n\n1225\n3\n775.000645\n\n\n1224\n2\n776.000000\n\n\n1224\n2\n776.000000\n\n\n1224\n2\n776.000000\n\n\n1224\n3\n776.000644\n\n\n1224\n3\n776.000644\n\n\n1224\n3\n776.000644\n\n\n1224\n4\n776.002577\n\n\n1223\n2\n777.000000\n\n\n1222\n2\n778.000000\n\n\n1221\n2\n779.000000\n\n\n1221\n4\n779.002567\n\n\n1220\n2\n780.000000\n\n\n1220\n2\n780.000000\n\n\n1220\n2\n780.000000\n\n\n1220\n2\n780.000000\n\n\n1218\n2\n782.000000\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n4\n782.002557\n\n\n1217\n2\n783.000000\n\n\n1217\n3\n783.000639\n\n\n1217\n3\n783.000639\n\n\n1216\n2\n784.000000\n\n\n1216\n2\n784.000000\n\n\n1216\n3\n784.000638\n\n\n1216\n3\n784.000638\n\n\n1216\n3\n784.000638\n\n\n2784\n5\n784.005740\n\n\n1215\n3\n785.000637\n\n\n2786\n4\n786.002544\n\n\n2787\n6\n787.010165\n\n\n2787\n6\n787.010165\n\n\n1212\n3\n788.000635\n\n\n1212\n3\n788.000635\n\n\n1212\n3\n788.000635\n\n\n1211\n2\n789.000000\n\n\n1210\n3\n790.000633\n\n\n1210\n3\n790.000633\n\n\n2790\n4\n790.002532\n\n\n1209\n3\n791.000632\n\n\n1208\n2\n792.000000\n\n\n1208\n2\n792.000000\n\n\n1208\n2\n792.000000\n\n\n1208\n2\n792.000000\n\n\n1208\n3\n792.000631\n\n\n1207\n3\n793.000631\n\n\n2795\n4\n795.002516\n\n\n1204\n2\n796.000000\n\n\n1204\n2\n796.000000\n\n\n1204\n3\n796.000628\n\n\n1203\n3\n797.000627\n\n\n2798\n3\n798.000627\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n3\n800.000625\n\n\n1200\n3\n800.000625\n\n\n1200\n1\n800.000625\n\n\n1200\n3\n800.000625\n\n\n1200\n3\n800.000625\n\n\n1200\n4\n800.002500\n\n\n1200\n4\n800.002500\n\n\n1196\n2\n804.000000\n\n\n1196\n3\n804.000622\n\n\n1195\n4\n805.002485\n\n\n1194\n3\n806.000620\n\n\n1194\n3\n806.000620\n\n\n1193\n2\n807.000000\n\n\n1193\n3\n807.000620\n\n\n1192\n3\n808.000619\n\n\n1192\n4\n808.002475\n\n\n1191\n2\n809.000000\n\n\n1190\n3\n810.000617\n\n\n1188\n1\n812.000616\n\n\n1187\n2\n813.000000\n\n\n1187\n3\n813.000615\n\n\n1187\n3\n813.000615\n\n\n2814\n4\n814.002457\n\n\n1183\n2\n817.000000\n\n\n1183\n3\n817.000612\n\n\n1182\n3\n818.000611\n\n\n1181\n3\n819.000610\n\n\n1180\n2\n820.000000\n\n\n1180\n2\n820.000000\n\n\n1178\n2\n822.000000\n\n\n1178\n3\n822.000608\n\n\n1178\n3\n822.000608\n\n\n2822\n4\n822.002433\n\n\n1176\n2\n824.000000\n\n\n1175\n3\n825.000606\n\n\n1175\n3\n825.000606\n\n\n1174\n2\n826.000000\n\n\n1174\n3\n826.000605\n\n\n2826\n3\n826.000605\n\n\n1173\n3\n827.000605\n\n\n1173\n3\n827.000605\n\n\n1172\n3\n828.000604\n\n\n1167\n3\n833.000600\n\n\n1167\n3\n833.000600\n\n\n1164\n3\n836.000598\n\n\n1163\n3\n837.000597\n\n\n2840\n4\n840.002381\n\n\n1159\n3\n841.000595\n\n\n1159\n3\n841.000595\n\n\n1158\n3\n842.000594\n\n\n1158\n3\n842.000594\n\n\n1158\n3\n842.000594\n\n\n1155\n3\n845.000592\n\n\n1154\n3\n846.000591\n\n\n1154\n3\n846.000591\n\n\n1154\n3\n846.000591\n\n\n1152\n2\n848.000000\n\n\n1152\n2\n848.000000\n\n\n1152\n2\n848.000000\n\n\n1152\n2\n848.000000\n\n\n1152\n3\n848.000590\n\n\n1152\n3\n848.000590\n\n\n1152\n3\n848.000590\n\n\n1151\n2\n849.000000\n\n\n1150\n3\n850.000588\n\n\n1150\n3\n850.000588\n\n\n1148\n3\n852.000587\n\n\n1146\n3\n854.000586\n\n\n1146\n3\n854.000586\n\n\n1145\n2\n855.000000\n\n\n2855\n4\n855.002339\n\n\n1144\n3\n856.000584\n\n\n1144\n3\n856.000584\n\n\n1144\n3\n856.000584\n\n\n1144\n3\n856.000584\n\n\n1143\n3\n857.000583\n\n\n1143\n1\n857.000583\n\n\n1142\n2\n858.000000\n\n\n1142\n3\n858.000583\n\n\n1142\n3\n858.000583\n\n\n1141\n3\n859.000582\n\n\n1141\n3\n859.000582\n\n\n1140\n3\n860.000581\n\n\n1138\n3\n862.000580\n\n\n1137\n4\n863.002318\n\n\n1136\n2\n864.000000\n\n\n1134\n2\n866.000000\n\n\n1134\n3\n866.000577\n\n\n1132\n2\n868.000000\n\n\n1132\n2\n868.000000\n\n\n2868\n4\n868.002304\n\n\n1131\n3\n869.000575\n\n\n1131\n3\n869.000575\n\n\n1130\n2\n870.000000\n\n\n1128\n2\n872.000000\n\n\n1128\n2\n872.000000\n\n\n1128\n3\n872.000573\n\n\n1128\n3\n872.000573\n\n\n2872\n4\n872.002294\n\n\n2872\n4\n872.002294\n\n\n1127\n3\n873.000573\n\n\n1126\n3\n874.000572\n\n\n1126\n3\n874.000572\n\n\n1125\n3\n875.000571\n\n\n1124\n3\n876.000571\n\n\n1121\n3\n879.000569\n\n\n1120\n3\n880.000568\n\n\n1120\n3\n880.000568\n\n\n1117\n3\n883.000566\n\n\n1117\n3\n883.000566\n\n\n1116\n3\n884.000566\n\n\n1116\n3\n884.000566\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1113\n3\n887.000564\n\n\n1112\n2\n888.000000\n\n\n1112\n2\n888.000000\n\n\n1112\n3\n888.000563\n\n\n1112\n4\n888.002252\n\n\n1111\n2\n889.000000\n\n\n1111\n3\n889.000562\n\n\n1110\n1\n890.000562\n\n\n1109\n3\n891.000561\n\n\n1108\n3\n892.000561\n\n\n1107\n3\n893.000560\n\n\n1103\n3\n897.000557\n\n\n1102\n2\n898.000000\n\n\n2898\n2\n898.000000\n\n\n1100\n3\n900.000556\n\n\n1100\n3\n900.000556\n\n\n1100\n3\n900.000556\n\n\n1099\n3\n901.000555\n\n\n1098\n2\n902.000000\n\n\n1098\n3\n902.000554\n\n\n1097\n3\n903.000554\n\n\n1096\n2\n904.000000\n\n\n1096\n3\n904.000553\n\n\n1096\n3\n904.000553\n\n\n1095\n2\n905.000000\n\n\n1094\n3\n906.000552\n\n\n1094\n3\n906.000552\n\n\n1093\n2\n907.000000\n\n\n1093\n3\n907.000551\n\n\n1092\n2\n908.000000\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1091\n2\n909.000000\n\n\n1090\n2\n910.000000\n\n\n1089\n3\n911.000549\n\n\n1088\n2\n912.000000\n\n\n1088\n2\n912.000000\n\n\n1086\n3\n914.000547\n\n\n1086\n3\n914.000547\n\n\n1082\n3\n918.000545\n\n\n1081\n3\n919.000544\n\n\n1080\n2\n920.000000\n\n\n1080\n2\n920.000000\n\n\n1080\n3\n920.000544\n\n\n1080\n3\n920.000544\n\n\n1078\n2\n922.000000\n\n\n1078\n3\n922.000542\n\n\n1077\n2\n923.000000\n\n\n1077\n2\n923.000000\n\n\n1077\n3\n923.000542\n\n\n1075\n3\n925.000541\n\n\n1074\n3\n926.000540\n\n\n1073\n2\n927.000000\n\n\n1073\n3\n927.000539\n\n\n1073\n3\n927.000539\n\n\n1073\n3\n927.000539\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1069\n2\n931.000000\n\n\n1069\n2\n931.000000\n\n\n1069\n2\n931.000000\n\n\n1068\n2\n932.000000\n\n\n1068\n3\n932.000536\n\n\n1067\n2\n933.000000\n\n\n1064\n2\n936.000000\n\n\n1064\n3\n936.000534\n\n\n1063\n3\n937.000534\n\n\n1062\n3\n938.000533\n\n\n1061\n1\n939.000532\n\n\n1060\n3\n940.000532\n\n\n1060\n1\n940.000532\n\n\n1060\n3\n940.000532\n\n\n1059\n3\n941.000531\n\n\n1057\n3\n943.000530\n\n\n1057\n3\n943.000530\n\n\n1057\n3\n943.000530\n\n\n1056\n2\n944.000000\n\n\n1056\n3\n944.000530\n\n\n2944\n3\n944.000530\n\n\n1056\n3\n944.000530\n\n\n1056\n3\n944.000530\n\n\n1056\n0\n944.002119\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n2945\n3\n945.000529\n\n\n1054\n3\n946.000528\n\n\n1054\n3\n946.000528\n\n\n1053\n2\n947.000000\n\n\n1053\n3\n947.000528\n\n\n1053\n3\n947.000528\n\n\n1052\n3\n948.000527\n\n\n1052\n3\n948.000527\n\n\n1051\n3\n949.000527\n\n\n1050\n2\n950.000000\n\n\n1050\n3\n950.000526\n\n\n1050\n3\n950.000526\n\n\n1049\n2\n951.000000\n\n\n1049\n2\n951.000000\n\n\n1048\n2\n952.000000\n\n\n1048\n3\n952.000525\n\n\n1045\n2\n955.000000\n\n\n1045\n3\n955.000524\n\n\n1045\n3\n955.000524\n\n\n1044\n3\n956.000523\n\n\n1043\n2\n957.000000\n\n\n1041\n3\n959.000521\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1039\n3\n961.000520\n\n\n1038\n2\n962.000000\n\n\n1036\n3\n964.000519\n\n\n1036\n1\n964.000519\n\n\n1034\n2\n966.000000\n\n\n1034\n3\n966.000518\n\n\n1034\n3\n966.000518\n\n\n1034\n1\n966.000518\n\n\n1033\n3\n967.000517\n\n\n1032\n2\n968.000000\n\n\n1032\n2\n968.000000\n\n\n1032\n2\n968.000000\n\n\n1032\n3\n968.000517\n\n\n1031\n2\n969.000000\n\n\n1030\n3\n970.000516\n\n\n1029\n3\n971.000515\n\n\n1027\n2\n973.000000\n\n\n1027\n3\n973.000514\n\n\n1024\n2\n976.000000\n\n\n1024\n3\n976.000512\n\n\n1022\n2\n978.000000\n\n\n2978\n5\n978.004601\n\n\n1020\n3\n980.000510\n\n\n1020\n1\n980.000510\n\n\n1015\n3\n985.000508\n\n\n1013\n3\n987.000507\n\n\n1012\n4\n988.002024\n\n\n1009\n3\n991.000505\n\n\n1008\n2\n992.000000\n\n\n1008\n3\n992.000504\n\n\n1008\n3\n992.000504\n\n\n1008\n1\n992.000504\n\n\n1006\n3\n994.000503\n\n\n1005\n2\n995.000000\n\n\n1005\n2\n995.000000\n\n\n1005\n3\n995.000503\n\n\n1004\n2\n996.000000\n\n\n1003\n3\n997.000502\n\n\n1002\n3\n998.000501\n\n\n1001\n2\n999.000000\n\n\n999\n3\n1001.000500\n\n\n999\n3\n1001.000500\n\n\n998\n3\n1002.000499\n\n\n996\n2\n1004.000000\n\n\n996\n3\n1004.000498\n\n\n3005\n3\n1005.000498\n\n\n992\n2\n1008.000000\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n988\n2\n1012.000000\n\n\n988\n2\n1012.000000\n\n\n988\n3\n1012.000494\n\n\n988\n3\n1012.000494\n\n\n988\n3\n1012.000494\n\n\n988\n1\n1012.000494\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n985\n2\n1015.000000\n\n\n985\n3\n1015.000493\n\n\n984\n3\n1016.000492\n\n\n984\n3\n1016.000492\n\n\n984\n3\n1016.000492\n\n\n981\n3\n1019.000491\n\n\n980\n3\n1020.000490\n\n\n980\n3\n1020.000490\n\n\n980\n3\n1020.000490\n\n\n980\n4\n1020.001961\n\n\n976\n2\n1024.000000\n\n\n976\n2\n1024.000000\n\n\n974\n3\n1026.000487\n\n\n972\n2\n1028.000000\n\n\n971\n3\n1029.000486\n\n\n970\n3\n1030.000485\n\n\n968\n2\n1032.000000\n\n\n968\n2\n1032.000000\n\n\n968\n2\n1032.000000\n\n\n968\n4\n1032.001938\n\n\n965\n2\n1035.000000\n\n\n964\n2\n1036.000000\n\n\n964\n3\n1036.000483\n\n\n960\n2\n1040.000000\n\n\n960\n2\n1040.000000\n\n\n960\n2\n1040.000000\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n0\n1040.001923\n\n\n958\n2\n1042.000000\n\n\n958\n2\n1042.000000\n\n\n958\n2\n1042.000000\n\n\n954\n3\n1046.000478\n\n\n952\n2\n1048.000000\n\n\n952\n2\n1048.000000\n\n\n952\n3\n1048.000477\n\n\n951\n2\n1049.000000\n\n\n950\n2\n1050.000000\n\n\n950\n3\n1050.000476\n\n\n949\n2\n1051.000000\n\n\n948\n2\n1052.000000\n\n\n948\n3\n1052.000475\n\n\n948\n3\n1052.000475\n\n\n948\n3\n1052.000475\n\n\n945\n2\n1055.000000\n\n\n943\n2\n1057.000000\n\n\n943\n2\n1057.000000\n\n\n943\n3\n1057.000473\n\n\n941\n3\n1059.000472\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n3\n1064.000470\n\n\n936\n3\n1064.000470\n\n\n935\n2\n1065.000000\n\n\n935\n3\n1065.000470\n\n\n934\n2\n1066.000000\n\n\n932\n2\n1068.000000\n\n\n928\n2\n1072.000000\n\n\n928\n3\n1072.000466\n\n\n925\n2\n1075.000000\n\n\n925\n2\n1075.000000\n\n\n925\n3\n1075.000465\n\n\n925\n3\n1075.000465\n\n\n925\n3\n1075.000465\n\n\n924\n2\n1076.000000\n\n\n924\n3\n1076.000465\n\n\n924\n3\n1076.000465\n\n\n923\n2\n1077.000000\n\n\n923\n3\n1077.000464\n\n\n922\n2\n1078.000000\n\n\n919\n2\n1081.000000\n\n\n919\n3\n1081.000462\n\n\n918\n2\n1082.000000\n\n\n918\n3\n1082.000462\n\n\n914\n2\n1086.000000\n\n\n914\n2\n1086.000000\n\n\n914\n2\n1086.000000\n\n\n914\n3\n1086.000460\n\n\n3086\n3\n1086.000460\n\n\n3086\n4\n1086.001842\n\n\n913\n3\n1087.000460\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n3\n1088.000460\n\n\n912\n3\n1088.000460\n\n\n912\n3\n1088.000460\n\n\n910\n3\n1090.000459\n\n\n909\n3\n1091.000458\n\n\n908\n2\n1092.000000\n\n\n907\n3\n1093.000458\n\n\n907\n3\n1093.000458\n\n\n907\n3\n1093.000458\n\n\n906\n2\n1094.000000\n\n\n904\n2\n1096.000000\n\n\n904\n1\n1096.000456\n\n\n904\n3\n1096.000456\n\n\n902\n2\n1098.000000\n\n\n901\n2\n1099.000000\n\n\n900\n3\n1100.000454\n\n\n900\n3\n1100.000454\n\n\n899\n3\n1101.000454\n\n\n898\n3\n1102.000454\n\n\n897\n3\n1103.000453\n\n\n896\n2\n1104.000000\n\n\n896\n2\n1104.000000\n\n\n894\n2\n1106.000000\n\n\n894\n2\n1106.000000\n\n\n894\n2\n1106.000000\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n893\n2\n1107.000000\n\n\n892\n3\n1108.000451\n\n\n892\n3\n1108.000451\n\n\n892\n3\n1108.000451\n\n\n890\n3\n1110.000450\n\n\n889\n3\n1111.000450\n\n\n888\n3\n1112.000450\n\n\n887\n3\n1113.000449\n\n\n884\n2\n1116.000000\n\n\n884\n2\n1116.000000\n\n\n884\n2\n1116.000000\n\n\n882\n2\n1118.000000\n\n\n882\n2\n1118.000000\n\n\n882\n3\n1118.000447\n\n\n882\n1\n1118.000447\n\n\n882\n3\n1118.000447\n\n\n879\n2\n1121.000000\n\n\n879\n3\n1121.000446\n\n\n876\n2\n1124.000000\n\n\n876\n3\n1124.000445\n\n\n874\n3\n1126.000444\n\n\n874\n3\n1126.000444\n\n\n874\n3\n1126.000444\n\n\n872\n2\n1128.000000\n\n\n872\n3\n1128.000443\n\n\n869\n2\n1131.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n861\n1\n1139.000439\n\n\n3140\n4\n1140.001754\n\n\n858\n2\n1142.000000\n\n\n858\n3\n1142.000438\n\n\n856\n2\n1144.000000\n\n\n854\n2\n1146.000000\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n846\n2\n1154.000000\n\n\n845\n2\n1155.000000\n\n\n845\n3\n1155.000433\n\n\n845\n1\n1155.000433\n\n\n845\n3\n1155.000433\n\n\n844\n2\n1156.000000\n\n\n841\n2\n1159.000000\n\n\n840\n2\n1160.000000\n\n\n840\n3\n1160.000431\n\n\n836\n2\n1164.000000\n\n\n835\n2\n1165.000000\n\n\n833\n3\n1167.000428\n\n\n832\n2\n1168.000000\n\n\n832\n2\n1168.000000\n\n\n827\n2\n1173.000000\n\n\n825\n2\n1175.000000\n\n\n825\n2\n1175.000000\n\n\n819\n2\n1181.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n3\n1184.000422\n\n\n816\n3\n1184.000422\n\n\n813\n2\n1187.000000\n\n\n810\n2\n1190.000000\n\n\n808\n2\n1192.000000\n\n\n808\n1\n1192.000419\n\n\n804\n2\n1196.000000\n\n\n803\n2\n1197.000000\n\n\n800\n1\n1200.000417\n\n\n797\n2\n1203.000000\n\n\n796\n2\n1204.000000\n\n\n796\n2\n1204.000000\n\n\n796\n2\n1204.000000\n\n\n796\n2\n1204.000000\n\n\n793\n2\n1207.000000\n\n\n792\n2\n1208.000000\n\n\n792\n2\n1208.000000\n\n\n792\n2\n1208.000000\n\n\n790\n2\n1210.000000\n\n\n789\n2\n1211.000000\n\n\n789\n2\n1211.000000\n\n\n784\n2\n1216.000000\n\n\n780\n2\n1220.000000\n\n\n773\n2\n1227.000000\n\n\n3228\n4\n1228.001629\n\n\n768\n2\n1232.000000\n\n\n768\n2\n1232.000000\n\n\n768\n2\n1232.000000\n\n\n768\n1\n1232.000406\n\n\n767\n1\n1233.000405\n\n\n765\n2\n1235.000000\n\n\n764\n2\n1236.000000\n\n\n759\n1\n1241.000403\n\n\n756\n2\n1244.000000\n\n\n754\n2\n1246.000000\n\n\n752\n2\n1248.000000\n\n\n747\n2\n1253.000000\n\n\n747\n1\n1253.000399\n\n\n733\n2\n1267.000000\n\n\n732\n2\n1268.000000\n\n\n725\n1\n1275.000392\n\n\n3279\n4\n1279.001564\n\n\n720\n2\n1280.000000\n\n\n720\n1\n1280.000391\n\n\n715\n2\n1285.000000\n\n\n704\n2\n1296.000000\n\n\n698\n2\n1302.000000\n\n\n694\n2\n1306.000000\n\n\n693\n2\n1307.000000\n\n\n691\n2\n1309.000000\n\n\n672\n2\n1328.000000\n\n\n670\n2\n1330.000000\n\n\n641\n2\n1359.000000\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n3390\n5\n1390.003237\n\n\n605\n2\n1395.000000\n\n\n3395\n8\n1395.012903\n\n\n572\n2\n1428.000000\n\n\n3447\n4\n1447.001382\n\n\n540\n1\n1460.000342\n\n\n3493\n3\n1493.000335\n\n\n3500\n4\n1500.001333\n\n\n498\n1\n1502.000333\n\n\n480\n1\n1520.000329\n\n\n438\n1\n1562.000320\n\n\n407\n1\n1593.000314\n\n\n3608\n4\n1608.001244\n\n\n3672\n5\n1672.002691\n\n\n3820\n5\n1820.002473\n\n\n4316\n4\n2316.000864\n\n\n4676\n3\n2676.000187\n\n\n5095\n2\n3095.000000"
  },
  {
    "objectID": "slides/06-knn-workflows.html#original-data",
    "href": "slides/06-knn-workflows.html#original-data",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Original data",
    "text": "Original data\n\nggplot(ames_train, aes(x = Gr_Liv_Area)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#centering",
    "href": "slides/06-knn-workflows.html#centering",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Centering",
    "text": "Centering\n\nWhat’s different between these two plots\n\n\n\nOriginal data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20)) \n\n\n\n\n\n\n\n\n\n\nCentered data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Centered = Gr_Liv_Area - mean(Gr_Liv_Area)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Centered)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaling",
    "href": "slides/06-knn-workflows.html#scaling",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaling",
    "text": "Scaling\n\nWhat’s different between these two plots\n\n\n\nOriginal data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20)) \n\n\n\n\n\n\n\n\n\n\nNormalized (centered and scaled) data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Centered_Scaled = (Gr_Liv_Area - mean(Gr_Liv_Area))/sd(Gr_Liv_Area)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Centered_Scaled)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#original-data-1",
    "href": "slides/06-knn-workflows.html#original-data-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Original Data",
    "text": "Original Data\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point()"
  },
  {
    "objectID": "slides/06-knn-workflows.html#centering-1",
    "href": "slides/06-knn-workflows.html#centering-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Centering",
    "text": "Centering\n\nWhat’s different between these two plots\n\n\n\nOriginal Data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point()+\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nCentered data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Centered = Gr_Liv_Area - mean(Gr_Liv_Area),\n                     Bedroom_AbvGr_Centered = Bedroom_AbvGr - mean(Bedroom_AbvGr)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Centered, y = Bedroom_AbvGr_Centered)) +\n  geom_point() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaling-1",
    "href": "slides/06-knn-workflows.html#scaling-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaling",
    "text": "Scaling\n\nWhat’s different between these two plots\n\n\n\nOriginal data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point()+\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nNormalized data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n                     Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) +\n  geom_point() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#new-observation-1",
    "href": "slides/06-knn-workflows.html#new-observation-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "New Observation",
    "text": "New Observation\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\")\n\n\n\n\nWhere should we put this point on the normalized plot?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#plotting-new-observation-witout-normalizing",
    "href": "slides/06-knn-workflows.html#plotting-new-observation-witout-normalizing",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Plotting new observation witout normalizing",
    "text": "Plotting new observation witout normalizing\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\names_train |&gt; mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area), Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr)) |&gt; \n  ggplot(aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) +\n  geom_point() +\n  geom_point(data = new_house, mapping = aes(Gr_Liv_Area, y = Bedroom_AbvGr), color = \"red\")\n\n\n\n\nDoes this look right?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#normalized-using-training-data",
    "href": "slides/06-knn-workflows.html#normalized-using-training-data",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Normalized using training data",
    "text": "Normalized using training data\n\n\nOriginal Data\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled data\n\n\nCode\names_train_scaled &lt;- \n  ames_train |&gt; mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area), Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\n\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2) |&gt; \n  mutate(Gr_Liv_Area_Scaled = (Gr_Liv_Area - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area), \n         Bedroom_AbvGr_Scaled = (Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/ sd(ames_train$Bedroom_AbvGr))\n\names_train_scaled |&gt; \n  ggplot(aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) +\n  geom_point() +\n  geom_point(data = new_house, color = \"red\")+\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#compute-new-distances",
    "href": "slides/06-knn-workflows.html#compute-new-distances",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Compute new distances",
    "text": "Compute new distances\n\nnew_house |&gt; kable()\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\n\n\n\n\n2000\n2\n0.9900212\n-1.041808\n\n\n\n\names_dist &lt;- ames_train_scaled |&gt; \n  mutate(orig_dist = sqrt((Gr_Liv_Area - 2000)^2 + (Bedroom_AbvGr-2)^2),\n         scaled_dist = sqrt((Gr_Liv_Area_Scaled - 0.99)^2 + (Bedroom_AbvGr_Scaled-(-1.04))^2))\n\norig_closest &lt;- ames_dist |&gt; \n  arrange(orig_dist) |&gt; \n  head(10)\n\nscaled_closest &lt;- ames_dist |&gt; \n  arrange(scaled_dist) |&gt; \n  head(10)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaled-distances-look-better",
    "href": "slides/06-knn-workflows.html#scaled-distances-look-better",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaled distances look better",
    "text": "Scaled distances look better\n\n\n\norig_closest |&gt; select(Gr_Liv_Area, Bedroom_AbvGr, Gr_Liv_Area_Scaled, Bedroom_AbvGr_Scaled, orig_dist, scaled_dist) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\norig_dist\nscaled_dist\n\n\n\n\n2000\n3\n0.9900212\n0.1621766\n1.000000\n1.2021766\n\n\n2000\n3\n0.9900212\n0.1621766\n1.000000\n1.2021766\n\n\n2004\n4\n0.9979593\n1.3661617\n4.472136\n2.4061749\n\n\n1995\n4\n0.9800985\n1.3661617\n5.385165\n2.4061821\n\n\n2006\n2\n1.0019284\n-1.0418085\n6.000000\n0.0120647\n\n\n2007\n3\n1.0039129\n0.1621766\n7.071068\n1.2022571\n\n\n2007\n3\n1.0039129\n0.1621766\n7.071068\n1.2022571\n\n\n1992\n3\n0.9741448\n0.1621766\n8.062258\n1.2022812\n\n\n2008\n3\n1.0058975\n0.1621766\n8.062258\n1.2022817\n\n\n1991\n3\n0.9721603\n0.1621766\n9.055385\n1.2023090\n\n\n\n\n\n\n\nscaled_closest |&gt;  select(Gr_Liv_Area, Bedroom_AbvGr, Gr_Liv_Area_Scaled, Bedroom_AbvGr_Scaled, orig_dist, scaled_dist) |&gt;  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\norig_dist\nscaled_dist\n\n\n\n\n2006\n2\n1.0019284\n-1.041809\n6\n0.01206470\n\n\n1987\n2\n0.9642222\n-1.041809\n13\n0.02584121\n\n\n2014\n2\n1.0178047\n-1.041809\n14\n0.02786344\n\n\n1978\n2\n0.9463613\n-1.041809\n22\n0.04367615\n\n\n1976\n2\n0.9423922\n-1.041809\n24\n0.04764210\n\n\n2028\n2\n1.0455882\n-1.041809\n28\n0.05561764\n\n\n2034\n2\n1.0574955\n-1.041809\n34\n0.06751968\n\n\n2046\n2\n1.0813099\n-1.041809\n46\n0.09132782\n\n\n1932\n2\n0.8550726\n-1.041809\n68\n0.13493957\n\n\n1922\n2\n0.8352272\n-1.041809\n78\n0.15478340"
  },
  {
    "objectID": "slides/06-knn-workflows.html#which-plot-looks-better",
    "href": "slides/06-knn-workflows.html#which-plot-looks-better",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Which plot looks better",
    "text": "Which plot looks better\n\n\nOriginal distances\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  geom_point(data = orig_closest, color = \"green\") +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled distances\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  geom_point(data = scaled_closest, color = \"green\") +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaling-test-set",
    "href": "slides/06-knn-workflows.html#scaling-test-set",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaling Test Set",
    "text": "Scaling Test Set\n\nSubtract mean of corresponding variable in training set\nDivide by sd of corresponding variable in training set"
  },
  {
    "objectID": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-untransformed",
    "href": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-untransformed",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Visualizing Training and Test Sets: Untransformed",
    "text": "Visualizing Training and Test Sets: Untransformed\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-self-centered",
    "href": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-self-centered",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Visualizing Training and Test Sets: Self Centered",
    "text": "Visualizing Training and Test Sets: Self Centered\n\n\nOriginal Data\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled Data\n\n\nCode\names_train_centered &lt;- ames_train |&gt; \n  mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n         Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\names_test_self_centered &lt;- ames_test |&gt; \n  mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n         Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\nggplot(ames_train_centered, aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test_self_centered, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-training-centered",
    "href": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-training-centered",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Visualizing Training and Test Sets: Training Centered",
    "text": "Visualizing Training and Test Sets: Training Centered\n\n\nOriginal data\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled data\n\n\nCode\names_train_centered &lt;- ames_train |&gt; \n  mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n         Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\names_test_self_centered &lt;- ames_test |&gt; \n  mutate(Gr_Liv_Area_Scaled = (Gr_Liv_Area - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area), \n         Bedroom_AbvGr_Scaled = (Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/ sd(ames_train$Bedroom_AbvGr))\nggplot(ames_train_centered, aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test_self_centered, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#preprocessing-feature-engineering",
    "href": "slides/06-knn-workflows.html#preprocessing-feature-engineering",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Preprocessing + Feature Engineering",
    "text": "Preprocessing + Feature Engineering\n\nPreprocessing: reformatting and transforming data so it can be used for modeling\n\nCentering and scaling\nConverting categorical data into dummy variables or the correct format\nDealing with missing values\n\nFeature Engineering: reformatting and transforming data to improve the performance of your model\n\nCombining variables and dimension reduction techniques\nTransforming variables (e.g. squaring or logging)\n\nFor recommended preprocessing steps see Appendix A of TMWR\nrecipes: combine feature engineering and preprocessing steps into single object that can be applied to different data sets"
  },
  {
    "objectID": "slides/06-knn-workflows.html#model-workflow",
    "href": "slides/06-knn-workflows.html#model-workflow",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Model workflow",
    "text": "Model workflow\n\nStart with raw data\nPreprocessing and feature engineering steps\nFit “model”\nMake predictions using model"
  },
  {
    "objectID": "slides/06-knn-workflows.html#where-does-my-model-start",
    "href": "slides/06-knn-workflows.html#where-does-my-model-start",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Where does my model start?",
    "text": "Where does my model start?\n\nStart with raw data\nModel starts here\n\nPreprocessing and feature engineering steps\nFit “model”\n\nMake predictions using model"
  },
  {
    "objectID": "slides/06-knn-workflows.html#model-assessment",
    "href": "slides/06-knn-workflows.html#model-assessment",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Model Assessment",
    "text": "Model Assessment\n\nStart with raw data\nSplit Data\nUsing training data\n\nPreprocessing and feature engineering steps\nFit “model”\n\nMake predictions on test set\nCompute error metrics"
  },
  {
    "objectID": "slides/06-knn-workflows.html#workflows-and-recipes-in-tidymodels",
    "href": "slides/06-knn-workflows.html#workflows-and-recipes-in-tidymodels",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "workflows and recipes in tidymodels",
    "text": "workflows and recipes in tidymodels\n\ntidymodels provides workflows that combine model fitting with preprocessing steps and consist of:\n\nA model object\nA recipe which combines all of the preprocessing steps"
  },
  {
    "objectID": "slides/06-knn-workflows.html#creating-a-knn-model",
    "href": "slides/06-knn-workflows.html#creating-a-knn-model",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Creating a KNN model",
    "text": "Creating a KNN model\n\nknn10_model &lt;- nearest_neighbor(neighbors = 10) |&gt;   # 10-nn regression\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/06-knn-workflows.html#recipes-in-tidymodels",
    "href": "slides/06-knn-workflows.html#recipes-in-tidymodels",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "recipe’s in tidymodels",
    "text": "recipe’s in tidymodels\n\nCall receipe with R formula and training set to assign features roles (predictor vs. outcome).\nSequence of step_* specifying the list of transformations.\nApply recipe to new data using predict."
  },
  {
    "objectID": "slides/06-knn-workflows.html#center-and-scale-recipe",
    "href": "slides/06-knn-workflows.html#center-and-scale-recipe",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Center and Scale Recipe",
    "text": "Center and Scale Recipe\n\names_preproc &lt;- recipe(Sale_Price ~ Bedroom_AbvGr + Gr_Liv_Area, data = ames_train) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/06-knn-workflows.html#create-workflow",
    "href": "slides/06-knn-workflows.html#create-workflow",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Create workflow",
    "text": "Create workflow\n\nknn10_workflow &lt;- workflow() |&gt; \n  add_model(knn10_model) |&gt; \n  add_recipe(ames_preproc)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#fitting-model",
    "href": "slides/06-knn-workflows.html#fitting-model",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Fitting Model",
    "text": "Fitting Model\n\nknnfit10 &lt;- knn10_workflow |&gt; \n  fit(data = ames_train)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#making-predictions-single-observation",
    "href": "slides/06-knn-workflows.html#making-predictions-single-observation",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Making predictions: Single observation",
    "text": "Making predictions: Single observation\nTest Point: Gr_Liv_area = 2000 square feet, and Bedroom_AbvGr = 3, then\n\nnew_house |&gt; kable()\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\n\n\n\n\n2000\n2\n0.9900212\n-1.041808\n\n\n\n\n# obtain 10-nn prediction\npredict(knnfit10, new_data = new_house) |&gt; kable()\n\n\n\n\n.pred\n\n\n\n\n342204.5"
  },
  {
    "objectID": "slides/06-knn-workflows.html#making-predictions-test-set",
    "href": "slides/06-knn-workflows.html#making-predictions-test-set",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Making Predictions: Test Set",
    "text": "Making Predictions: Test Set\n\n# obtain 10-nn prediction\npredict(knnfit10, new_data = ames_test) |&gt; head() |&gt; kable()\n\n\n\n\n.pred\n\n\n\n\n178219.0\n\n\n167930.0\n\n\n177574.5\n\n\n238122.2\n\n\n126330.0\n\n\n80109.5"
  },
  {
    "objectID": "slides/06-knn-workflows.html#linear-regression-vs-k-nearest-neighbors",
    "href": "slides/06-knn-workflows.html#linear-regression-vs-k-nearest-neighbors",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Linear Regression vs K-Nearest Neighbors",
    "text": "Linear Regression vs K-Nearest Neighbors\n\nLinear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.\nLinear regression works for regression problems (\\(Y\\) numerical), KNN can be used for both regression and classification - i.e. \\(Y\\) qualitative\nLinear regression is interpretable, KNN is not.\nLinear regression can accommodate qualitative predictors and can be extended to include interaction terms as well while KNN does not allow for qualitative predictors\nPerformance: KNN can be pretty good for small \\(p\\), that is, \\(p \\le 4\\) and large \\(n\\). Performance of KNN deteriorates as \\(p\\) increases - curse of dimensionality"
  },
  {
    "objectID": "slides/06-knn-workflows.html#linear-regression",
    "href": "slides/06-knn-workflows.html#linear-regression",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Linear regression",
    "text": "Linear regression\n\n# Best model from last time\nfit3 &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#nn",
    "href": "slides/06-knn-workflows.html#nn",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "5-NN",
    "text": "5-NN\n\nknn5_model &lt;- nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\")\n\nknn5_fit &lt;- workflow() |&gt; \n  add_model(knn5_model) |&gt; \n  add_recipe(ames_preproc) |&gt; \n  fit(data = ames_train)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#making-test-predictions",
    "href": "slides/06-knn-workflows.html#making-test-predictions",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Making test predictions",
    "text": "Making test predictions\n\names_test_preds &lt;- ames_test |&gt; \n  mutate(lr_preds = predict(fit3, new_data = ames_test)$.pred,\n         knn5_preds = predict(knn5_fit, new_data = ames_test)$.pred,\n         knn10_preds = predict(knnfit10, new_data = ames_test)$.pred)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computing-error-metrics-rmse",
    "href": "slides/06-knn-workflows.html#computing-error-metrics-rmse",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Computing Error Metrics: RMSE",
    "text": "Computing Error Metrics: RMSE\n\nrmse(ames_test_preds, estimate = lr_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n52272.26\n\n\n\n\nrmse(ames_test_preds, estimate = knn5_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n50954.43\n\n\n\n\nrmse(ames_test_preds, estimate = knn10_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n48926.63"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computing-error-metrics-r2",
    "href": "slides/06-knn-workflows.html#computing-error-metrics-r2",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Computing Error Metrics: \\(R^2\\)",
    "text": "Computing Error Metrics: \\(R^2\\)\n\nrsq(ames_test_preds, estimate = lr_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.574702\n\n\n\n\nrsq(ames_test_preds, estimate = knn5_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.605208\n\n\n\n\nrsq(ames_test_preds, estimate = knn10_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.6296263\n\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/13-preproc-cont.html#announcements",
    "href": "slides/13-preproc-cont.html#announcements",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Announcements",
    "text": "Announcements\n\nCoyote Connections Thu March 13th 7-9pm\nPlease print out and bring four copies of your resume and cover letter to class on Friday\nDebrief from talk today"
  },
  {
    "objectID": "slides/13-preproc-cont.html#computational-set-up",
    "href": "slides/13-preproc-cont.html#computational-set-up",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\nlibrary(readODS)\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/13-preproc-cont.html#data-different-ames-housing-prices",
    "href": "slides/13-preproc-cont.html#data-different-ames-housing-prices",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Data: Different Ames Housing Prices",
    "text": "Data: Different Ames Housing Prices\nGoal: Predict Sale_Price.\n\names &lt;- read_rds(\"../data/AmesHousing.rds\")\names |&gt; glimpse()\n\nRows: 881\nColumns: 20\n$ Sale_Price    &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, …\n$ Gr_Liv_Area   &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, …\n$ Garage_Type   &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, …\n$ Garage_Cars   &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,…\n$ Garage_Area   &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7…\n$ Street        &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa…\n$ Utilities     &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, …\n$ Pool_Area     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Neighborhood  &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest…\n$ Screen_Porch  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Overall_Qual  &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average…\n$ Lot_Area      &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101…\n$ Lot_Frontage  &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6…\n$ MS_SubClass   &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_…\n$ Misc_Val      &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,…\n$ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7…\n$ First_Flr_SF  &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15…\n$ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11…\n$ Year_Built    &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20…"
  },
  {
    "objectID": "slides/13-preproc-cont.html#today",
    "href": "slides/13-preproc-cont.html#today",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Today",
    "text": "Today\nWell cover some common pre-processing tasks:\n\nDealing with zero-variance (zv) and/or near-zero variance (nzv) variables\nImputing missing entries\nLabel encoding ordinal categorical variables\nStandardizing (centering and scaling) numeric predictors\nLumping predictors\nOne-hot/dummy encoding categorical predictor"
  },
  {
    "objectID": "slides/13-preproc-cont.html#pre-split-cleaning",
    "href": "slides/13-preproc-cont.html#pre-split-cleaning",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Pre-Split Cleaning",
    "text": "Pre-Split Cleaning\n\nBefore you split your data: make sure data is in correct format\nThis may mean different things for different data sets\nCommon examples:\n\nFixing names of columns\nEnsure all variable types are correct\nEnsure all factor levels are correct and in order (if applicable)\nRemove any variables that are not important (or harmful) to your analysis\nEnsure missing values are coded as such (i.e. as NA instead of 0 or -1 or “missing”)\nFilling in missing values where you know what the answer should be (i.e. if a missing value really means 0 instead of missing)"
  },
  {
    "objectID": "slides/13-preproc-cont.html#example-factor-levels-in-wrong-order",
    "href": "slides/13-preproc-cont.html#example-factor-levels-in-wrong-order",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Example: Factor Levels in Wrong Order",
    "text": "Example: Factor Levels in Wrong Order\n\names |&gt; pull(Overall_Qual) |&gt; levels()\n\n [1] \"Above_Average\"  \"Average\"        \"Below_Average\"  \"Excellent\"     \n [5] \"Fair\"           \"Good\"           \"Poor\"           \"Very_Excellent\"\n [9] \"Very_Good\"      \"Very_Poor\""
  },
  {
    "objectID": "slides/13-preproc-cont.html#re-factoring",
    "href": "slides/13-preproc-cont.html#re-factoring",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Re-Factoring",
    "text": "Re-Factoring\n\names &lt;- ames |&gt; \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")))\names |&gt; pull(Overall_Qual) |&gt; levels()\n\n [1] \"Very_Poor\"      \"Poor\"           \"Fair\"           \"Below_Average\" \n [5] \"Average\"        \"Above_Average\"  \"Good\"           \"Very_Good\"     \n [9] \"Excellent\"      \"Very_Excellent\""
  },
  {
    "objectID": "slides/13-preproc-cont.html#zero-variance-zv-andor-near-zero-variance-nzv-variables",
    "href": "slides/13-preproc-cont.html#zero-variance-zv-andor-near-zero-variance-nzv-variables",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables",
    "text": "Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables\nHeuristic for detecting near-zero variance features is:\n\nThe fraction of unique values over the sample size is low (say ≤ 10%).\nThe ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say ≥ 20%).\n\n\nlibrary(caret)\nnearZeroVar(ames, saveMetrics = TRUE) |&gt; kable()\n\n\n\n\n\nfreqRatio\npercentUnique\nzeroVar\nnzv\n\n\n\n\nSale_Price\n1.000000\n55.7321226\nFALSE\nFALSE\n\n\nGr_Liv_Area\n1.333333\n62.9965948\nFALSE\nFALSE\n\n\nGarage_Type\n2.196581\n0.6810443\nFALSE\nFALSE\n\n\nGarage_Cars\n1.970213\n0.5675369\nFALSE\nFALSE\n\n\nGarage_Area\n2.250000\n38.0249716\nFALSE\nFALSE\n\n\nStreet\n219.250000\n0.2270148\nFALSE\nTRUE\n\n\nUtilities\n880.000000\n0.2270148\nFALSE\nTRUE\n\n\nPool_Area\n876.000000\n0.6810443\nFALSE\nTRUE\n\n\nNeighborhood\n1.476744\n2.9511918\nFALSE\nFALSE\n\n\nScreen_Porch\n199.750000\n6.6969353\nFALSE\nTRUE\n\n\nOverall_Qual\n1.119816\n1.1350738\nFALSE\nFALSE\n\n\nLot_Area\n1.071429\n79.7956867\nFALSE\nFALSE\n\n\nLot_Frontage\n1.617021\n11.5777526\nFALSE\nFALSE\n\n\nMS_SubClass\n1.959064\n1.7026107\nFALSE\nFALSE\n\n\nMisc_Val\n141.833333\n1.9296254\nFALSE\nTRUE\n\n\nOpen_Porch_SF\n23.176471\n19.2962543\nFALSE\nFALSE\n\n\nTotRms_AbvGrd\n1.311225\n1.2485812\nFALSE\nFALSE\n\n\nFirst_Flr_SF\n1.777778\n63.7911464\nFALSE\nFALSE\n\n\nSecond_Flr_SF\n64.250000\n31.3280363\nFALSE\nFALSE\n\n\nYear_Built\n1.125000\n12.0317821\nFALSE\nFALSE"
  },
  {
    "objectID": "slides/13-preproc-cont.html#recipe-near-zero-variance",
    "href": "slides/13-preproc-cont.html#recipe-near-zero-variance",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Recipe: Near-Zero Variance",
    "text": "Recipe: Near-Zero Variance\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) # remove zero or near-zero variable predictors"
  },
  {
    "objectID": "slides/13-preproc-cont.html#missing-data",
    "href": "slides/13-preproc-cont.html#missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Missing Data",
    "text": "Missing Data\n\nMany times, you can’t just drop missing data\nEven if you can, dropping missing values can generate biased data/models\nSometimes missing data gives you more information\nTypes of missing data:\n\nMissing completely at random (MCAR): there is no pattern to your missing values\nMissing at random (MAR): missing values are dependent on other values in the data set\nMissing not at random (MNAR): missing values are dependent on the value that is missing\n\nStructured missingness (SM): when the missingness of certain values are depends on one another, regardless of whether the missing values are MCAR, MAR, or MNAR"
  },
  {
    "objectID": "slides/13-preproc-cont.html#mcar-examples",
    "href": "slides/13-preproc-cont.html#mcar-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "MCAR: Examples",
    "text": "MCAR: Examples\n\nSensor data: occasionally sensors break so you’re missing data randomly\nSurvey data: sometimes people just randomly skip questions\nSurvey data: customers are randomly given 5 questions from a bank of 100 questions"
  },
  {
    "objectID": "slides/13-preproc-cont.html#mar-examples",
    "href": "slides/13-preproc-cont.html#mar-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "MAR: Examples",
    "text": "MAR: Examples\n\nMen are less likely to respond to surveys about depression\nMedical study: patients who miss follow-up appointments are more likely to be young\nSurvey responses: ESL respondents may be more likely to skip certain questions that are difficult to interpret (only MAR if you know they are ESL)\nMeasure of student performance: students who score lower are more likely to skip questions"
  },
  {
    "objectID": "slides/13-preproc-cont.html#mnar-examples",
    "href": "slides/13-preproc-cont.html#mnar-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "MNAR: Examples",
    "text": "MNAR: Examples\n\nSurvey on income: respondent may be less likely to report their income if they are poor\nSurvey about political beliefs: respondent may be more likely to skip questions when their answer is perceived as undesirable\nCustomer satisfaction: only customers who feel strongly respond\nMedical study: patients refuse to report unhealthy habits"
  },
  {
    "objectID": "slides/13-preproc-cont.html#structurally-missing-examples",
    "href": "slides/13-preproc-cont.html#structurally-missing-examples",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Structurally Missing: Examples",
    "text": "Structurally Missing: Examples\n\nHealth survey: all questions related to pregnancy are left blank by males\nBank data set: combination of home, auto, and credit cards… not all customer have all three so have missing data in certain portions\nSurvey: many respondents by stop the survey early so all questions after a certain point are missing\nNetflix: customers may only watch similar movies and TV shows"
  },
  {
    "objectID": "slides/13-preproc-cont.html#remedies-for-missing-data",
    "href": "slides/13-preproc-cont.html#remedies-for-missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Remedies for Missing Data",
    "text": "Remedies for Missing Data\n\nLot of complicated ways that you can read about\nCan drop column of too much of the data is missing\nImputing:\n\nstep_impute_median: used for numeric (especially discrete) variables\nstep_impute_mean: used for numeric variables\nstep_impute_knn: used for both numeric and categorical variables (computationally expensive)\nstep_impute_mode: used for nominal (having no order) categorical variable"
  },
  {
    "objectID": "slides/13-preproc-cont.html#exploring-missing-data",
    "href": "slides/13-preproc-cont.html#exploring-missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Exploring Missing Data",
    "text": "Exploring Missing Data\n\names |&gt; \n  summarize(across(everything(), ~ sum(is.na(.)))) |&gt; \n  pivot_longer(everything()) |&gt; \n  filter(value &gt; 0) |&gt; \n  kable()\n\n\n\n\nname\nvalue\n\n\n\n\nGr_Liv_Area\n113\n\n\nGarage_Type\n54\n\n\nYear_Built\n41"
  },
  {
    "objectID": "slides/13-preproc-cont.html#missing-data-garage_type",
    "href": "slides/13-preproc-cont.html#missing-data-garage_type",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Missing Data: Garage_Type",
    "text": "Missing Data: Garage_Type\n\nThe reason that Garage_Type is missing is because there is no basement\n\nSolution: replace NAs with No_Garage\nDo this before data splitting"
  },
  {
    "objectID": "slides/13-preproc-cont.html#fixing-garage_type",
    "href": "slides/13-preproc-cont.html#fixing-garage_type",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Fixing Garage_Type",
    "text": "Fixing Garage_Type\n\names &lt;- ames |&gt; \n  mutate(Garage_Type = as_factor(if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type)))"
  },
  {
    "objectID": "slides/13-preproc-cont.html#missing-data-year_built",
    "href": "slides/13-preproc-cont.html#missing-data-year_built",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Missing Data: Year_Built",
    "text": "Missing Data: Year_Built\n\nMCAR\n\nSolution 1: Impute with mean or median\nSolution 2: Impute with KNN… maybe we can infer what the values are based on other values in the data set?"
  },
  {
    "objectID": "slides/13-preproc-cont.html#missing-data-gr_liv_area",
    "href": "slides/13-preproc-cont.html#missing-data-gr_liv_area",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Missing Data: Gr_Liv_Area",
    "text": "Missing Data: Gr_Liv_Area\n\nMCAR\n\nSolution 1: Impute with mean or median\nSolution 2: Impute with KNN… maybe we can infer what the values are based on other values in the data set?"
  },
  {
    "objectID": "slides/13-preproc-cont.html#recipe-missing-data",
    "href": "slides/13-preproc-cont.html#recipe-missing-data",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Recipe: Missing Data",
    "text": "Recipe: Missing Data\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) # impute missing values in Overall_Qual and Year_Built\n\n\nNote: step_imput_knn uses the “Gower’s Distance” so don’t need to worry about normalizing"
  },
  {
    "objectID": "slides/13-preproc-cont.html#encoding-ordinal-features",
    "href": "slides/13-preproc-cont.html#encoding-ordinal-features",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Encoding Ordinal Features",
    "text": "Encoding Ordinal Features\nTwo types of categorical features:\n\nOrdinal (order is important)\nNominal (order is not important)"
  },
  {
    "objectID": "slides/13-preproc-cont.html#encoding-ordinal-features-1",
    "href": "slides/13-preproc-cont.html#encoding-ordinal-features-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Encoding Ordinal Features",
    "text": "Encoding Ordinal Features\n\names |&gt; pull(Overall_Qual) |&gt; levels()\n\n [1] \"Very_Poor\"      \"Poor\"           \"Fair\"           \"Below_Average\" \n [5] \"Average\"        \"Above_Average\"  \"Good\"           \"Very_Good\"     \n [9] \"Excellent\"      \"Very_Excellent\"\n\n\n\nVery_Poor = 1, Poor = 2, Fair = 3, etc…"
  },
  {
    "objectID": "slides/13-preproc-cont.html#recipe-encoding-ordinal-features",
    "href": "slides/13-preproc-cont.html#recipe-encoding-ordinal-features",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Recipe: Encoding Ordinal Features",
    "text": "Recipe: Encoding Ordinal Features\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) # convert Overall_Qual into ordinal encoding"
  },
  {
    "objectID": "slides/13-preproc-cont.html#lump-small-categories-together",
    "href": "slides/13-preproc-cont.html#lump-small-categories-together",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Lump Small Categories Together",
    "text": "Lump Small Categories Together\n\names |&gt; count(Neighborhood) |&gt; kable()\n\n\n\n\nNeighborhood\nn\n\n\n\n\nNorth_Ames\n127\n\n\nCollege_Creek\n86\n\n\nOld_Town\n83\n\n\nEdwards\n49\n\n\nSomerset\n50\n\n\nNorthridge_Heights\n52\n\n\nGilbert\n47\n\n\nSawyer\n49\n\n\nNorthwest_Ames\n41\n\n\nSawyer_West\n31\n\n\nMitchell\n33\n\n\nBrookside\n33\n\n\nCrawford\n22\n\n\nIowa_DOT_and_Rail_Road\n28\n\n\nTimberland\n21\n\n\nNorthridge\n22\n\n\nStone_Brook\n17\n\n\nSouth_and_West_of_Iowa_State_University\n21\n\n\nClear_Creek\n16\n\n\nMeadow_Village\n14\n\n\nBriardale\n10\n\n\nBloomington_Heights\n10\n\n\nVeenker\n9\n\n\nNorthpark_Villa\n3\n\n\nBlueste\n3\n\n\nGreens\n4"
  },
  {
    "objectID": "slides/13-preproc-cont.html#lump-small-categories-together-1",
    "href": "slides/13-preproc-cont.html#lump-small-categories-together-1",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Lump Small Categories Together",
    "text": "Lump Small Categories Together\n\names |&gt; mutate(Neighborhood = fct_lump_prop(Neighborhood, 0.05)) |&gt; \n  count(Neighborhood) |&gt;  kable()\n\n\n\n\nNeighborhood\nn\n\n\n\n\nNorth_Ames\n127\n\n\nCollege_Creek\n86\n\n\nOld_Town\n83\n\n\nEdwards\n49\n\n\nSomerset\n50\n\n\nNorthridge_Heights\n52\n\n\nGilbert\n47\n\n\nSawyer\n49\n\n\nOther\n338"
  },
  {
    "objectID": "slides/13-preproc-cont.html#recipe-lumping-small-factors-together",
    "href": "slides/13-preproc-cont.html#recipe-lumping-small-factors-together",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Recipe: Lumping Small Factors Together",
    "text": "Recipe: Lumping Small Factors Together\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") # lump all categories with less than 1% representation into a category called Other for each variable"
  },
  {
    "objectID": "slides/13-preproc-cont.html#one-hotdummy-encoding-categorical-predictors",
    "href": "slides/13-preproc-cont.html#one-hotdummy-encoding-categorical-predictors",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "One-hot/dummy encoding categorical predictors",
    "text": "One-hot/dummy encoding categorical predictors\n\nFigure 3.9: Machine Learning with R"
  },
  {
    "objectID": "slides/13-preproc-cont.html#recipe-dummy-variables",
    "href": "slides/13-preproc-cont.html#recipe-dummy-variables",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Recipe: Dummy Variables",
    "text": "Recipe: Dummy Variables\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)  # in general use one_hot unless doing linear regression"
  },
  {
    "objectID": "slides/13-preproc-cont.html#recipe-center-and-scale",
    "href": "slides/13-preproc-cont.html#recipe-center-and-scale",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Recipe: Center and scale",
    "text": "Recipe: Center and scale\n\npreproc &lt;- recipe(Sale_Price ~ ., data = ames) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;  # in general use one_hot unless doing linear regression\n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/13-preproc-cont.html#order-of-preprocessing-step",
    "href": "slides/13-preproc-cont.html#order-of-preprocessing-step",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Order of Preprocessing Step",
    "text": "Order of Preprocessing Step\nQuestions to ask:\n\nShould this be done before or after data splitting?\nIf I do step_A first what is the impact on step_B? For example, do you want to encode categorical variables before or after normalizing?\nWhat data format is required by the model I’m fitting and how will my model react to these changes?\nIs this step part of my “model”? I.e. is this a decision I’m making based on the data or based on subject matter expertise?\nDo I have access to my test predictors?"
  },
  {
    "objectID": "slides/13-preproc-cont.html#questions",
    "href": "slides/13-preproc-cont.html#questions",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Questions",
    "text": "Questions\n\nShould I lump before or after dummy coding?\nShould I dummy code before or after normalizing?\nShould I lump before my initial split?\nHow does ordinal encoding impact linear regression vs. KNN?"
  },
  {
    "objectID": "slides/13-preproc-cont.html#clean-data-set",
    "href": "slides/13-preproc-cont.html#clean-data-set",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Clean Data Set",
    "text": "Clean Data Set\n\names &lt;- ames |&gt; \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")),\n         Garage_Type = if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type),\n         Garage_Type = as_factor(Garage_Type)\n         )"
  },
  {
    "objectID": "slides/13-preproc-cont.html#initial-data-split",
    "href": "slides/13-preproc-cont.html#initial-data-split",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Initial Data Split",
    "text": "Initial Data Split\n\nset.seed(427)\n\ndata_split &lt;- initial_split(ames, strata = \"Sale_Price\")\names_train &lt;- training(data_split)\names_test  &lt;- testing(data_split)"
  },
  {
    "objectID": "slides/13-preproc-cont.html#define-folds",
    "href": "slides/13-preproc-cont.html#define-folds",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Define Folds",
    "text": "Define Folds\n\names_folds &lt;- vfold_cv(ames_train, v = 10, repeats = 10)\names_folds\n\n#  10-fold cross-validation repeated 10 times \n# A tibble: 100 × 3\n   splits           id       id2   \n   &lt;list&gt;           &lt;chr&gt;    &lt;chr&gt; \n 1 &lt;split [594/66]&gt; Repeat01 Fold01\n 2 &lt;split [594/66]&gt; Repeat01 Fold02\n 3 &lt;split [594/66]&gt; Repeat01 Fold03\n 4 &lt;split [594/66]&gt; Repeat01 Fold04\n 5 &lt;split [594/66]&gt; Repeat01 Fold05\n 6 &lt;split [594/66]&gt; Repeat01 Fold06\n 7 &lt;split [594/66]&gt; Repeat01 Fold07\n 8 &lt;split [594/66]&gt; Repeat01 Fold08\n 9 &lt;split [594/66]&gt; Repeat01 Fold09\n10 &lt;split [594/66]&gt; Repeat01 Fold10\n# ℹ 90 more rows"
  },
  {
    "objectID": "slides/13-preproc-cont.html#define-models",
    "href": "slides/13-preproc-cont.html#define-models",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Define Model(s)",
    "text": "Define Model(s)\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine('lm')\n\nknn5_model &lt;- nearest_neighbor(neighbors = 5) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")\n\nknn10_model &lt;- nearest_neighbor(neighbors = 10) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/13-preproc-cont.html#define-preprocessing-linear-regression",
    "href": "slides/13-preproc-cont.html#define-preprocessing-linear-regression",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Define Preprocessing: Linear regression",
    "text": "Define Preprocessing: Linear regression\n\nlm_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt;  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |&gt; # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations"
  },
  {
    "objectID": "slides/13-preproc-cont.html#define-preprocessing-knn",
    "href": "slides/13-preproc-cont.html#define-preprocessing-knn",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Define Preprocessing: KNN",
    "text": "Define Preprocessing: KNN\n\nknn_preproc &lt;- recipe(Sale_Price ~ ., data = ames_train) |&gt; \n  step_nzv(all_predictors()) |&gt;  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |&gt;  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |&gt; # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |&gt; # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/13-preproc-cont.html#define-workflows",
    "href": "slides/13-preproc-cont.html#define-workflows",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Define Workflows",
    "text": "Define Workflows\n\nlm_wf &lt;- workflow() |&gt; add_model(lm_model) |&gt; add_recipe(lm_preproc)\nknn5_wf &lt;- workflow() |&gt; add_model(knn5_model) |&gt; add_recipe(knn_preproc)\nknn10_wf &lt;- workflow() |&gt; add_model(knn10_model) |&gt; add_recipe(knn_preproc)"
  },
  {
    "objectID": "slides/13-preproc-cont.html#define-metrics",
    "href": "slides/13-preproc-cont.html#define-metrics",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Define Metrics",
    "text": "Define Metrics\n\names_metrics &lt;- metric_set(rmse, rsq)"
  },
  {
    "objectID": "slides/13-preproc-cont.html#fit-and-assess-models",
    "href": "slides/13-preproc-cont.html#fit-and-assess-models",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Fit and Assess Models",
    "text": "Fit and Assess Models\n\nlm_results &lt;- lm_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn5_results &lt;- knn5_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn10_results &lt;- knn10_wf |&gt; fit_resamples(resamples = ames_folds, metrics = ames_metrics)"
  },
  {
    "objectID": "slides/13-preproc-cont.html#collecting-metrics",
    "href": "slides/13-preproc-cont.html#collecting-metrics",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Collecting Metrics",
    "text": "Collecting Metrics\n\ncollect_metrics(lm_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.979336e+04\n100\n768.846396\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.548961e-01\n100\n0.008863\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn5_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n40063.355604\n100\n963.9908286\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.758572\n100\n0.0063228\nPreprocessor1_Model1\n\n\n\n\ncollect_metrics(knn10_results) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.957217e+04\n100\n1011.2439632\nPreprocessor1_Model1\n\n\nrsq\nstandard\n7.673612e-01\n100\n0.0065484\nPreprocessor1_Model1"
  },
  {
    "objectID": "slides/13-preproc-cont.html#final-evaluate-final-model",
    "href": "slides/13-preproc-cont.html#final-evaluate-final-model",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Final & Evaluate Final Model",
    "text": "Final & Evaluate Final Model\n\nAfter choosing best model/workflow, fit on full training set and assess on test set\n\n\nfinal_fit &lt;- knn10_wf |&gt; fit(data = ames_train)\nfinal_fit_perf &lt;- final_fit |&gt; \n  predict(new_data = ames_test) |&gt; \n  bind_cols(ames_test) |&gt; \n  ames_metrics(truth = Sale_Price, estimate = .pred)\n\nfinal_fit_perf |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n4.947411e+04\n\n\nrsq\nstandard\n7.174733e-01"
  },
  {
    "objectID": "slides/13-preproc-cont.html#tips",
    "href": "slides/13-preproc-cont.html#tips",
    "title": "MATH 427: Preprocessing, Missing Data, and Resampling Continued",
    "section": "Tips",
    "text": "Tips\n\nCan try out different pre-processing to see if it improves your model!\nProcess can be intense for you computer, so might take a while\nNo 100% correct way to do it, although there are some 100% incorrect ways to do it\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#computational-set-up",
    "href": "slides/18-regularization-tuning.html#computational-set-up",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Computational Set-Up",
    "text": "Computational Set-Up\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(fivethirtyeight) # for candy rankings data\n\ntidymodels_prefer()\n\nset.seed(427)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#data-candy",
    "href": "slides/18-regularization-tuning.html#data-candy",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data for this lecture comes from the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy match-ups (e.g. Reese’s vs. Skittles). Click here to check out some of the match ups.\nThe data set contains 12 characteristics and win percentage from 85 candies in the experiment."
  },
  {
    "objectID": "slides/18-regularization-tuning.html#data-candy-1",
    "href": "slides/18-regularization-tuning.html#data-candy-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data: Candy",
    "text": "Data: Candy\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#data-cleaning",
    "href": "slides/18-regularization-tuning.html#data-cleaning",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\ncandy_rankings_clean &lt;- candy_rankings |&gt; \n  select(-competitorname) |&gt; \n  mutate(sugarpercent = sugarpercent*100, # convert proportions into percentages\n         pricepercent = pricepercent*100, # convert proportions into percentages\n         across(where(is.logical), ~ factor(.x, levels = c(\"FALSE\", \"TRUE\")))) # convert logicals into factors"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#data-cleaning-1",
    "href": "slides/18-regularization-tuning.html#data-cleaning-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nglimpse(candy_rankings_clean)\n\nRows: 85\nColumns: 12\n$ chocolate        &lt;fct&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;fct&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;fct&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;fct&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;fct&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 73.2, 60.4, 1.1, 1.1, 90.6, 46.5, 60.4, 31.3, 90.6, 6…\n$ pricepercent     &lt;dbl&gt; 86.0, 51.1, 11.6, 51.1, 51.1, 76.7, 76.7, 51.1, 32.5,…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#data-splitting",
    "href": "slides/18-regularization-tuning.html#data-splitting",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Data Splitting",
    "text": "Data Splitting\n\ncandy_split &lt;- initial_split(candy_rankings_clean, strata = winpercent)\ncandy_train &lt;- training(candy_split)\ncandy_test &lt;- testing(candy_split)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#tuning-parameters",
    "href": "slides/18-regularization-tuning.html#tuning-parameters",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Tuning Parameters",
    "text": "Tuning Parameters\n\nTuning Parameters or Hyperparameters are parameters that cannot (or should not) be estimated when a model is being trained\nThese parameters control something about the learning process and changing them will result in a different model when fit to the full training data (i.e. after cross-validation)\nFrequently: tuning parameters control model complexity\nExample: \\(\\lambda\\) in LASSO and Ridge regression\nToday: How to choose our tuning parameters?"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#question",
    "href": "slides/18-regularization-tuning.html#question",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Question",
    "text": "Question\n\nWhich of the following are tuning parameters:\n\n\\(\\beta_0\\): the intercept of linear regression\n\\(k\\) in KNN\nstep size in gradient descent\nThe number of folds in cross-validation\nType of distance to use in KNN (i.e. rectangular vs. Gower’s vs. weighted etc)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#basic-idea",
    "href": "slides/18-regularization-tuning.html#basic-idea",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Basic Idea",
    "text": "Basic Idea\n\nUse CV to try out a bunch of different tuning parameters and choose the “best” one\nHow do we choose which tuning parameters to try?\nTwo general approaches:\n\nGrid Search\nIterative Search"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#grid-search-1",
    "href": "slides/18-regularization-tuning.html#grid-search-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Grid Search",
    "text": "Grid Search\n\nCreate a grid of tuning parameters and try out each combination\nTypes of grids:\n\nRegular Grid: tuning parameter values are spaced deterministically using a linear or logarithmic scale and all combinations of parameters are used (mostly what you want to use in this class)\nIrregular Grids: tuning parameter values are chosen stochastically\n\nUse when you have A LOT of parameters"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#grid-search-in-r",
    "href": "slides/18-regularization-tuning.html#grid-search-in-r",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Grid Search in R",
    "text": "Grid Search in R\n\nTake advantage of package dials which is part of the tidyverse\n\nSet every tuning variable equal to tune()"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#lasso-and-ridge-in-r",
    "href": "slides/18-regularization-tuning.html#lasso-and-ridge-in-r",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "LASSO and Ridge in R",
    "text": "LASSO and Ridge in R\n\nols &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n\nridge &lt;- linear_reg(mixture = 0, penalty = tune()) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")\n\nlasso &lt;- linear_reg(mixture = 1, penalty = tune()) |&gt; # penalty set's our lambda\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#create-recipe",
    "href": "slides/18-regularization-tuning.html#create-recipe",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Create Recipe",
    "text": "Create Recipe\n\nNote: no tuning variables in this case\n\n\nlm_preproc &lt;- recipe(winpercent ~ . , data = candy_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#create-workflows",
    "href": "slides/18-regularization-tuning.html#create-workflows",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Create workflows",
    "text": "Create workflows\n\ngeneric_wf &lt;- workflow() |&gt; add_recipe(lm_preproc)\nols_wf &lt;- generic_wf |&gt; add_model(ols)\nridge_wf &lt;- generic_wf |&gt; add_model(ridge)\nlasso_wf &lt;- generic_wf |&gt; add_model(lasso)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#create-metric-set",
    "href": "slides/18-regularization-tuning.html#create-metric-set",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Create Metric Set",
    "text": "Create Metric Set\n\ncandy_metrics &lt;- metric_set(rmse, rsq)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#create-folds",
    "href": "slides/18-regularization-tuning.html#create-folds",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Create Folds",
    "text": "Create Folds\n\n# Since sample size is so small, not using stratification\ncandy_folds &lt;- vfold_cv(candy_train, v = 2, repeats = 10)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#grid-search-in-r-1",
    "href": "slides/18-regularization-tuning.html#grid-search-in-r-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Grid Search in R",
    "text": "Grid Search in R\n\nTake advantage of package dials which is part of the tidyverse\n\nSet every tuning variable equal to tune()\nGenerate grid for hyperparameters"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#generate-grid",
    "href": "slides/18-regularization-tuning.html#generate-grid",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Generate Grid",
    "text": "Generate Grid\n\n# note that dials treats penalty on a log10 scale\npenalty_grid &lt;- grid_regular(penalty(range = c(-10, 2)), # I had to play around with these \n                             levels = 10)\n\n\n\n\npenalty_grid |&gt; head() |&gt;  kable(digits = 11, format.args = list(scientific = TRUE))\n\n\n\n\npenalty\n\n\n\n\n1.000000e-10\n\n\n2.150000e-09\n\n\n4.642000e-08\n\n\n1.000000e-06\n\n\n2.154435e-05\n\n\n4.641589e-04\n\n\n\n\n\n\n\npenalty_grid |&gt; tail() |&gt;  kable()\n\n\n\n\npenalty\n\n\n\n\n0.0000215\n\n\n0.0004642\n\n\n0.0100000\n\n\n0.2154435\n\n\n4.6415888\n\n\n100.0000000"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#regular-grid-search-in-r",
    "href": "slides/18-regularization-tuning.html#regular-grid-search-in-r",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Regular Grid Search in R",
    "text": "Regular Grid Search in R\n\nTake advantage of package dials which is part of the tidyverse\n\nSet every tuning variable equal to tune()\nGenerate grid for hyperparameters using grid_regular\nTune your model: fit all hyperparameter combination on resamples using tune_grid"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#tune-models",
    "href": "slides/18-regularization-tuning.html#tune-models",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Tune Models",
    "text": "Tune Models\n\ntuning_ridge_results &lt;- tune_grid(\n  ridge_wf,\n  resamples= candy_folds,\n  grid = penalty_grid\n)\n\ntuning_lasso_results &lt;- tune_grid(\n  lasso_wf,\n  resamples = candy_folds,\n  grid = penalty_grid\n)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#regular-grid-search-in-r-1",
    "href": "slides/18-regularization-tuning.html#regular-grid-search-in-r-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Regular Grid Search in R",
    "text": "Regular Grid Search in R\n\nTake advantage of package dials which is part of the tidyverse\n\nSet every tuning variable equal to tune()\nGenerate grid for hyperparameters using grid_regular\nTune your model: fit all hyperparameter combination on resamples using tune_grid\nVisualize and Choose final model"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#visualizing-results-ridge",
    "href": "slides/18-regularization-tuning.html#visualizing-results-ridge",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Visualizing Results: Ridge",
    "text": "Visualizing Results: Ridge\n\nautoplot(tuning_ridge_results)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#selecting-best-model-ridge",
    "href": "slides/18-regularization-tuning.html#selecting-best-model-ridge",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Selecting Best Model: Ridge",
    "text": "Selecting Best Model: Ridge\n\n\nBest RMSE\n\nbest_rmse_ridge &lt;- tuning_ridge_results |&gt; \n  select_best(metric = \"rmse\")\nbest_rmse_ridge |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n4.64\nPreprocessor1_Model09\n\n\n\n\n\n\nBest \\(R^2\\)\n\nbest_rsq_ridge &lt;- tuning_ridge_results |&gt; \n  select_best(metric = \"rsq\",)\nbest_rsq_ridge |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n100\nPreprocessor1_Model10\n\n\n\n\n\n\n\nWhich should be use?"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#finalize-model",
    "href": "slides/18-regularization-tuning.html#finalize-model",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Finalize Model",
    "text": "Finalize Model\n\nRMSE estimate is less flexible and seems to be sacrificing less \\(R^2\\) than the opposite\n\n\nridge_rmse_final &lt;- finalize_workflow(ridge_wf, best_rmse_ridge)\nridge_rmse_fit &lt;- fit(ridge_rmse_final, data = candy_train)\ntidy(ridge_rmse_fit) |&gt; \n  kable()\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n49.8221817\n4.641589\n\n\nsugarpercent\n1.6105235\n4.641589\n\n\npricepercent\n-0.8755351\n4.641589\n\n\nchocolate_TRUE.\n5.5626786\n4.641589\n\n\nfruity_TRUE.\n0.8290067\n4.641589\n\n\ncaramel_TRUE.\n0.9577679\n4.641589\n\n\npeanutyalmondy_TRUE.\n2.8412296\n4.641589\n\n\nnougat_TRUE.\n0.5018516\n4.641589\n\n\ncrispedricewafer_TRUE.\n1.6202899\n4.641589\n\n\nhard_TRUE.\n-1.2156466\n4.641589\n\n\nbar_TRUE.\n1.4669984\n4.641589\n\n\npluribus_TRUE.\n0.4265386\n4.641589"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#visualizing-results-lasso",
    "href": "slides/18-regularization-tuning.html#visualizing-results-lasso",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Visualizing Results: LASSO",
    "text": "Visualizing Results: LASSO\n\nautoplot(tuning_lasso_results)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#selecting-best-model-lasso",
    "href": "slides/18-regularization-tuning.html#selecting-best-model-lasso",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Selecting Best Model: LASSO",
    "text": "Selecting Best Model: LASSO\n\n\nBest RMSE\n\nbest_rmse_lasso &lt;- tuning_lasso_results |&gt; \n  select_best(metric = \"rmse\")\nbest_rmse_lasso |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n4.64\nPreprocessor1_Model09\n\n\n\n\n\n\nBest \\(R^2\\)\n\nbest_rsq_lasso &lt;- tuning_lasso_results |&gt; \n  select_best(metric = \"rsq\")\nbest_rsq_lasso |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n4.64\nPreprocessor1_Model09\n\n\n\n\n\n\n\nWhich should be use?"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#finalize-model-1",
    "href": "slides/18-regularization-tuning.html#finalize-model-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Finalize Model",
    "text": "Finalize Model\n\nRMSE estimate is less flexible and seems to be sacrificing less \\(R^2\\) than the opposite\n\n\nlasso_rmse_final &lt;- finalize_workflow(lasso_wf, best_rmse_lasso)\nlasso_rmse_fit &lt;- fit(lasso_rmse_final, data = candy_train)\ntidy(lasso_rmse_fit) |&gt; \n  filter(estimate != 0) |&gt; \n  kable()\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n49.822182\n4.641589\n\n\nchocolate_TRUE.\n5.165031\n4.641589"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#using-parsimony-as-a-tie-breaker",
    "href": "slides/18-regularization-tuning.html#using-parsimony-as-a-tie-breaker",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Using Parsimony as a Tie-Breaker",
    "text": "Using Parsimony as a Tie-Breaker\n\nGood heuristic: One-Standard Error Rule\n\nUse resampling to estimate error metrics\nCompute standard error for error metrics\nSelect most parsimonious model that is within one standard error of the best performance metric"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#selecting-best-model-ridge-1",
    "href": "slides/18-regularization-tuning.html#selecting-best-model-ridge-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Selecting Best Model: Ridge",
    "text": "Selecting Best Model: Ridge\n\n\nBest RMSE\n\nbest_ose_rmse_ridge &lt;- tuning_ridge_results |&gt; \n  select_by_one_std_err(metric = \"rmse\", desc(penalty)) # why are we descending\nbest_ose_rmse_ridge |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n4.64\nPreprocessor1_Model09\n\n\n\n\n\n\nBest \\(R^2\\)\n\nbse_ose_rsq_ridge &lt;- tuning_ridge_results |&gt; \n  select_by_one_std_err(metric = \"rsq\", desc(penalty))\nbse_ose_rsq_ridge |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n100\nPreprocessor1_Model10\n\n\n\n\n\n\n\nWhich should be use?"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#visualizing-results-ridge-1",
    "href": "slides/18-regularization-tuning.html#visualizing-results-ridge-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Visualizing Results: Ridge",
    "text": "Visualizing Results: Ridge\n\nautoplot(tuning_ridge_results)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#finalize-model-2",
    "href": "slides/18-regularization-tuning.html#finalize-model-2",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Finalize Model",
    "text": "Finalize Model\n\nRMSE estimate is less flexible and seems to be sacrificing less \\(R^2\\) than the opposite\n\n\nridge_rmse_final &lt;- finalize_workflow(ridge_wf, best_ose_rmse_ridge)\nridge_rmse_fit &lt;- fit(ridge_rmse_final, data = candy_train)\ntidy(ridge_rmse_fit) |&gt; \n  kable()\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n49.8221817\n4.641589\n\n\nsugarpercent\n1.6105235\n4.641589\n\n\npricepercent\n-0.8755351\n4.641589\n\n\nchocolate_TRUE.\n5.5626786\n4.641589\n\n\nfruity_TRUE.\n0.8290067\n4.641589\n\n\ncaramel_TRUE.\n0.9577679\n4.641589\n\n\npeanutyalmondy_TRUE.\n2.8412296\n4.641589\n\n\nnougat_TRUE.\n0.5018516\n4.641589\n\n\ncrispedricewafer_TRUE.\n1.6202899\n4.641589\n\n\nhard_TRUE.\n-1.2156466\n4.641589\n\n\nbar_TRUE.\n1.4669984\n4.641589\n\n\npluribus_TRUE.\n0.4265386\n4.641589"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#visualizing-results-lasso-1",
    "href": "slides/18-regularization-tuning.html#visualizing-results-lasso-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Visualizing Results: LASSO",
    "text": "Visualizing Results: LASSO\n\nautoplot(tuning_lasso_results)"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#selecting-best-model-ridge-2",
    "href": "slides/18-regularization-tuning.html#selecting-best-model-ridge-2",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Selecting Best Model: Ridge",
    "text": "Selecting Best Model: Ridge\n\n\nBest RMSE\n\nbse_ose_rmse_lasso &lt;- tuning_lasso_results |&gt; \n  select_by_one_std_err(metric = \"rmse\", desc(penalty)) # why are we descending\nbse_ose_rmse_lasso |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n4.64\nPreprocessor1_Model09\n\n\n\n\n\n\nBest \\(R^2\\)\n\nbse_ose_rsq_lasso &lt;- tuning_lasso_results |&gt; \n  select_by_one_std_err(metric = \"rsq\", desc(penalty)) # why are we descending\nbse_ose_rsq_lasso |&gt; kable(digits = 2)\n\n\n\n\npenalty\n.config\n\n\n\n\n4.64\nPreprocessor1_Model09\n\n\n\n\n\n\n\nWhich should be use?"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#finalize-model-3",
    "href": "slides/18-regularization-tuning.html#finalize-model-3",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Finalize Model",
    "text": "Finalize Model\n\nRMSE estimate is less flexible and seems to be sacrificing less \\(R^2\\) than the opposite\n\n\nlasso_rmse_final &lt;- finalize_workflow(lasso_wf, bse_ose_rmse_lasso)\nlasso_rmse_fit &lt;- fit(lasso_rmse_final, data = candy_train)\ntidy(lasso_rmse_fit) |&gt; \n  filter(estimate != 0) |&gt; \n  kable()\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n49.822182\n4.641589\n\n\nchocolate_TRUE.\n5.165031\n4.641589"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#using-the-test-set-as-a-tie-breaker",
    "href": "slides/18-regularization-tuning.html#using-the-test-set-as-a-tie-breaker",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Using the Test Set as a Tie Breaker",
    "text": "Using the Test Set as a Tie Breaker\n\nOnce you’ve found you “best” candidate from several different classes of model, it’s ok to compare on test set\nIn this case, we have our best ridge and our best lasso model\nMain this to avoid… LOTS of comparisons on your test set"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#using-the-test-set-as-a-tie-breaker-1",
    "href": "slides/18-regularization-tuning.html#using-the-test-set-as-a-tie-breaker-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Using the Test Set as a Tie Breaker",
    "text": "Using the Test Set as a Tie Breaker\n\ncandy_test_wpreds &lt;- candy_test |&gt; \n  mutate(ridge_preds = predict(ridge_rmse_fit, new_data = candy_test)$.pred,\n         lasso_preds = predict(lasso_rmse_fit, new_data = candy_test)$.pred)\ncandy_test_wpreds |&gt; rmse(estimate = ridge_preds, truth = winpercent)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        10.6\n\ncandy_test_wpreds |&gt; rmse(estimate = lasso_preds, truth = winpercent)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        12.0\n\ncandy_test_wpreds |&gt; rsq(estimate = ridge_preds, truth = winpercent)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.476\n\ncandy_test_wpreds |&gt; rsq(estimate = lasso_preds, truth = winpercent)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.341\n\n\n\nRidge Wins!"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#good-strategies",
    "href": "slides/18-regularization-tuning.html#good-strategies",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Good strategies",
    "text": "Good strategies\n\nFirst use coarse grid with large range to find general area\n\nThen use fine grid with small range to fine tune\nUse iterative method to fine tune"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#brief-overview",
    "href": "slides/18-regularization-tuning.html#brief-overview",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Brief Overview",
    "text": "Brief Overview\n\nBasic Idea: iterative select parameter values to choose based on how previous ones have done\nTMWR gives two examples:\n\nBayesian search\nStochastic Annealing\n\nBoth require an understanding of multivariate probability distributions"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-gp",
    "href": "slides/18-regularization-tuning.html#bayesian-search-gp",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: GP",
    "text": "Bayesian Search: GP\n\n\nAssume performance metrics follow a Gaussian process… dafuq\nConsider two sets of tuning parameters: \\(x_1 = (\\lambda_1, k_1)\\) and \\(x_2 = (\\lambda_2, k_2)\\)\nLet \\(Y_1\\) and \\(Y_2\\) be random variables representing the performance metric at these \\(x\\)’s\nAssume \\(\\text{Cov}(Y_1, Y_2)\\) depends on how far apart \\(x_1\\) and \\(x_2\\)\n\nCovariance should decrease the further apart \\(x_1\\) and \\(x_2\\) are\nThis covariance function is typically parameterized and estimated from an initial tuning grid"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-acquisition-function",
    "href": "slides/18-regularization-tuning.html#bayesian-search-acquisition-function",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: Acquisition Function",
    "text": "Bayesian Search: Acquisition Function\n\n\nGP process gives us predicted mean and variance of performance metrics\nImagine two scenario’s:\n\n1: Predicted mean is slightly better than current parameter choice and variance is low\n\nLow-Risk, Low-Reward\n\n2: Predicted mean is slightly worst than current parameter choice but variance is high\n\nHigh-Risk, High-Reward\n\n\nHow do we balance?\nTwo competing goals: Exploration (got toward high variance) and Exploitation (go toward best mean)\nAcquisition function: balances these two goals (several to choose from)\ntmwr"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nridge_params &lt;- ridge_wf |&gt; \n  extract_parameter_set_dials() |&gt; \n  update(penalty = penalty(c(-2, 1)))\n\nbayes_ridge &lt;- ridge_wf |&gt; \n  tune_bayes(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metrics is what's optimized (rmse in this case)\n    initial = tuning_ridge_results,\n    param_info = ridge_params,\n    iter = 25\n  )"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-1",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nautoplot(bayes_ridge, type = \"performance\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-2",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-2",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nautoplot(bayes_ridge, type = \"parameters\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-3",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-3",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nshow_best(bayes_ridge) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n\n9.999186\nrmse\nstandard\n13.20055\n20\n0.2720582\nIter11\n11\n\n\n9.999034\nrmse\nstandard\n13.20055\n20\n0.2720587\nIter8\n8\n\n\n9.996720\nrmse\nstandard\n13.20057\n20\n0.2720676\nIter7\n7\n\n\n9.996350\nrmse\nstandard\n13.20057\n20\n0.2720690\nIter9\n9\n\n\n9.995326\nrmse\nstandard\n13.20058\n20\n0.2720729\nIter10\n10"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-4",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-4",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nlasso_params &lt;- lasso_wf |&gt; \n  extract_parameter_set_dials() |&gt; \n  update(penalty = penalty(c(-2,1)))\n\nbayes_lasso &lt;- lasso_wf |&gt; \n  tune_bayes(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metrics is what's optimized (rmse in this case)\n    initial = tuning_lasso_results,\n    param_info = lasso_params,\n    iter = 25\n  )"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-5",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-5",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nautoplot(bayes_lasso, type = \"performance\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-6",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-6",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nautoplot(bayes_lasso, type = \"parameters\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-7",
    "href": "slides/18-regularization-tuning.html#bayesian-search-tidymodels-7",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Bayesian Search: tidymodels",
    "text": "Bayesian Search: tidymodels\n\nshow_best(bayes_lasso) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n\n2.936401\nrmse\nstandard\n12.76520\n20\n0.3059454\nIter16\n16\n\n\n2.937813\nrmse\nstandard\n12.76520\n20\n0.3059406\nIter20\n20\n\n\n2.934824\nrmse\nstandard\n12.76520\n20\n0.3059506\nIter19\n19\n\n\n2.940832\nrmse\nstandard\n12.76521\n20\n0.3059305\nIter25\n25\n\n\n2.932606\nrmse\nstandard\n12.76521\n20\n0.3059580\nIter22\n22"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing",
    "href": "slides/18-regularization-tuning.html#simulated-annealing",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n\n\nDoes anyone know what annealing is in Physics/Material Science?\nStart with initial parameter combination \\(x_0\\)(think gradient descent)\nEmbark on random walk… in each step \\(i\\)\n\nConsider small perturbation from \\(x_{i-1}\\)… let’s call it \\(x_{i-1}^{\\epsilon}\\)\nGet performance for \\(x_{i-1}^\\epsilon\\)\nIf performance is better set \\(x_i = x_{i-1}^\\epsilon\\)\nIf performance is worse set \\(x_i = x_{i-1}^\\epsilon\\) with probability \\(\\exp(-c\\times D_i\\times i)\\)\n\n\\(c\\) is user specified constant called cooling coefficient\n\\(D_i\\) percent difference between old and new\n\nOtherwise \\(x_i = x_{i-1}\\)\n\nIdea:\n\nstart hot: likely to accept suboptimal parameter combinations and move around\ncools over time: decrease number sub-optimal acceptances"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing: tidymodels",
    "text": "Simulated Annealing: tidymodels\n\nlibrary(finetune)\nridge_sim_anneal &lt;- ridge_wf |&gt; \n  tune_sim_anneal(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metric is what's optimized (rmse in this case)\n    initial = tuning_ridge_results,\n    param_info = ridge_params,\n    iter = 50\n  )"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-1",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-1",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing tidymodels",
    "text": "Simulated Annealing tidymodels\n\nautoplot(ridge_sim_anneal, type = \"performance\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-2",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-2",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing tidymodels",
    "text": "Simulated Annealing tidymodels\n\nautoplot(ridge_sim_anneal, type = \"parameters\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-3",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-3",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing tidymodels",
    "text": "Simulated Annealing tidymodels\n\nshow_best(ridge_sim_anneal) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n\n10.000000\nrmse\nstandard\n13.20054\n20\n0.2720551\nIter19\n19\n\n\n9.513817\nrmse\nstandard\n13.20542\n20\n0.2739433\nIter42\n42\n\n\n9.079220\nrmse\nstandard\n13.21026\n20\n0.2758191\nIter31\n31\n\n\n8.818478\nrmse\nstandard\n13.21363\n20\n0.2770621\nIter9\n9\n\n\n8.415657\nrmse\nstandard\n13.21916\n20\n0.2790967\nIter45\n45"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-4",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-4",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing: tidymodels",
    "text": "Simulated Annealing: tidymodels\n\nlasso_sim_anneal &lt;- ridge_wf |&gt; \n  tune_sim_anneal(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metrics is what's optimized (rmse in this case)\n    initial = tuning_lasso_results,\n    param_info = lasso_params,\n    iter = 50\n  )"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-5",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-5",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing tidymodels",
    "text": "Simulated Annealing tidymodels\n\nautoplot(lasso_sim_anneal, type = \"performance\")"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-6",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-6",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing tidymodels",
    "text": "Simulated Annealing tidymodels\n\nautoplot(lasso_sim_anneal, type = \"parameters\") +\n  scale_x_continuous(trans = 'log10')"
  },
  {
    "objectID": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-7",
    "href": "slides/18-regularization-tuning.html#simulated-annealing-tidymodels-7",
    "title": "MATH 427: Regularization & Model Tuning",
    "section": "Simulated Annealing tidymodels",
    "text": "Simulated Annealing tidymodels\n\nshow_best(lasso_sim_anneal) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n.iter\n\n\n\n\n4.641589\nrmse\nstandard\n13.09389\n20\n0.3774972\ninitial_Preprocessor1_Model09\n0\n\n\n9.934398\nrmse\nstandard\n13.20114\n20\n0.2723052\nIter39\n39\n\n\n9.189444\nrmse\nstandard\n13.20896\n20\n0.2752983\nIter25\n25\n\n\n8.786187\nrmse\nstandard\n13.21407\n20\n0.2772191\nIter37\n37\n\n\n8.333199\nrmse\nstandard\n13.22037\n20\n0.2795509\nIter26\n26\n\n\n\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "prepare/prep-24.html",
    "href": "prepare/prep-24.html",
    "title": "Preparation for Classification",
    "section": "",
    "text": "Re-read 9.4 of TMWF"
  },
  {
    "objectID": "prepare/prep-24.html#assigned-readings-videos",
    "href": "prepare/prep-24.html#assigned-readings-videos",
    "title": "Preparation for Classification",
    "section": "",
    "text": "Re-read 9.4 of TMWF"
  },
  {
    "objectID": "prepare/prep-25.html",
    "href": "prepare/prep-25.html",
    "title": "Preparation for Bagging and Boosting",
    "section": "",
    "text": "Read 8.2 of ISLR"
  },
  {
    "objectID": "prepare/prep-25.html#assigned-readings-videos",
    "href": "prepare/prep-25.html#assigned-readings-videos",
    "title": "Preparation for Bagging and Boosting",
    "section": "",
    "text": "Read 8.2 of ISLR"
  },
  {
    "objectID": "prepare/prep-14.html",
    "href": "prepare/prep-14.html",
    "title": "Preparation for Resume and Cover Letter Review",
    "section": "",
    "text": "Review the Resume and Cover Letter Rubrics\nBring four print-outs of your Resume and Cover Letter to class\nCatch-up on any reading you haven’t done"
  },
  {
    "objectID": "prepare/prep-14.html#assigned-readings-videos",
    "href": "prepare/prep-14.html#assigned-readings-videos",
    "title": "Preparation for Resume and Cover Letter Review",
    "section": "",
    "text": "Review the Resume and Cover Letter Rubrics\nBring four print-outs of your Resume and Cover Letter to class\nCatch-up on any reading you haven’t done"
  },
  {
    "objectID": "prepare/prep-22.html",
    "href": "prepare/prep-22.html",
    "title": "Preparation for Classification Trees",
    "section": "",
    "text": "Re-read 9.4 of TMWF"
  },
  {
    "objectID": "prepare/prep-22.html#assigned-readings-videos",
    "href": "prepare/prep-22.html#assigned-readings-videos",
    "title": "Preparation for Classification Trees",
    "section": "",
    "text": "Re-read 9.4 of TMWF"
  },
  {
    "objectID": "prepare/prep-29.html",
    "href": "prepare/prep-29.html",
    "title": "Preparation for Imbalanced Classes",
    "section": "",
    "text": "SMOTE Paper"
  },
  {
    "objectID": "prepare/prep-29.html#assigned-readings-videos",
    "href": "prepare/prep-29.html#assigned-readings-videos",
    "title": "Preparation for Imbalanced Classes",
    "section": "",
    "text": "SMOTE Paper"
  },
  {
    "objectID": "prepare/prep-06.html",
    "href": "prepare/prep-06.html",
    "title": "Preparation for KNN and Preprocessing",
    "section": "",
    "text": "Read Chapters 8 of TMWR"
  },
  {
    "objectID": "prepare/prep-06.html#assigned-readings-videos",
    "href": "prepare/prep-06.html#assigned-readings-videos",
    "title": "Preparation for KNN and Preprocessing",
    "section": "",
    "text": "Read Chapters 8 of TMWR"
  },
  {
    "objectID": "prepare/prep-01.html",
    "href": "prepare/prep-01.html",
    "title": "Preparation for Big Picture Discussion",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 1 from ISLR2"
  },
  {
    "objectID": "prepare/prep-01.html#assigned-reading",
    "href": "prepare/prep-01.html#assigned-reading",
    "title": "Preparation for Big Picture Discussion",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 1 from ISLR2"
  },
  {
    "objectID": "prepare/prep-17.html",
    "href": "prepare/prep-17.html",
    "title": "Preparation for Model Tuning",
    "section": "",
    "text": "Read Chapters 13 and 14 of TMWR"
  },
  {
    "objectID": "prepare/prep-17.html#assigned-readings-videos",
    "href": "prepare/prep-17.html#assigned-readings-videos",
    "title": "Preparation for Model Tuning",
    "section": "",
    "text": "Read Chapters 13 and 14 of TMWR"
  },
  {
    "objectID": "prepare/prep-21.html",
    "href": "prepare/prep-21.html",
    "title": "Preparation for Job Interview Discussion",
    "section": "",
    "text": "Review Job Interview Description"
  },
  {
    "objectID": "prepare/prep-21.html#assigned-readings-videos",
    "href": "prepare/prep-21.html#assigned-readings-videos",
    "title": "Preparation for Job Interview Discussion",
    "section": "",
    "text": "Review Job Interview Description"
  },
  {
    "objectID": "prepare/prep-28.html",
    "href": "prepare/prep-28.html",
    "title": "Preparation for Imbalanced Classes",
    "section": "",
    "text": "SMOTE Paper"
  },
  {
    "objectID": "prepare/prep-28.html#assigned-readings-videos",
    "href": "prepare/prep-28.html#assigned-readings-videos",
    "title": "Preparation for Imbalanced Classes",
    "section": "",
    "text": "SMOTE Paper"
  },
  {
    "objectID": "prepare/prep-07.html",
    "href": "prepare/prep-07.html",
    "title": "Preparation for Classification and Logistic Regression",
    "section": "",
    "text": "Read Chapter 4.1-4.3 from ISLR2\nRead Chapters 7 of TMWR"
  },
  {
    "objectID": "prepare/prep-07.html#assigned-readings-videos",
    "href": "prepare/prep-07.html#assigned-readings-videos",
    "title": "Preparation for Classification and Logistic Regression",
    "section": "",
    "text": "Read Chapter 4.1-4.3 from ISLR2\nRead Chapters 7 of TMWR"
  },
  {
    "objectID": "prepare/prep-26.html",
    "href": "prepare/prep-26.html",
    "title": "Preparation for Bagging and Boosting",
    "section": "",
    "text": "Read 8.2 of ISLR"
  },
  {
    "objectID": "prepare/prep-26.html#assigned-readings-videos",
    "href": "prepare/prep-26.html#assigned-readings-videos",
    "title": "Preparation for Bagging and Boosting",
    "section": "",
    "text": "Read 8.2 of ISLR"
  },
  {
    "objectID": "prepare/prep-11.html",
    "href": "prepare/prep-11.html",
    "title": "Preparation for Cross-Validation and Resampling",
    "section": "",
    "text": "Read Chapter 5.2 from ISLR2\nRead Chapters 10 of TMWR"
  },
  {
    "objectID": "prepare/prep-11.html#assigned-readings-videos",
    "href": "prepare/prep-11.html#assigned-readings-videos",
    "title": "Preparation for Cross-Validation and Resampling",
    "section": "",
    "text": "Read Chapter 5.2 from ISLR2\nRead Chapters 10 of TMWR"
  },
  {
    "objectID": "prepare/prep-02.html",
    "href": "prepare/prep-02.html",
    "title": "Preparation for Introduction to Statistical Learning",
    "section": "",
    "text": "Read Chapter 2 from ISLR2"
  },
  {
    "objectID": "prepare/prep-02.html#assigned-reading",
    "href": "prepare/prep-02.html#assigned-reading",
    "title": "Preparation for Introduction to Statistical Learning",
    "section": "",
    "text": "Read Chapter 2 from ISLR2"
  },
  {
    "objectID": "prepare/prep-31.html",
    "href": "prepare/prep-31.html",
    "title": "Preparation for Principal Component Analysis",
    "section": "",
    "text": "Chapter 16 of TMWR\nChapter 12.1-12.2 ISLR"
  },
  {
    "objectID": "prepare/prep-31.html#assigned-readings-videos",
    "href": "prepare/prep-31.html#assigned-readings-videos",
    "title": "Preparation for Principal Component Analysis",
    "section": "",
    "text": "Chapter 16 of TMWR\nChapter 12.1-12.2 ISLR"
  },
  {
    "objectID": "prepare/prep-08.html",
    "href": "prepare/prep-08.html",
    "title": "Preparation for Classification Metrics",
    "section": "",
    "text": "Read Chapter 4.4 from ISLR2\nRead Chapters 9 of TMWR"
  },
  {
    "objectID": "prepare/prep-08.html#assigned-readings-videos",
    "href": "prepare/prep-08.html#assigned-readings-videos",
    "title": "Preparation for Classification Metrics",
    "section": "",
    "text": "Read Chapter 4.4 from ISLR2\nRead Chapters 9 of TMWR"
  },
  {
    "objectID": "hack-a-thon/hack-a-thon-instruction.html",
    "href": "hack-a-thon/hack-a-thon-instruction.html",
    "title": "Hack-A-Thon Instructions",
    "section": "",
    "text": "The U.S. Constitution requires that a census be conducted every ten years in order to allocate congressional representatives. The first census was conducted in 1790 and the U.S. Census Bureau is still compiling the data from the 2020 census. While the Constitution only requires an “actual enumeration” of citizens, the census has expanded to include a number of demographic questions. As described by the Census Bureau, the results of the 2020 census will,\n\n“…determine congressional representation, inform hundreds of billions in federal funding, and provide data that will impact communities for the next decade.”\n\n\n\nIn this hack-a-thon, you will use census data to predict whether or not someone has an annual income of more than $50,000. The data for making your predictions are contained in two files:\n\ncensus_train.csv contains 35,000 rows representing unique individuals, and 15 columns, representing demographic information about those individuals (including whether their income is above or below $50,000).\ncensus_test.csv contains 13,840 rows, but only 14 columns since the income column has been removed.\n\nA complete description of the variables in the data set is below.\n\n\n\nThere are two deliverables for this project:\n\nAn HTML file outlining your modeling process. This should following the same guidelines for the Job Application 2 Resources. This should be pushed to the GitHub classroom assignment. This is also where you can get all of the data.\nA .csv file, uploaded to Canvas, containing a vector of your predictions for whether the individuals in the test set make more than $50,000. That is, you will create a length 13,480 vector of 0s and 1s where 1 indicates an individual makes MORE than $50K and write them to a file using:\n\n\nwrite.csv(prediction_vector, \"my_predictions.csv\", row.names = FALSE)\n\nYour submission will be evaluated for predictive quality (accuracy), writing quality, and clarity. Not all columns in this data set contain numerical values, so some will need to be translated into appropriate forms before beginning data analysis. There are also instances of missing or incomplete data, and some issues with how the data have been entered that you will need to address. You are also welcome to use any other techniques or packages you would like, but make sure that you can explain your analysis well.\n\n\n\nThis data set is great for practicing data science techniques. Because of this, if you search the internet you will find the original data set as well as articles that describe exactly how to apply statistical learning methods to it. There are several reasons NOT TO DO THIS. For one, it is cheating and will result in a 0. But also, it ruins the fun. This project is a great chance for you to go more in depth about something we have discussed in class and to challenge yourself to make the best predictions possible! You are welcome to search the internet for general advice regarding predictive modeling, but if you encounter census data, look away! If you have more specific questions about the data set, please ask me.\n\n\n\nage: continuous.\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\nfnlwgt: continuous. A weight that represents how common people with these exact age and racial demographics are in the United States.\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\neducation-num: continuous. Numerical representation of education level.\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. (“civ” and “AF” represent “civilian” (not in military) or “Armed Forces” (in military)).\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\nsex: Female, Male.\ncapital-gain: continuous. (Income from the sale of a capital asset, e.g., stocks or property)\ncapital-loss: continuous. (A loss occurred when a capital asset, e.g., stocks or property, decreases in value.)\nhours-per-week: continuous. Number of hours worked per week.\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US (Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holland-Netherlands.\nincome: whether or not annual income from all sources is above or below $50,000.\n\n\n\nYour submission will be assessed in each of the following areas as either Outstanding, Good, Acceptable, Needs Work, or Inadequate. The questions below will be used as a guide to determine the quality of your analysis in each area.\n\n\n\nAre the submitted predictions in the appropriate form?\nAre appropriate modeling techniques employed given the data and the task?\nAre the predictions reasonably accurate given the available data?\nWhere do the predictions rank in terms of the class?\n\n\n\n\n\nIs the report well-organized?\nIs the modeling process described in a clear, logical, and engaging manner?\nIs the modeling choices clearly justified?\nIs the report free of spelling and grammatical errors?\nIs the report aimed at the appropriate (technical) audience?\n\n\n\n\n\nAre the mathematical and statistical methods described accurately?\nAre appropriate predictor variables used in the analysis?\nAre the reasons for excluding and/or transforming the data sensible and well-explained?\nHow was the model chosen and validated?\nHow were the parameters of the model tuned (if applicable)?\n\n\n\n\n\nYou must submit two things:\n\nYour report must be pushed to GitHub.\nThe csv file containing your predictions must be uploaded to Canvas.",
    "crumbs": [
      "Hack-A-Thon",
      "Hack-A-Thon Instructions"
    ]
  },
  {
    "objectID": "hack-a-thon/hack-a-thon-instruction.html#project-overview",
    "href": "hack-a-thon/hack-a-thon-instruction.html#project-overview",
    "title": "Hack-A-Thon Instructions",
    "section": "",
    "text": "In this hack-a-thon, you will use census data to predict whether or not someone has an annual income of more than $50,000. The data for making your predictions are contained in two files:\n\ncensus_train.csv contains 35,000 rows representing unique individuals, and 15 columns, representing demographic information about those individuals (including whether their income is above or below $50,000).\ncensus_test.csv contains 13,840 rows, but only 14 columns since the income column has been removed.\n\nA complete description of the variables in the data set is below.",
    "crumbs": [
      "Hack-A-Thon",
      "Hack-A-Thon Instructions"
    ]
  },
  {
    "objectID": "hack-a-thon/hack-a-thon-instruction.html#deliverables",
    "href": "hack-a-thon/hack-a-thon-instruction.html#deliverables",
    "title": "Hack-A-Thon Instructions",
    "section": "",
    "text": "There are two deliverables for this project:\n\nAn HTML file outlining your modeling process. This should following the same guidelines for the Job Application 2 Resources. This should be pushed to the GitHub classroom assignment. This is also where you can get all of the data.\nA .csv file, uploaded to Canvas, containing a vector of your predictions for whether the individuals in the test set make more than $50,000. That is, you will create a length 13,480 vector of 0s and 1s where 1 indicates an individual makes MORE than $50K and write them to a file using:\n\n\nwrite.csv(prediction_vector, \"my_predictions.csv\", row.names = FALSE)\n\nYour submission will be evaluated for predictive quality (accuracy), writing quality, and clarity. Not all columns in this data set contain numerical values, so some will need to be translated into appropriate forms before beginning data analysis. There are also instances of missing or incomplete data, and some issues with how the data have been entered that you will need to address. You are also welcome to use any other techniques or packages you would like, but make sure that you can explain your analysis well.",
    "crumbs": [
      "Hack-A-Thon",
      "Hack-A-Thon Instructions"
    ]
  },
  {
    "objectID": "hack-a-thon/hack-a-thon-instruction.html#notes",
    "href": "hack-a-thon/hack-a-thon-instruction.html#notes",
    "title": "Hack-A-Thon Instructions",
    "section": "",
    "text": "This data set is great for practicing data science techniques. Because of this, if you search the internet you will find the original data set as well as articles that describe exactly how to apply statistical learning methods to it. There are several reasons NOT TO DO THIS. For one, it is cheating and will result in a 0. But also, it ruins the fun. This project is a great chance for you to go more in depth about something we have discussed in class and to challenge yourself to make the best predictions possible! You are welcome to search the internet for general advice regarding predictive modeling, but if you encounter census data, look away! If you have more specific questions about the data set, please ask me.",
    "crumbs": [
      "Hack-A-Thon",
      "Hack-A-Thon Instructions"
    ]
  },
  {
    "objectID": "hack-a-thon/hack-a-thon-instruction.html#description-of-variables",
    "href": "hack-a-thon/hack-a-thon-instruction.html#description-of-variables",
    "title": "Hack-A-Thon Instructions",
    "section": "",
    "text": "age: continuous.\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\nfnlwgt: continuous. A weight that represents how common people with these exact age and racial demographics are in the United States.\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\neducation-num: continuous. Numerical representation of education level.\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. (“civ” and “AF” represent “civilian” (not in military) or “Armed Forces” (in military)).\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\nsex: Female, Male.\ncapital-gain: continuous. (Income from the sale of a capital asset, e.g., stocks or property)\ncapital-loss: continuous. (A loss occurred when a capital asset, e.g., stocks or property, decreases in value.)\nhours-per-week: continuous. Number of hours worked per week.\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US (Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holland-Netherlands.\nincome: whether or not annual income from all sources is above or below $50,000.",
    "crumbs": [
      "Hack-A-Thon",
      "Hack-A-Thon Instructions"
    ]
  },
  {
    "objectID": "hack-a-thon/hack-a-thon-instruction.html#evaluation-guidelines",
    "href": "hack-a-thon/hack-a-thon-instruction.html#evaluation-guidelines",
    "title": "Hack-A-Thon Instructions",
    "section": "",
    "text": "Your submission will be assessed in each of the following areas as either Outstanding, Good, Acceptable, Needs Work, or Inadequate. The questions below will be used as a guide to determine the quality of your analysis in each area.\n\n\n\nAre the submitted predictions in the appropriate form?\nAre appropriate modeling techniques employed given the data and the task?\nAre the predictions reasonably accurate given the available data?\nWhere do the predictions rank in terms of the class?\n\n\n\n\n\nIs the report well-organized?\nIs the modeling process described in a clear, logical, and engaging manner?\nIs the modeling choices clearly justified?\nIs the report free of spelling and grammatical errors?\nIs the report aimed at the appropriate (technical) audience?\n\n\n\n\n\nAre the mathematical and statistical methods described accurately?\nAre appropriate predictor variables used in the analysis?\nAre the reasons for excluding and/or transforming the data sensible and well-explained?\nHow was the model chosen and validated?\nHow were the parameters of the model tuned (if applicable)?",
    "crumbs": [
      "Hack-A-Thon",
      "Hack-A-Thon Instructions"
    ]
  },
  {
    "objectID": "hack-a-thon/hack-a-thon-instruction.html#submitting-your-work",
    "href": "hack-a-thon/hack-a-thon-instruction.html#submitting-your-work",
    "title": "Hack-A-Thon Instructions",
    "section": "",
    "text": "You must submit two things:\n\nYour report must be pushed to GitHub.\nThe csv file containing your predictions must be uploaded to Canvas.",
    "crumbs": [
      "Hack-A-Thon",
      "Hack-A-Thon Instructions"
    ]
  },
  {
    "objectID": "hw/04-hw-classification.html",
    "href": "hw/04-hw-classification.html",
    "title": "Homework 4: Intro to Classification",
    "section": "",
    "text": "In this homework we will focus on classification. That is when the response variable is categorical. We’ll be on predicting a binary response variable, which is a categorical variable with only two possible outcomes. You will also practice collaborating with teams using GitHub.\n\n\nBy the end of the homework, you will…\n\nBe able to work create and merge branches in GitHub\nFit and interpret logistic regression models\nFit and evaluation binary classification models"
  },
  {
    "objectID": "hw/04-hw-classification.html#learning-goals",
    "href": "hw/04-hw-classification.html#learning-goals",
    "title": "Homework 4: Intro to Classification",
    "section": "",
    "text": "By the end of the homework, you will…\n\nBe able to work create and merge branches in GitHub\nFit and interpret logistic regression models\nFit and evaluation binary classification models"
  },
  {
    "objectID": "hw/04-hw-classification.html#teams-rules",
    "href": "hw/04-hw-classification.html#teams-rules",
    "title": "Homework 4: Intro to Classification",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW4. Your group will consist of 2-3 people and has been randomly generated. But first, some rules:\n\nAt the start of your homework, you and your team members should be in the same physical room for Exercise 0.1.\nExercise 0.2 will teach your how to create your own branch. Each team member will create their own branch and complete the homework themselves. Note that I’ll be able to see all of your commit so I’ll know if you didn’t do this.\nOnce everyone is finished, you and your team members should get together in the same room again to Complete Exercise 0.3 which will teach you how to merge your branches together and decide on the best version of each question.\n\nAs always:\n\nYou are all responsible for understanding the work that you turn in.\nIf you encounter a merge error that you don’t know how to fix, contact Dr. Friedlander as soon as possible. I recommend starting this assignment early so there is time for Dr. Friedlander to help you resolve any problems before the deadline."
  },
  {
    "objectID": "hw/04-hw-classification.html#clone-the-repo-create-your-own-branch-merging",
    "href": "hw/04-hw-classification.html#clone-the-repo-create-your-own-branch-merging",
    "title": "Homework 4: Intro to Classification",
    "section": "Clone the repo, create your own branch, & merging",
    "text": "Clone the repo, create your own branch, & merging\nThe following directions will guide you through the process of setting up your homework to work as a group."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-0.1",
    "href": "hw/04-hw-classification.html#exercise-0.1",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 0.1",
    "text": "Exercise 0.1\n\n\n\n\n\n\nQuestion\n\n\n\nIn your group, decide on a team name. Then have one member of your group:\n\nClick this link to accept the assignment and enter your team name.\nRepeat the directions for creating a project from HW 1 with the HW 4 repository.\n\nOnce this is complete, the other members can do the same thing, being careful to join the already created team on GitHub classroom."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-0.2",
    "href": "hw/04-hw-classification.html#exercise-0.2",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 0.2",
    "text": "Exercise 0.2\nWe will now learn how to create branches and run git from the command line. Each team member will create their own branch and complete a version of their homework.\n\n\n\n\n\n\nQuestion\n\n\n\n\nClick on the “Terminal” tab right next to the “Console” tab in the bottom left of RStudio. This is a “bash terminal”. You run “bash scripts” here rather than R code.\nCreate your own branch by typing git checkout -b banch_name where branch_name is whatever you want to name your branch. Typing git tells the terminal that you want to run a git command, checkout is a git command that will switch you to a different branch, the -b option will create a new branch for you. If you want to switch between existing branches leave out -b.\nPut your name in one of the slots below and render your document.\nStage your changes by typing git add .. Think of this like clicking the check boxes in the top right screen. If you don’t want to stage ALL of the changes you can type git add filename where filename corresponds to the files you want to stage. . will simply stage all changes.\nCommit your changes by typing git commit -m \"insert commit message\" where you replace insert commit message with your actual commit message. This will commit your changes.\nTo push your changes to GitHub, type git push.\nIf you ever want to pull your changes you can type git pull.\nIf all group members are working different branches you shouldn’t encounter any merge conflicts.\n\n\n\n\nTeam Name: [Insert Name]\nMember 1: [Insert Name]\nMember 2: [Insert Name]\nMember 3: [Insert Name/Delete line if you only have two members]"
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-0.3",
    "href": "hw/04-hw-classification.html#exercise-0.3",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 0.3",
    "text": "Exercise 0.3\n\n\n\n\n\n\nWarning\n\n\n\nSkip this exercise until everyone has finished the homework. Test 1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nRead the warning above!\n(DO NOT SKIP) Go through all of the questions together, discussing each others solutions to get a good idea of what you want the final submission to look like.\nOn any computer, type git checkout main. This will move you into the main branch.\nType git merge name_of_branch_you_want_to_merge where name_of_branch_you_want_to_merge is the name of the branch you want to merge into main.\nDo this for each team members branch. Each time you will likely get a merge-conflict that you need to resolve. Resolve them in the same way you did for the previous homework.\nGive your document a once-over, ensuring that you have blended all your work into a coherent format.\nRender, commit, and push one last time.\n\nIf you do the following, I will give you a zero until you fix them:\n\nPut multiple answers for the same problem. E.g. (“Eric’s Answer…”, “Adam’s Answer…”). You must discuss and decide on what the best solution is as a group.\nCreate a final document without merging. I must see each team member merge into the main branch even if you don’t plan on keeping any of their changes."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-1",
    "href": "hw/04-hw-classification.html#exercise-1",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\n\nLoad the data set into R using the read_csv function. The haberman.data file does not contain column names so you will need to use the col_names argument to specify them yourself. Choose sensible names based on the information in haberman.names.\nConvert the Survival Status variable into a factor, giving appropriate names (i.e. not numbers) to each category.\nGive a brief summary of the data set containing any information you feel would be important.\nSplit the data into a training and test set. Use a 60-40 split. Once again, please set your seed to 427."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-2",
    "href": "hw/04-hw-classification.html#exercise-2",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nUsing the data from your training set build a logistic regression model to predict whether or not a patient will survive based only on the number of axillary nodes detected. The same summary and broom functions will work for exploring your logistic model. Does the probability of survival increase or decrease with the number of nodes detected?"
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-2-1",
    "href": "hw/04-hw-classification.html#exercise-2-1",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nUse the predict function to evaluate your model on the integers from 0 to 50. Create a plot with the integers from 0 to 50 on the x-axis and the predicted probabilities on the y-axis. Based on this image, estimate the input that would be needed to give an output of 0.75. What does this mean in the context of the model? Note. To use predict the new_data must be a tibble where the columns have the same names as the those in the data frame you used to train your model."
  },
  {
    "objectID": "hw/04-hw-classification.html#exericse-4",
    "href": "hw/04-hw-classification.html#exericse-4",
    "title": "Homework 4: Intro to Classification",
    "section": "Exericse 4",
    "text": "Exericse 4\n\n\n\n\n\n\nQuestion\n\n\n\n\nFor the rest of the homework, treat the patient dying as our “Positive” class.\nUsing a threshold value of 0.5, obtain a vector of class predictions for the test data set (the if_else function might be useful here). You need not display it.\nConstruct a confusion matrix.\nUsing the numbers from your confusion matrix (i.e. without using functions from yardstick) compute the following:\n\nAccuracy\nPrecision\nRecall\nSpecificity\nNegative Predictive Value"
  },
  {
    "objectID": "hw/04-hw-classification.html#baseline-model",
    "href": "hw/04-hw-classification.html#baseline-model",
    "title": "Homework 4: Intro to Classification",
    "section": "Baseline Model",
    "text": "Baseline Model\nNow, we may be asking ourselves, “Is this a good accuracy?” and the answer is, as always, “It depends on your data and the goals of your analysis!”. The question below illustrates some of the nuances of using accuracy as a performance metric."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-5",
    "href": "hw/04-hw-classification.html#exercise-5",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nSuppose you decided to create a super simple model and just predict that everyone survives. What would the accuracy on the training set be? Note that you should not need to use construct a confusion matrix to answer this question.\n\n\nPerhaps we shouldn’t be so excited about the accuracy obtained in question 4! Accuracy is a good metric but it isn’t perfect and suffers in situations where our classes are unbalanced."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-6",
    "href": "hw/04-hw-classification.html#exercise-6",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nA threshold of 0.5 isn’t necessarily the best choice for the threshold. Write out a for-loop to test every threshold between 0 and 1 (increase by steps of 0.01). Create a single line-plot with the the threshold on the x-axis and the following on the y-axis: - accuracy - recall - precision You may use any parsnip functions you like. Hint: you should NOT be fitting any models here."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-7",
    "href": "hw/04-hw-classification.html#exercise-7",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nProduce an ROC curve and AUC statistic on the test set. Try to add your AUC to your plot. Comment on your results."
  },
  {
    "objectID": "hw/04-hw-classification.html#exercise-8",
    "href": "hw/04-hw-classification.html#exercise-8",
    "title": "Homework 4: Intro to Classification",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nKaggle is a website that runs a variety of data science competitions. Read about the framingham heart study and the associated data set here. Please stop reading at the section entitled “logistic regression” as this will spoil the fun of analyzing this data set. Since you have to create an account to download data off of Kaggle I’ve included a csv in the assignment repository. Create the best model that you can. Best model get’s a high five from Dr. Friedlander. Note, split your data into a train, and test sets using the seed 10520."
  },
  {
    "objectID": "hw/09-hw-RandomForests.html",
    "href": "hw/09-hw-RandomForests.html",
    "title": "Homework 9: Random Forests",
    "section": "",
    "text": "In this homework, we will practice using bagging and random forests to make classifications. In addition, we will talk more about cross validation and parameter tuning.\n\n\nIn this assignment, you will…\n\nFit random forests\nUse variable importance plots to interpret random forests\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#learning-goals",
    "href": "hw/09-hw-RandomForests.html#learning-goals",
    "title": "Homework 9: Random Forests",
    "section": "",
    "text": "In this assignment, you will…\n\nFit random forests\nUse variable importance plots to interpret random forests\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#teams-rules",
    "href": "hw/09-hw-RandomForests.html#teams-rules",
    "title": "Homework 9: Random Forests",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW9. Your group will consist of 2-3 people and has been randomly generated. The GitHub assignment can be found here. Rules:\n\nYou are all responsible for understanding the work that your team turns in.\nAll team members must make roughly equal contributions to the homework.\nAny work completed by a team member must be committed and pushed to GitHub by that person."
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#data-titanic",
    "href": "hw/09-hw-RandomForests.html#data-titanic",
    "title": "Homework 9: Random Forests",
    "section": "Data: Titanic",
    "text": "Data: Titanic\nWe will be using the famous Titanic data set which includes data for many passengers on the Titanic, including whether or not they survived. The files have already been cleaned up a bit for you and are split into testing Titanic_test.csv and training sets Titanic_train.csv. If you ever participate in a “hack-a-thon” or Kaggle-type data science competition, you will typically be given a “labeled” training set (i.e. including response variables) and unlabeled test set. You will then be scored on how well your predictions on the test set match the true values that you are unable to see. In this homework, I will be giving you the labels on the test set. The variables in the data represent, in order:\n\nWhether the passenger survived (1) or died (0)\nPassenger Class (1st, 2nd, or 3rd)\nSex\nAge (NA if unknown),\nSiblings/spouses on board\nParents/children on board\nFare paid,\nPort of Embarkation (either Cherbourg, Queenstown, or Southhampton)"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-1",
    "href": "hw/09-hw-RandomForests.html#exercise-1",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nLoad the two data sets. Take a look at the data (don’t print anything) and take care of any cleaning that is necessary (e.g. removing an ID column or columns or row numbers). Based on the characteristics above, which two or three features do you think are going to be most important for predicting who survives? Give a brief justification for each."
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-2",
    "href": "hw/09-hw-RandomForests.html#exercise-2",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nConduct a BRIEF EDA to explore the relationship between survival status and the variables you describe above. Practice making this looks professional. Advice:\n\nUse plots instead of table unless there is something interesting about specific numbers that you’d like to point out.\nMake sure you interpret each plot. Do not simply create the plot and say “Look at this plot that I made!”\nPut your explanations and interpretations directly before and after your plots. I.e. if you’re going to make three plots, display and explain them one at a time."
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-3",
    "href": "hw/09-hw-RandomForests.html#exercise-3",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate a recipe that can be used with a random forest and classification tree. You are welcome to create more than one if you feel it’s necessary."
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-4",
    "href": "hw/09-hw-RandomForests.html#exercise-4",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nBuild a classification tree on the training set to predict who will survive. Plot your classification tree below. What variables seem most important?"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-5",
    "href": "hw/09-hw-RandomForests.html#exercise-5",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nApply your model to the test set to predict who will survive. Print the confusion matrix below. What is the accuracy of this tree model?"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-6",
    "href": "hw/09-hw-RandomForests.html#exercise-6",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nBuild a single random forest for the training set. Make sure to\n\nChoose the number of factors to consider at each step (e.g. mtry)\nChoose the number of trees to build by specifying ntree\nMake sure to set importance = \"impurity\" in the engine"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-7",
    "href": "hw/09-hw-RandomForests.html#exercise-7",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate a variable importance plot for your forest. How do the variables it identifies as important compare to the classification tree?"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-8",
    "href": "hw/09-hw-RandomForests.html#exercise-8",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nHow does your model perform on the test data? How does it’s performance compare to the classification tree?"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#cross-validation-and-parameter-tuning",
    "href": "hw/09-hw-RandomForests.html#cross-validation-and-parameter-tuning",
    "title": "Homework 9: Random Forests",
    "section": "Cross Validation and Parameter Tuning",
    "text": "Cross Validation and Parameter Tuning"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-9",
    "href": "hw/09-hw-RandomForests.html#exercise-9",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nFor tuning a random forest model, we are mostly interested in finding the best value of mtry, the number of variables used at each split when building the trees. Conduct a grid search to find the best value of mtry."
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-10",
    "href": "hw/09-hw-RandomForests.html#exercise-10",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nWhen you have your best model, use it to predict who will survive in Titanic_test and report the accuracy.\nMost likely, this is greater than the accuracy of your classification tree model from Problem 5. But maybe not. There is still some randomness involved and it is always possible that on a particular data set, one method will outperform another. Still, by tuning parameters and doing cross-validation, we give ourselves the best chance of creating an accurate model."
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-11",
    "href": "hw/09-hw-RandomForests.html#exercise-11",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 11",
    "text": "Exercise 11\n\n\n\n\n\n\nQuestion\n\n\n\nCompare and contrast the information given by the variable importance plot of the random forest and looking at the actual classification tree. What information is contained in both? What information is only contained in one but not the other? Which is better for interpretability?"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-12",
    "href": "hw/09-hw-RandomForests.html#exercise-12",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 12",
    "text": "Exercise 12\n\n\n\n\n\n\nQuestion\n\n\n\nIn class, Dr. Friedlander described the number of tree grown in a random forest as being “kind of” a hyper parameter. What did he mean by that?"
  },
  {
    "objectID": "hw/09-hw-RandomForests.html#exercise-13",
    "href": "hw/09-hw-RandomForests.html#exercise-13",
    "title": "Homework 9: Random Forests",
    "section": "Exercise 13",
    "text": "Exercise 13\n\n\n\n\n\n\nQuestion\n\n\n\nDiscuss the mtry parameter in the context of the bias-variance trade-off. How should mtry impact the flexibility, bias, and variance of the model?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html",
    "href": "hw/06-feature-selection-regularization.html",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "",
    "text": "In this homework you will practice using regularization and model tuning to select models and features.\n\n\nIn this assignment, you will…\n\nFit linear and logistic regression models using regularization\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#learning-goals",
    "href": "hw/06-feature-selection-regularization.html#learning-goals",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "",
    "text": "In this assignment, you will…\n\nFit linear and logistic regression models using regularization\nUse grid-based techniques to choose tuning parameters"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#teams-rules",
    "href": "hw/06-feature-selection-regularization.html#teams-rules",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW6. Your group will consist of 2-3 people and has been randomly generated. The GitHub assignment can be found here. Rules:\n\nYou are all responsible for understanding the work that your team turns in.\nAll team members must make roughly equal contributions to the homework.\nAny work completed by a team member must be committed and pushed to GitHub by that person."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#cleaning-and-preprocessing",
    "href": "hw/06-feature-selection-regularization.html#cleaning-and-preprocessing",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Cleaning and Preprocessing",
    "text": "Cleaning and Preprocessing"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-1",
    "href": "hw/06-feature-selection-regularization.html#exercise-1",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nLoad the data set CommViolPredUnnormalizedDataCleaned.csv. This data set contains quite a bit of missing data but it uses question marks to denote values which are missing. Use read_csv to load this data set into Rand include the argument na = \"?\". R should automatically replace question marks with missing values. Comment on any aspects of the data which you find pertinent."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-2",
    "href": "hw/06-feature-selection-regularization.html#exercise-2",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nClean the data by:\n\nREAD THE DOCUMENTATION!\nDropping any columns that seem like they won’t be helpful to your analysis including all of the “non-predictive” and “potential goal” variables (other than ViolentCrimesPerPop) in the “Additional Variable Information” section in the documentation.\nEnsuring all features have the correct type."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-3",
    "href": "hw/06-feature-selection-regularization.html#exercise-3",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nUse drop_na to drop any rows which have a missing value in the ViolentCrimesPerPop column, our target variable. Why do we want to do this instead of trying to impute them?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-4",
    "href": "hw/06-feature-selection-regularization.html#exercise-4",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nHow many observation are left? Split the remaining data into a training and test set using an 80-20 split. Use the seed 427."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-5",
    "href": "hw/06-feature-selection-regularization.html#exercise-5",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate a recipe that can be used for ridge regression and LASSO. At a minimum it should include the following steps (not necessarily in this order):\n\nDummy code all factors.\nImputing missing values.\nNormalization of all predictors… why?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-6",
    "href": "hw/06-feature-selection-regularization.html#exercise-6",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nFit two models to the training data, a ridge regression model and a LASSO model. Set the penalty for both to 0. Plot the coefficient estimates against the penalty as in these plots. Explain what you see and why. (Practice interview question)."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-7",
    "href": "hw/06-feature-selection-regularization.html#exercise-7",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nUse cross-validation and grid-search to find the best penalty (according to RMSE) for your Ridge and LASSO models. Tip: This step will take a while to run so start with two folds and one repetition, maybe even on a subset of your data, until you’re sure that it is running correctly. Then use 5-folds and 10-repeats to get your final estimate of \\(\\lambda\\). In addition, make use of caching so you don’t need to re-run the cross-validation every time the document is Rendered."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-8",
    "href": "hw/06-feature-selection-regularization.html#exercise-8",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nFit your best Ridge, and your best LASSO model to the full training set. Assess both models performance on the test set. Which is better? List all variables that LASSO includes in the final model. How does this compare to Ridge? What does this mean for the interpretability of a model fit with LASSO compared to Ridge?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-9",
    "href": "hw/06-feature-selection-regularization.html#exercise-9",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nOpen the data in Excel. Clean up the spreadsheet and save it as a csv so that it can be loaded into R. Load the data, partition the data using a 80-20 split."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-10",
    "href": "hw/06-feature-selection-regularization.html#exercise-10",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nClean the data by:\n\nDropping any columns that seem like they won’t be helpful to your analysis. There is at least one.\nEnsuring all features have the correct type.\nConvert the Made column into a factor with informative levels (e.g. Made, Missed)."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-11",
    "href": "hw/06-feature-selection-regularization.html#exercise-11",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 11",
    "text": "Exercise 11\n\n\n\n\n\n\nQuestion\n\n\n\nWhat proportion of field goals in your training set were made? What does this mean in the context of determining a baseline level of accuracy that we want to beat?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-12",
    "href": "hw/06-feature-selection-regularization.html#exercise-12",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 12",
    "text": "Exercise 12\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate a recipe that can be used for logistic regression with \\(L_1\\)-regularization. At a minimum it should include the following steps (not necessarily in this order):\n\nDummy code all factors.\nNormalization of all predictors… why?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-13",
    "href": "hw/06-feature-selection-regularization.html#exercise-13",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 13",
    "text": "Exercise 13\n\n\n\n\n\n\nQuestion\n\n\n\nFit a logistic regression model with \\(L_1\\) regularization to the training data. This can be done in the exact same was as with OLS, simply use logisic_reg instead of linear_reg when you start your model and make sure to use glmnet as your engine. Set the penalty to 0. Plot the coefficient estimates against the penalty as in these plots. Explain what you see and why. (Practice interview question)."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-14",
    "href": "hw/06-feature-selection-regularization.html#exercise-14",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 14",
    "text": "Exercise 14\n\n\n\n\n\n\nQuestion\n\n\n\nUse cross-validation and grid-search to find the best penalty (according to accuracy) for your logistic regression model. Tip: This step will take a while to run so start with two folds and one repetition, maybe even on a subset of your data, until you’re sure that it is running correctly. Then use 5-folds and 10-repeats to get your final performance estimates. In addition, make use of caching so you don’t need to re-run the cross-validation every time the document is Rendered."
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-15",
    "href": "hw/06-feature-selection-regularization.html#exercise-15",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 15",
    "text": "Exercise 15\n\n\n\n\n\n\nQuestion\n\n\n\nFit your final model on the full training set and assess it’s performance on the test set. Which variables are included in your final model?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-16-very-long-question",
    "href": "hw/06-feature-selection-regularization.html#exercise-16-very-long-question",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 16 (Very long question)",
    "text": "Exercise 16 (Very long question)\n\n\n\n\n\n\nQuestion\n\n\n\nUse grid search and cross-validation to find the best \\(k\\) for a KNN classification model on this data. Fit the final model on the full training set and assess it’s performance on the test set. How does it perform compared to the model from Exercise 15?"
  },
  {
    "objectID": "hw/06-feature-selection-regularization.html#exercise-17-practice-interview-question",
    "href": "hw/06-feature-selection-regularization.html#exercise-17-practice-interview-question",
    "title": "Homework 6: Feature/Model Selection and Regularization",
    "section": "Exercise 17 (Practice Interview Question)",
    "text": "Exercise 17 (Practice Interview Question)\n\n\n\n\n\n\nQuestion\n\n\n\nWhy doesn’t it make sense to use \\(L_1\\) regularization with KNN?"
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html",
    "href": "hw/10-hw-Boosting_Multiclass.html",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "",
    "text": "In this homework, we will practice solving multi-class classification problems using gradient boosted trees.\n\n\nIn this assignment, you will…\n\nFit gradient boosted trees\nUse variable importance plots to interpret your model\nInterpret multi-class classification metrics"
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#learning-goals",
    "href": "hw/10-hw-Boosting_Multiclass.html#learning-goals",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "",
    "text": "In this assignment, you will…\n\nFit gradient boosted trees\nUse variable importance plots to interpret your model\nInterpret multi-class classification metrics"
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#teams-rules",
    "href": "hw/10-hw-Boosting_Multiclass.html#teams-rules",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW10. Your group will consist of 2-3 people and has been randomly generated. The GitHub assignment can be found here. Rules:\n\nYou are all responsible for understanding the work that your team turns in.\nAll team members must make roughly equal contributions to the homework.\nAny work completed by a team member must be committed and pushed to GitHub by that person."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#data-academic_success",
    "href": "hw/10-hw-Boosting_Multiclass.html#data-academic_success",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Data: academic_success",
    "text": "Data: academic_success\nWe will be using a data set from the UCI Machine Learning Repository. From the website:\n\nA dataset created from a higher education institution (acquired from several disjoint databases) related to students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies. The dataset includes information known at the time of student enrollment (academic path, demographics, and social-economic factors) and the students’ academic performance at the end of the first and second semesters. The data is used to build classification models to predict students’ dropout and academic sucess. The problem is formulated as a three category classification task, in which there is a strong imbalance towards one of the classes.\n\nOur goal will be to predict the column Target which contains three different factors:\n\nDropout: the student dropped out before graduating\nEnrolled: the student graduated but took extra time\nGradute: the student graduated on time\n\nMore information (such as feature descriptions) can be found on the website here."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-1",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-1",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nLoad the data set. Note that the file is delimited by semicolons (;) instead of commas. You may want to use the clean_names function from the janitor package to make the variable names a bit nicer. Do any cleaning that you think in necessary before you split your data. Make sure to read the documentation so that you are treating factors as factors rather than numbers.\nOnce that is done, split your data."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-2",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-2",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nDescribe the different metrics and strategies you might use to evaluate how a model performs on this data. Generate a plot of the response variable and comment on anything that you think may impact your model and these different metrics."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-3",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-3",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate a recipe that can be used with a boosted tree. You are welcome to create more than one if you feel it’s necessary."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-4",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-4",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nUsing all of the default settings in boost_tree fit a gradient boosted tree."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-5",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-5",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nCompute all of the metrics you described in Exercise 2.\nOur goal is to identify any student in the Dropout or Enroll categories so that we can target interventions (e.g. extra advising, tutoring, mentorship, etc.) towards these students. It’s not a big deal if students that were going to graduate on time also get these interventions because it doesn’t harm students to give them some extra support they don’t need. That said, the school’s budget isn’t infinite so we’d like to minimize the number of students who get unnecessary support, as long as the students who do need it are still getting it. Based on this goal, which metrics would you be most invested in? How does the model perform on these metrics?"
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-6",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-6",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nUse cross-validation to tune the number of trees and choose your best model. You may try to tune more than just trees if you’d like, but this may take a long time for your computer to run. Try to optimize over a metric that best aligns with the goal outlined in Exercise 5. Once you choose you best model fit it to the full training data."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-7",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-7",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate and interpret a variable importance plot."
  },
  {
    "objectID": "hw/10-hw-Boosting_Multiclass.html#exercise-8",
    "href": "hw/10-hw-Boosting_Multiclass.html#exercise-8",
    "title": "Homework 10: Boosting and Multiclass Classification",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nGenerate one-vs-all metrics and a confusion matrix and interpret both in the context of our goal."
  },
  {
    "objectID": "hw/03-hw-knn.html",
    "href": "hw/03-hw-knn.html",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "",
    "text": "In this homework, you will practice applying the \\(K\\)-Nearest Neighbors (KNN) method which is capable of performing both classification and regression. You will also practice collaborating with a team over GitHub.\n\n\nBy the end of the homework, you will…\n\nBe able to work simultaneously with teammates on the same document using GitHub\nFit and interpret KNN models in both regression and classification settings\nCompare and evaluate different KNN models"
  },
  {
    "objectID": "hw/03-hw-knn.html#learning-goals",
    "href": "hw/03-hw-knn.html#learning-goals",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "",
    "text": "By the end of the homework, you will…\n\nBe able to work simultaneously with teammates on the same document using GitHub\nFit and interpret KNN models in both regression and classification settings\nCompare and evaluate different KNN models"
  },
  {
    "objectID": "hw/03-hw-knn.html#teams-rules",
    "href": "hw/03-hw-knn.html#teams-rules",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Teams & Rules",
    "text": "Teams & Rules\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW3. Your group will consist of 2-3 people and has been randomly generated. But first, some rules:\n\nYou and your team members should be in the same physical room for Exercises 0.1 and 0.2. After that, you are welcome to divide up the work as you see fit. Please note that most of these problems have to be done sequentially except for Exercises 10-12, which can be done apart from 1-9.\nYou are all responsible for understanding the work that you turn in.\nIn order to receive credit, each team member but make a roughly equal contribution. Since this project has 12 exercises (not including 0), each team member should commit and push at least 4 different exercises.\nIf you are working on the same document simultaneously, make sure to render, commit, push, and pull FREQUENTLY.\nIf you encounter a merge error that you don’t know how to fix, contact Dr. Friedlander as soon as possible. I recommend starting this assignment early so there is time for Dr. Friedlander to help you resolve any problems before the deadline."
  },
  {
    "objectID": "hw/03-hw-knn.html#clone-the-repo-start-new-rstudio-project",
    "href": "hw/03-hw-knn.html#clone-the-repo-start-new-rstudio-project",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\nThe following directions will guide you through the process of setting up your homework to work as a group."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-0.1",
    "href": "hw/03-hw-knn.html#exercise-0.1",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 0.1",
    "text": "Exercise 0.1\n\n\n\n\n\n\nQuestion\n\n\n\nIn your group, decide on a team name. Then have one member of your group:\n\nClick this link to accept the assignment and enter your team name.\nRepeat the directions for creating a project from HW 1 with the HW 3 repository.\n\nOnce this is complete, the other members can do the same thing, being careful to join the already created team on GitHub classroom."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-0.2",
    "href": "hw/03-hw-knn.html#exercise-0.2",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 0.2",
    "text": "Exercise 0.2\nWe will now learn how to collaborate on the same document at the same time by creating a merge conflict. You can read more about it here and here.\n\n\n\n\n\n\nQuestion\n\n\n\n\nHave Members 1, 2, and 3 all create write different team names below and Render the document.\nHave Member 1 add, commit, and push changes.\nHave Member 2 add, commit, and push changes. This should cause an error.\nHave Member 2 pull changes from the remote repo which should generate something like this: CONFLICT (content): Merge conflict in 03-hw-knn.qmd.\nHave Member 2 open the .qmd file. You should see something like the third block of code in Section 22.4 of this link.\nHave Member edit the document so that it has only their team name, then render, commit, and push. This should not cause an error.\nHave Member 3 repeat steps 3-6.\nAgree on what you want your team name to actually be and have Member 1 repeat steps 3-6.\nNote that you will only generate a merge conflict if you make edits to the same line of code. If you are working on the same document simultaneously but are editing different portions, git should automatically merge for you.\n\n\n\n\nTeam Name: [Insert Name]\nMember 1: [Insert Name]\nMember 2: [Insert Name]\nMember 3: [Insert Name/Delete line if you only have two members]"
  },
  {
    "objectID": "hw/03-hw-knn.html#knn-for-classification",
    "href": "hw/03-hw-knn.html#knn-for-classification",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "KNN for Classification",
    "text": "KNN for Classification\nWe’ll start by using KNN for classification."
  },
  {
    "objectID": "hw/03-hw-knn.html#data",
    "href": "hw/03-hw-knn.html#data",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Data",
    "text": "Data\nWe will be working with the famous iris data set which consists of four measurements (in centimeters) for 150 plants belonging to three species of iris. This data set was first published in a classic 1936 paper by English statistician, and notable racist/eugenicist, Ronald Fisher. In that paper, multivariate linear models were applied to classify these plants. Of course, back then, model fitting was an extremely laborious process that was done without the aid of calculators or statistical software."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-1",
    "href": "hw/03-hw-knn.html#exercise-1",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nImport the datasets package, take a look at the columns of iris, and split your data into training and test sets using a 70-30 split. IMPORTANT: Make sure that each species is represented proportionally in the training set by using the strata argument in the initial_split function! Once again, set your seed to 427."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-2",
    "href": "hw/03-hw-knn.html#exercise-2",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a scatter plot of your training set with Sepal.Width and Petal.Width on the x- and y- axes, respectively, and color the points by Species."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-3",
    "href": "hw/03-hw-knn.html#exercise-3",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nAs the name suggests, the \\(K\\)-nearest neighbors (KNN) method classifies a point based on the classification of the observations in the training set that are nearest to that point. If \\(k &gt; 1\\), then the neighbors essentially “vote” on the classification of the point. Using only your graph, if \\(k = 1\\), how would KNN classify a flower that had sepal width 3cm and petal width 1cm?"
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-4",
    "href": "hw/03-hw-knn.html#exercise-4",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nJust to verify that we are correct, find the sepal width, petal width, and species of the observation in your training set that is closest to our flower with sepal width 3cm and petal width 1cm. This should be done by computing the Euclidean distance of (3, 1) to each observation and then sorting the resulting tibble to get the row with the smallest distance. Don’t worry about normalizaing the data."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-5",
    "href": "hw/03-hw-knn.html#exercise-5",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a recipe to center and scale your data sets using the mean and standard deviation from your training set."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-6",
    "href": "hw/03-hw-knn.html#exercise-6",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a workflow to fit a KNN model that uses weight_func = \"rectangular\" with 1 neighbor that includes the recipe above and fit the model to your training set.\n\n\nWe would like to understand how the method of \\(K\\)-nearest neighbors will classify points in the plane. That is, we would like to view the decision boundaries of this model. To do this, we will use our model to classify a large grid of points in the plane, and color them by their classification. The code below creates a data frame called grid consisting of 6.25^{4} points in the plane.\n\n# g1 &lt;- rep((200:450)*(1/100), 250)\n# g2 &lt;- rep((0:250)*(1/100), each = 250)\n# grid &lt;- tibble(  x1 = g1\n#                    , x2 = g2)"
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-7",
    "href": "hw/03-hw-knn.html#exercise-7",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nUncomment the code above, and change the variable names so that it will work with your model. Classify the points in grid using your training data and \\(k = 1\\). Then, plot the points in grid colored by their classification. Make sure your code is written so that the grid points are being centered and scaled before predictions are being made for them."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-8",
    "href": "hw/03-hw-knn.html#exercise-8",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nNotice that the decision boundary between versicolor and virginica looks a little strange. What do you observe? Why do you think this is happening? Does using \\(k = 2\\) make things better or worse? Why do you think that is?"
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-9",
    "href": "hw/03-hw-knn.html#exercise-9",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nDetermine which value of \\(k\\), the number of neighbors selected, gives the highest accuracy on the test set. Test all \\(k\\)s between 1 and 40. Note that there may be ties because our data set is a little bit too small. To break ties just choose the smallest \\(k\\) among the ones which are tied. Hint: A for loop may be helpful. What is the accuracy of the model you ended up choosing?\n\n\nAwesome!! Your model probably did pretty well, because KNN performs really well on the iris data set. However, this isn’t a very challenging data set for most classification methods. More challenging data sets have data on different scales and class imbalance where there are very few observations belonging to a particular class."
  },
  {
    "objectID": "hw/03-hw-knn.html#data-dummy-variables",
    "href": "hw/03-hw-knn.html#data-dummy-variables",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Data & Dummy Variables",
    "text": "Data & Dummy Variables\nFor this portion of the homework, we’ll use the Carseat data from the ISLR2 package. Frequently, when working with categorical data, you will be required to transform that data into dummy variables. Namely, you’ll create a unique variable for each column which gets a 1 if the corresponding observation is from that category and a 0 otherwise. In data science, this format is sometimes referred to as one-hot encoding."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-10",
    "href": "hw/03-hw-knn.html#exercise-10",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nLook at the Carseat data from the ISLR2 package. Then, split the data into a training and test set using a 70-30 split and a seed of 427 (as usual)."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-11",
    "href": "hw/03-hw-knn.html#exercise-11",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 11",
    "text": "Exercise 11\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a recipe which first, converts all categorical variables into dummy variables using step_dummy() then centers and scales all of the predictors based on the training data."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-12",
    "href": "hw/03-hw-knn.html#exercise-12",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 12",
    "text": "Exercise 12\n\n\n\n\n\n\nQuestion\n\n\n\nFit a KNN model to predict Sales from the data we have. Fit your model on the training data and use the test set to choose the appropriate variables and the number of neighbors to include. You may find it useful to plot the \\(R^2\\) and RMSE against the number of neighbors you include in your model. You may find that the RMSE and \\(R^2\\) disagree on what the best model is. You will have to make a judgement call on which model is “best”. One thing that can be helpful is looking at plots of your target variables (Sales in this case) against the model residuals."
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#office-hours-homework-lab",
    "href": "support.html#office-hours-homework-lab",
    "title": "Course support",
    "section": "Office hours + Homework Lab",
    "text": "Office hours + Homework Lab\nDr. Friedlander is here to help you be successful in the course. You are encouraged to attend office hours and the homework lab during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters or course content, you may email Dr. Friedlander at efriedlander@collegeofidaho.edu. If you email me about an error please include a screenshot of the error and the code causing the error. Barring extenuating circumstances, I will respond to MAT 427 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "jobs/job-app-1-resources.html",
    "href": "jobs/job-app-1-resources.html",
    "title": "Job Application 1 Resources",
    "section": "",
    "text": "Using Quarto & Netlify\nUsing Quarto, Netlify, & GitHub\nUsing Quarto & GitHub Pages",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Application 1 Resources"
    ]
  },
  {
    "objectID": "jobs/job-app-1-resources.html#building-a-website",
    "href": "jobs/job-app-1-resources.html#building-a-website",
    "title": "Job Application 1 Resources",
    "section": "",
    "text": "Using Quarto & Netlify\nUsing Quarto, Netlify, & GitHub\nUsing Quarto & GitHub Pages",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Application 1 Resources"
    ]
  },
  {
    "objectID": "jobs/job-app-1-resources.html#cover-letter-resume",
    "href": "jobs/job-app-1-resources.html#cover-letter-resume",
    "title": "Job Application 1 Resources",
    "section": "Cover Letter & Resume",
    "text": "Cover Letter & Resume\n\nCofI Career Prep Resources\nGeneral Tips from Eric:\n\nAlways customize your application materials to the job you’re applying to.\nThe first people who read your application may be HR recruiters with no expertise in the job you’re applying for so write your application accordingly. Alternatively, it could be an AI filtering software so make sure to include any important keywords.\nThe first people who read your application may be HR recruiters who are grumpy baby boomers who care deeply about “professionalism”. Make sure there is nothing on your job application that is unprofessional. This includes, but is not limited to, typos & grammar and GitHub handles or usernames that are anything other than a variation on your name (note you don’t need to change your GitHub handle for this class). Furthermore, make sure there is nothing on your social media that could be disqualifying, like pictures or videos of you partying/drinking.\nBe prepared to discuss anything on your resume in a job interview.\nJobs care A LOT about communication and teamwork skills. Anything you can do emphasize your skills in this area is worth including in your application. Make sure that all of your application materials have ZERO typos and grammatical errors.\n\n\n\nResume\n\nStart with Resume Template\nAdditional tips from Eric\n\nDON’T GO MORE THAN 1 PAGE. You should only ever go more than 1 page if you have done a lot of stuff. As college students, you haven’t… It may feel like you have… but you haven’t. Going over 1 page is a good way to get your application thrown out without being read.\nCUSTOMIZE YOUR RESUME TO THE JOB AD!\nRename “Skills” as “Technical Skills” and indicate level of proficiency next to each. You can include programming languages (e.g. R (Proficient)) and major packages (e.g. tidyverse (proficient)). It is OK to be a A LITTLE BIT optimistic about your skills. However, DO NOT LIE. If you claim you’re proficient they may put you through a coding interview. A good rule of thumb is to ask yourself “What level of proficiency could I reach with one weekend of HARD studying?”\nAdd a link to your personal website and GitHub to your resume.\nDon’t list relevant coursework near the top but if you have experience with certain content that you think is important (e.g. Linear Regression, K-Nearest Neighbors, Cross-Validation, etc.) include it in your Skills section.\nYour resume may first go through an AI filter or be reviewed by an HR hiring manager without expertise in the field you’re applying for. Make sure you include important key words in your resume that match what’s in the job description. For example, if the job ad says they want expertise in tidymodels, say tidymodels in your resume. Make sure that all of your application materials have ZERO typos and grammatical errors. Pat from HR may not know what cross-validation is, but they sure know what a period is.\nThink of the “preferred qualifications” as a wishlist rather than a list of necessary skills. Remember… you miss 100% of the shots you don’t take.\nWhen possible, quantify your achievements. An example from my old resume (emphasis added): “Devised cache replacement policy for Video-on-Demand settings which takes advantage of”chunked” files and achieves a 32% improvement install duration compared to the LRU policy; work published in peer-reviewed article.”\nIf you don’t have a lot of Experience, include a Projects section. Upload your project to GitHub or your website and link to them from your resume.\nAlways save your resume as a PDF. (1) You never know what software they will be using to view your resume and you want it to ALWAYS looks the same. (2) You don’t want anyone to accidentally edit it.\n\nA nice article\n\n\n\nCover Letter\n\nStart with Cover Letter Template\nAdditional tips from Eric\n\nRemember that people may not be carefully be reading your cover letter from top to bottom. Design your cover letter so that it still makes sense if someone skims it.\nYou can use bold, italicized, or underlined text to highlight or emphasize important skills. Don’t overdo it though. If you feel like you’ve done too much, you probably have.\n\n\n\n\nSample Analysis\n\nWhile not required, I recommend you start with something you’ve been working on\n\nIf you have a side project, use may use that.\nIf not, I recommend starting with one of your homeworks.\n\nYour data analysis should do the following\n\nTell a coherent story and narrative. If you start with a homework, make sure you take out all of my narrative and the exercise headings.\nInclude something similar to the following sections:\n\nIntroduction outlining and motivating the problem\nExploratory data analysis and pre-process/feature engineering\nModeling or Model Selection\nPerformance Analysis\nConclusion summarizing your results\n\nStrike a balance between addressing all of the relevant technical qualifications listed in the job ad without feeling like you’re just doing a bunch of unnecessary a non-relevant work. Keep in mind that it is not just important that you know HOW to apply different machine learning methods, but understand WHEN apply different techniques to data. Imagine that you’re being hired to be a construction worker. If you are asked to hammer in a nail, don’t take out a sledgehammer.\nShow that you are able to communicate well. Communicating well involves ensuring that you incorporate the appropriate level of detail (not too much/not too little) and that you are able to interpret the results of your analyses. It goes beyond just having perfect grammar and spelling.\n\nPro-tip: Write up a draft of your report and then go through it once taking out anything that isn’t absolutely necessary.\n\nBe professional and be grammatically correct and devoid of typos.\n\nThings to avoid:\n\nWalls of code without context. You should be explaining what you do every time you write code. However, don’t put too much detail. Assume the reader is familiar with machine learning. For example, say “We now fit the logistic regression model to the training data.” instead of “A logistic regression model is a machine learning model that blah, blah, blah”.\nWalls of data without context. Sometimes you may want to print out data for yourself to make sure some analysis step was done correctly. You don’t need to include that in your final report. Make sure that anything you display is done with purpose and that you’re talking about that purpose.\nTalking about functions rather than what code is doing. For example, say “We now fit the logistic regression model to the training data.” instead of “We not use the fit function to fit the logistic regression model to the training data.”\nEnsure all plots have professional axis labels.\nWhen talking about your variables, mostly refer to them by what they represent rather than their name. For example say “We will be predicting whether a customer will default on their loan.” instead of “We will be using default as the response variable. There are situation in which it is appropriate to refer to them by their names in R but be careful.\nIncluding a bunch of unnecessary analysis steps to prove that you can do them. If the project you’ve chosen does not call of more advanced analysis steps, choose a different one.",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Application 1 Resources"
    ]
  },
  {
    "objectID": "jobs/job-interview-resources.html",
    "href": "jobs/job-interview-resources.html",
    "title": "Job Interview Resources",
    "section": "",
    "text": "As discussed at the beginning of the course, you will engage in two different Job Interviews this semester, each worth 15% of your grade. This document will guide you through preparing for your first one.",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Interview Resources"
    ]
  },
  {
    "objectID": "jobs/job-interview-resources.html#job-interview-components",
    "href": "jobs/job-interview-resources.html#job-interview-components",
    "title": "Job Interview Resources",
    "section": "Job Interview Components",
    "text": "Job Interview Components\nYour interview will simulate the interview process at many companies and include two rounds:\n\nFirst-Round: Your first round interview will be with Dani Carmack. This interview will last about 15 minutes and will mimic an initial screening interview that you might encounter with an recruiter or HR representative without technical expertise. More information about preparing can be found in the Screening Interview Preparation section below.\n\nYour first-round interview must be completed by Friday, April 11th at 5pm. You may schedule 15 minutes with Dani using this link. Make sure to scroll down and select the option Friedlander - Screening Interview.\nIt is your responsibility to schedule your interview in a timely manner. Dani’s time is limited and she will run out of time-slots quickly. Inability to find a time will not be an acceptable excuse for being unable to complete the interview on time, unless the issue is reported before Wednesday, April 3rd.\n\nSecond-Round: Your second round interview will be with Dr. Friedlander. This interview will last about 45 minutes and will mimic a longer form interview with a hiring manager. This person will typically be your prospective manager (and maybe some colleagues) and have intimate knowledge of the subject matter and skills necessary to be successful in the role you are applying for. More information can be found in the Second Round Interview Preparation section below.\n\nYour second round interview must be completed by the last day of class. Please note that we will be going through a second round of Job Applications and you will be completing a second interview for that job during Finals Week.\nYou will be given a link to schedule your second round interview after you complete your first round. Once again, it is your responsibility to schedule your interview in a timely manner. My time is limited and I may run out of time-slots quickly. Inability to find a time will not be an acceptable excuse for being unable to complete the interview on time. I recommend you schedule a time with me as soon as you receive your link.",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Interview Resources"
    ]
  },
  {
    "objectID": "jobs/job-interview-resources.html#screen-prep",
    "href": "jobs/job-interview-resources.html#screen-prep",
    "title": "Job Interview Resources",
    "section": "Screening Interview Preparation",
    "text": "Screening Interview Preparation\nTo prepare, review the Interviewing 101 and Sample Interview Questions on the Career Prep Resources Canvas Page. The format of the interview will be as follows:\n\nYour interview will begin with the following two prompts:\n\nTell me about yourself.\nWhy do you want to work for Clearwater Analytics?\n\nYou will then be asked three questions chosen from the Sample Interview Questions referenced above.\nYou will then be asked one or two questions you have yet to see. These will not come out of left field but will most likely address strengths, weaknesses, or clarifications from your resume such as:\n\nA cool project or job you had\nClarification on the size of a group you worked with\nYour level of involvement with a project or job\nA lack of experience\nA lack of technical skills\nA lack of teamwork or soft skills\n\nThe interviewer will ask you whether you have any questions. It is expected that you have at least one but ideally more questions about the company or role. That said, quality is a lot more important than quantity. The goal here is to demonstrate enthusiasm and that you have researched both the company and the role.\n\n\nSome General Tips\n\nBehavioral or screening interviews conducted with someone from HR are usually intended to screen out candidates rather than choose the best candidate. Put more bluntly, they are looking to screen out the candidates who (a) lied on their resumes or very clearly don’t meet the requirements for the job, (b) have communication skills that are so bad as to be disqualifying, or (c) have some personality trait that makes them really difficult to be around. Your main goals in these interview should be to NOT DISQUALIFY YOURSELF and BE LIKEABLE. If you are receiving a first-round interview, it means that the company thinks that, on paper, you have the skills to succeed. Do not show them that you don’t.\nYour second goal should be to demonstrate that you are good at communicating:\n\nDo not speak overly fast. Don’t speak overly slow either but I’ve never seen someone do that.\nDemonstrate awareness of your audience. If you are speaking with someone from HR with no technical expertise, do not use technical jargon.\nDo your best not to sounds overly rehearsed. While you should prepare, you shouldn’t sound like you’re reading off a script. One thing I like to do for interviews and presentations is type out an answer to a question, but then don’t memorize it. This will help you organize your thoughts and point you toward parts of your answer where you need to think of clever wording or a smooth transition.\nUse professional language (i.e. minimal slang) but do not try to use big words and “sound smart”.\n\nAssorted tips:\n\nIt is OK to take a minute to collect your thoughts. If you need a second to formulate your thoughts just say “Hmm… that’s a great question, let me think for a second.” Then take 15-30 second to collect your thoughts and give a concise answer.\nDO NOT LIE!\nAs you are preparing, use the STAR method as a guide. That is, make sure you are citing examples when talking about your personal characteristics.\nThat said, make sure you can go with the flow and don’t sound like an overly rehearsed robot. You all have friends and know how to talk to people. Don’t just throw out everything you’ve learned about talking to people just because you’re in an interview.\nShow up early. If you run into traffic or get lost, you won’t be late.\nDon’t force the interview over time. If your time is up but you still have questions to ask you can say “I have a few more questions, but it looks like we’re out of time. Is it OK if I email them to you?”. Sometimes they will offer to extend the interview (this is usually a good sign) and sometimes they won’t (not necessarily a bad sign).\nBring a pen and notepad to take notes. You won’t always use them, but you might need them and it looks professional to have them. It also communicates that you care about what they are saying.\n\n\n\n\nGrading (30% of total grade)\nYour screening interview will be worth 30 percent of your Job Interview total.\n\nAttire and Professionalism\nYour attire and professionalism will account for 1/3 of your screening interview grade. The following criteria will be used to determine your grade:\n\nDid the student arrive on time?\nDid the student wear business casual attire (including dress shoes)?\n\nIf you do not have business casual attire or are unclear what business casual attire is, please notify Dr. Friedlander ASAP.\n\nDid student speak at an appropriate pace (i.e. not too fast or too slow)?\nDid student use professional language, avoiding any slang?\nDid student stay focused on the question being asked?\nWere the student’s answers too long or too short?\nDid the student bring a notepad and writing implement?\nWas the student respectful?\n\n\n\nPreparation and Enthusiasm\nThe next 1/3 of your screening interview grade will be based on your preparation and enthusiasm. The following criteria will be used to determine your grade:\n\nDid the student seem prepared to answer all of the questions given in advance?\nWhen answering questions, did the student have examples to point to that re-enforce what they are saying?\nDid the student demonstrate knowledge of the company and role they are applying to?\nDid the student demonstrate enthusiasm for the company and role?\nDid the student have good questions at the end of the interview?\n\n\n\nContent and Adaptability\nThe next 1/3 of your screening interview grade will be based on your how well you actually answer the questions, along with your adaptability to unseen questions. The following criteria will be used to determine your grade:\n\nDid the student’s answers demonstrate that they would be a good fit for the company and role?\nDid the student show awareness of their audience? I.e. did they make use of overly technical jargon or did they show awareness that they were talking to someone without technical expertise?\nDid the student appear to be honest? For example, did the student appear to be overselling themselves?\nDid the student successfully address weaknesses in their application?\nHow well did the student handle questions they had not seen before?\nDid the student sound overly rehearsed?",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Interview Resources"
    ]
  },
  {
    "objectID": "jobs/job-interview-resources.html#second-prep",
    "href": "jobs/job-interview-resources.html#second-prep",
    "title": "Job Interview Resources",
    "section": "Second Round Interview Preparation",
    "text": "Second Round Interview Preparation\nYour second round interview will mimic a technical interview. It will last 45 minutes and have the following format:\n\n(5 minutes) Your interview will begin with the following two prompts:\n\nTell me about yourself.\nWhy are you interested in this role?\n\n(10 minutes) You will be asked questions about your resume, asking you describe the details of some of the projects or jobs you have listed. If you don’t have any projects or job’s that are relevant you will be asked questions about your previous coursework. In real life, it is unlikely that you’d make it to this interview without anything relevant listed on your resume.\n(10 minutes) You will be asked questions about the sample data analysis you turned in. For example, “Why do you choose method X instead of method Y?”\n(20 minutes) You will be asked general technical questions related to the role. Each of these questions will fall into one of the following categories:\n\nQuestions asking you to explain something you did on your homework.\nQuestions similar in style to those asked in Chapter 5-8 of this book. Please note that only content that has been covered in this class, a pre-requisite for this class, or is listed on your resume will be included with a heavy focus on content from this class.\nPseudo-coding questions: E.g. “Write pseudo-code to perform gradient-descent”.\nTheory questions: E.g. “Why do we do cross-validation?”\nClosed Case-Study Questions: E.g. “Suppose you are trying to build an model that classifies pictures of food as either a hot dog or not a hot dog. What metrics would you use to evaluate performance?\nOpen-Ended Case-Study Questions: E.g. “Suppose you are tasked with building a model to predict whether an employee will leave the company within a year. How would you approach this problem?”\n\n\n\nGrading (70% of total grade)\nYour technical interview will be worth 70 percent of your Job Interview total.\n\nAttire and Professionalism (5% of total grade)\nYour attire and professionalism will account for 1/14 of your technical interview grade. The following criteria will be used to determine your grade:\n\nDid the student arrive on time?\nDid the student wear business casual attire (including dress shoes)?\n\nIf you do not have business casual attire or are unclear what business casual attire is, please notify Dr. Friedlander ASAP.\n\nDid the student bring a notepad and writing implement?\nWas the student respectful?\n\n\n\nMeta-cognition (5% of total grade)\nThe next 1/14 of your technical interview will be based on your meta-cognition. That is, whether you are able to self-assess what you know and what you don’t know. The following criteria will be used to determine your grade:\n\nDid the student seem to understand where the gaps in their knowledge were?\nDid the student seem to know if their answers were complete?\nDid the student think that answers that were wrong were actually right?\nHas the student actually mastered the skills on their resume?\n\n\n\nCommunication (20% of total grade)\nThe next 4/14 of your screening interview grade will be based on your communication skills. The following criteria will be used to determine your grade:\n\nDid student speak at an appropriate pace (i.e. not too fast or too slow)?\nDid student use professional language, avoiding any slang?\nDid student stay focused on the question being asked?\nWere the student’s answers too long or too short?\nWere the student’s answers clear?\nWas the student good and talking through their thought process?\n\n\n\nContent (40% of total grade)\nThe remainder of your grade will be determined based on whether the answers to the questions asked of you are correct and complete. The following criteria will be used to determine your grade:\n\nWere the student’s answers correct?\nWere the student’s answers complete?\nFor open-ended questions, did the student ask for any good questions and request necessary information?\nFor open-ended questions, did the student state any assumptions they were making?\n\n\n\n\nSome General Tips\n\nIf you are asked a question you don’t know the answer to, be honest. It is OK to say something like “I’m honestly not sure.” However, unless you absolutely know NOTHING about it I think the best way to answer is to state that you don’t really know but say what you do know. For example, suppose the interviewer asks “What are some strategies for dealing with imbalanced data?” You may not know the answer to this question, but it’s not like you know nothing about it! You could say something like, “Hmm… I’m not familiar with techniques for dealing with imbalanced data, but I do know that when you’re data is extremely imbalanced, accuracy isn’t super useful. You might want to focus more on metrics which are a bit more robust to imbalanced data like F-measure, AUC, or precision.”\nDon’t be afraid to ask for more information, especially when you are being asked an open-ended question! Remember, they are trying to evaluate whether you’ll be good at your job. An important part of any job is asking good questions and obtaining the appropriate information necessary to solve a problem. In fact, some questions are purposely vague to see if you can properly scope a problem and obtain the information you need to solve it.\nPractice thinking through problems out loud. Employers are really interested in seeing how you think through problems. Even if you don’t know the answer, talk through your thoughts as you grappling with it.\nThat said, don’t be afraid of a little silence. As with the screening interview, it is OK to take some time to think things through.",
    "crumbs": [
      "Job Applications & Interviews",
      "Job Interview Resources"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "Not that we will go through this as a call and on your first homework. Feel free to revisit it here if you need to.",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "computing-access.html#r-rstudio",
    "href": "computing-access.html#r-rstudio",
    "title": "Computing access",
    "section": "R & RStudio",
    "text": "R & RStudio\nIt is highly recommended that you install R and RStudio on your own personal computer. Follow the directions here to install R and RStudio on your computer\nIf, for some reason, you are unable to use R or RStudio on your personal computer, you may use the College of Idaho’s RStudio Server. However, I do not recommend you do this as I want you to practice installing packages in this course and you do not have the permissions to do that on the server. If you must use the RStudio Server, please notify Dr. Friedlander immediately.",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "computing-access.html#git-github",
    "href": "computing-access.html#git-github",
    "title": "Computing access",
    "section": "Git & Github",
    "text": "Git & Github\nYou need to create a GitHub account. You may want to use this to show-off work to future employers so I recommend using something professional (like your name) as your user name. Once you have done this, email it to Dr. Friedlander so he can add you to the GitHub Classroom.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Type install.packages(\"credentials\") into your console.\nStep 1: Type credentials::ssh_setup_github() into your console.\nStep 2: R will ask “No SSH key found. Generate one now?” Click 1 for yes.\nStep 3: You will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., mat427)\n\nYou can find more detailed instructions here if you’re interested.\n\n\nConfigure git\nThere is one more thing we need to do before getting started on the assignment. Specifically, we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package. You will need to install the usethis package in the same way you installed the credentials packages above.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"EricFriedlander\",\n  user.email = \"efriedlander@collegeofidaho.edu\")\n\nYou are now ready interact between GitHub and RStudio!",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  }
]