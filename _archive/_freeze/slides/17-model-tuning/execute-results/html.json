{
  "hash": "e59af8b5cd6ac9dd0e06413ba62ba418",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Regularization & Model Tuning'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(fivethirtyeight) # for candy rankings data\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n## Data: Candy {.smaller}\n\nThe data for this lecture comes from the article FiveThirtyEight [*The Ultimate Halloween Candy Power Ranking*](https://fivethirtyeight.com/features/the-ultimate-halloween-candy-power-ranking) by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy match-ups (e.g. Reese's vs. Skittles). Click [here](http://walthickey.com/2017/10/18/whats-the-best-halloween-candy/) to check out some of the match ups.\n\nThe data set contains 12 characteristics and win percentage from 85 candies in the experiment.\n\n## Data: Candy\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 13\n$ competitorname   <chr> \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarterâ€¦\n$ chocolate        <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31â€¦\n$ pricepercent     <dbl> 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n## Data Cleaning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_rankings_clean <- candy_rankings |> \n  select(-competitorname) |> \n  mutate(sugarpercent = sugarpercent*100, # convert proportions into percentages\n         pricepercent = pricepercent*100, # convert proportions into percentages\n         across(where(is.logical), ~ factor(.x, levels = c(\"FALSE\", \"TRUE\")))) # convert logicals into factors\n```\n:::\n\n\n\n## Data Cleaning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 12\n$ chocolate        <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <fct> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <fct> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <fct> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 73.2, 60.4, 1.1, 1.1, 90.6, 46.5, 60.4, 31.3, 90.6, 6â€¦\n$ pricepercent     <dbl> 86.0, 51.1, 11.6, 51.1, 51.1, 76.7, 76.7, 51.1, 32.5,â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n## Data Splitting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_split <- initial_split(candy_rankings_clean, strata = winpercent)\ncandy_train <- training(candy_split)\ncandy_test <- testing(candy_split)\n```\n:::\n\n\n\n# Lasso & Ridge Regression\n\n## Question\n\n-   What criteria do we use to fit a linear regression model? Write down an expression with $\\hat{\\beta_i}$'s, $x_{ij}$'s, and $y_j$'s in it.\n\n## OLS {.smaller}\n\n-   Ordinary Least Squares Regression:\n\n$$\n\\begin{aligned}\n\\hat{\\beta} =\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}SSE(\\hat{\\beta}) &= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{y}_j)^2\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2\n\\end{aligned}\n$$\n\n-   $\\hat{\\beta} = (\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p)$ is the vector of all my coefficients\n-   $\\operatorname{argmin}$ is a function (operator) that returns the *arguments* that minimize the quantity it's being applied to\n\n## Ridge Regression {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_2^2\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\n\\end{aligned}\n$$\n\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients? [[THEY SHRINK!]{.span style=\"color:red;\"}]{.fragment .fade-in}\n-   $\\|\\cdot\\|_2$ is called the *$L_2$-norm*\n\n## But... but... but why?!? {.smaller}\n\n::: incremental\n-   Recall the Bias-Variance Trade-Off\n-   Our reducible error can partitioned into:\n    +   Bias: how much $\\hat{f}$ misses $f$ by *on average*\n    +   Variance: how much $\\hat{f}$ moves around from sample to sample\n-   Ridge: increase bias a little bit in exchange for large decrease in variance\n-   As we increase $\\lambda$ do we increase or decrease the penalty for large coefficients?\n-   As we increase $\\lambda$ do we increase or decrease the *flexibility* of our model?\n:::\n\n## LASSO {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_1\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\n\\end{aligned}\n$$\n\n-   LASSO: **L**east **A**bsolute **S**hrinkage and **S**election **O**perator\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients [THEY SHRINK!]{.span style=\"color:red;\"}\n-   $\\|\\cdot\\|_1$ is called the *$L_1$-norm*\n\n## Question\n\n-   What should happen to our coefficients as we *increase* $\\lambda$?\n\n## Ridge vs. LASSO\n\n::: incremental\n-   Ridge has a *closed-form solution*... how might we calculate it?\n-   Ridge has some nice linear algebra properties that makes it EXTREMELY FAST to fit\n-   LASSO has no *closed-form solution*... why?\n-   LASSO coefficients estimated *numerically*... how?\n    +   Gradient descent works (kind of) but something called **coordinate descent** is typically better\n-   MOST IMPORTANT PROPERTY OF LASSO: it *induces sparsity* while Ridge does not\n:::\n\n## Sparsity in Applied Math {.smaller}\n\n-   **sparse** typically means \"most things are zero\"\n-   Example: sparse matrices are matrices where most entries are zero\n    +   for large matrices this can provide HUGE performance gains\n-   LASSO *induces sparsity* by setting most of the parameter estimates to zero\n    +   this means it fits the model and does feature selection SIMULTANEOUSLY\n-   Let's do some board work to see why this is...\n\n## LASSO and Ridge in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols <- linear_reg() |> \n  set_engine(\"lm\")\nridge_0 <- linear_reg(mixture = 0, penalty = 0) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_1 <- linear_reg(mixture = 0, penalty = 1) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_10 <- linear_reg(mixture = 0, penalty = 10) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_0 <- linear_reg(mixture = 1, penalty = 0) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_1 <- linear_reg(mixture = 1, penalty = 1) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_10 <- linear_reg(mixture = 1, penalty = 10) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n## Create Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preproc <- recipe(winpercent ~ . , data = candy_train) |> \n  step_dummy(all_nominal_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Question\n\n-   Why do we need to normalize our data?\n\n## Create workflows\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeneric_wf <- workflow() |> add_recipe(lm_preproc)\nols_wf <- generic_wf |> add_model(ols)\nridge0_wf <- generic_wf |> add_model(ridge_0)\nridge1_wf <- generic_wf |> add_model(ridge_1)\nridge10_wf <- generic_wf |> add_model(ridge_10)\nlasso0_wf <- generic_wf |> add_model(lasso_0)\nlasso1_wf <- generic_wf |> add_model(lasso_1)\nlasso10_wf <- generic_wf |> add_model(lasso_10)\n```\n:::\n\n\n\n## Fit Models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_fit <- ols_wf |> fit(candy_train)\nridge0_fit <- ridge0_wf |> fit(candy_train)\nridge1_fit <- ridge1_wf |> fit(candy_train)\nridge10_fit <- ridge10_wf |> fit(candy_train)\nlasso0_fit <- lasso0_wf |> fit(candy_train)\nlasso1_fit <- lasso1_wf |> fit(candy_train)\nlasso10_fit <- lasso10_wf |> fit(candy_train)\n```\n:::\n\n\n\n## Collect coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs <- bind_cols(model = \"ols\", tidy(ols_fit)) |> \n  bind_rows(bind_cols(model = \"ridge0\", tidy(ridge0_fit))) |> \n  bind_rows(bind_cols(model = \"ridge1\", tidy(ridge1_fit))) |> \n  bind_rows(bind_cols(model = \"ridge10\", tidy(ridge10_fit))) |> \n  bind_rows(bind_cols(model = \"lasso0\", tidy(lasso0_fit))) |> \n  bind_rows(bind_cols(model = \"lasso1\", tidy(lasso1_fit))) |> \n  bind_rows(bind_cols(model = \"lasso10\", tidy(lasso10_fit)))\n```\n:::\n\n\n\n## Question\n\n::: incremental\n-   What should we expect from `ols`, `ridge0`, and `lasso0`?\n-   They should be the same!\n:::\n\n## Visualize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"ridge0\", \"lasso0\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n## Uh-Oh!\n\n::: incremental\n-   They're not the same\n-   Any idea why?\n-   Algorithm used to fit model\n    +   `lm` estimates coefficients *analytically*\n    +   `glmnet` estimates coefficients *numerically* using an algorithm named \"coordinate-descent\"\n-   Moral: you must understand theory AND application\n:::\n\n## Visualizing Coefficients: Ridge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"ridge1\", \"ridge10\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing Coefficients: LASSO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"lasso1\", \"lasso10\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n## Ridge Coefficients vs. Penalty ( $\\lambda$ )\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge0_fit |> extract_fit_engine() |> autoplot()\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## LASSO Coefficients vs. Penalty ( $\\lambda$ )\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso0_fit |> extract_fit_engine() |> autoplot()\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n\n## Why should I learn math for data science?\n\n-   Today:\n    +   Ordinary Least Squares: Linear algebra, Calc 3\n    +   LASSO: Calc 3, Geometry, Numerical Analysis\n    +   Ridge: Linear Algebra, Calc 3\n    +   Bias-Variance Trade-Off: Probability\n    \n## Question\n\n-   How do you think we should choose $\\lambda$?\n\n# Model Tuning\n\n## Tuning Parameters\n\n-   **Tuning Parameters** or **Hyperparameters** are parameters that cannot (or should not) be estimated when a model is being trained\n-   These parameters control something about the learning process and changing them will result in a different model when fit to the full training data (i.e. after cross-validation)\n-   Frequently: tuning parameters control model complexity\n-   Example: $\\lambda$ in LASSO and Ridge regression\n-   Today: How to choose our tuning parameters?\n\n## Question\n\n-   Which of the following are tuning parameters:\n    +   $\\beta_0$: the intercept of linear regression\n    +   $k$ in KNN\n    +   step size in gradient descent\n    +   The number of folds in cross-validation\n    +   Type of distance to use in KNN (i.e. rectangular vs. Gower's vs. weighted etc)\n\n## Basic Idea\n\n-   Use CV to try out a bunch of different tuning parameters and choose the \"best\" one\n-   How do we choose which tuning parameters to try?\n-   Two general approaches:\n    +   Grid Search\n    +   Iterative Search\n    \n## Grid Search\n\n-   Create a grid of tuning parameters and try out each combination\n-   Types of grids:\n    +   Regular Grid: tuning parameter values are spaced *deterministically* using a linear or logarithmic scale and all combinations of parameters are used\n    +   Irregular Grids: tuning parameter values are chosen *stochastically*\n    \n## Grid Search in R\n\n-   Take advantage of package `dials` which is part of the `tidyverse`\n    +   Set every tuning variable equal to `tune()`\n    \n## LASSO and Ridge in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols <- linear_reg() |> \n  set_engine(\"lm\")\n\nridge <- linear_reg(mixture = 0, penalty = tune()) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\n\nlasso <- linear_reg(mixture = 1, penalty = tune()) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\n```\n:::\n\n\n    \n## Create Recipe\n\n-   Note: no tuning variables in this case\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preproc <- recipe(winpercent ~ . , data = candy_train) |> \n  step_dummy(all_nominal_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Create workflows\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeneric_wf <- workflow() |> add_recipe(lm_preproc)\nols_wf <- generic_wf |> add_model(ols)\nridge_wf <- generic_wf |> add_model(ridge)\nlasso_wf <- generic_wf |> add_model(lasso)\n```\n:::\n\n\n\n## Create Metric Set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_metrics <- metric_set(rmse, rsq)\n```\n:::\n\n\n\n## Create Folds\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Since sample size is so small, not using stratification\ncandy_folds <- vfold_cv(candy_train, v = 5, repeats = 10)\n```\n:::\n\n\n\n## Grid Search in R\n\n-   Take advantage of package `dials` which is part of the `tidyverse`\n    +   Set every tuning variable equal to `tune()`\n    +   Generate grid for hyperparameters\n\n## Generate Grid\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# note that dials treats penalty on a log10 scale\npenalty_grid <- grid_regular(penalty(range = c(-10, 5)), \n                             levels = 100)\n```\n:::\n\n\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenalty_grid |> head() |>  kable(digits = 11, format.args = list(scientific = TRUE))\n```\n\n::: {.cell-output-display}\n\n\n| penalty|\n|-------:|\n| 1.0e-10|\n| 1.4e-10|\n| 1.8e-10|\n| 2.5e-10|\n| 3.4e-10|\n| 4.5e-10|\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenalty_grid |> tail() |>  kable()\n```\n\n::: {.cell-output-display}\n\n\n|   penalty|\n|---------:|\n|  220.5131|\n|  298.3647|\n|  403.7017|\n|  546.2277|\n|  739.0722|\n| 1000.0000|\n\n\n:::\n:::\n\n\n:::\n::::\n\n\n## Regular Grid Search in R\n\n-   Take advantage of package `dials` which is part of the `tidyverse`\n    +   Set every tuning variable equal to `tune()`\n    +   Generate grid for hyperparameters using `grid_regular`\n    +   Tune your model: fit all hyperparameter combination on resamples using `tune_grid`\n\n## Tune Models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuning_ridge_results <- tune_grid(\n  ridge_wf,\n  resamples= candy_folds,\n  grid = penalty_grid\n)\n\ntuning_lasso_results <- tune_grid(\n  lasso_wf,\n  resamples = candy_folds,\n  grid = penalty_grid\n)\n```\n:::\n\n\n\n## Visualizing Results: Ridge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_ridge_results)\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n\n## Selecting Best Model: Ridge\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rmse_ridge <- tuning_ridge_results |> \n  select_best(metric = \"rmse\")\nbest_rmse_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|    8.11|Preprocessor1_Model091 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rsq_ridge <- tuning_ridge_results |> \n  select_best(metric = \"rsq\",)\nbest_rsq_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|    2.66|Preprocessor1_Model087 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_rmse_final <- finalize_workflow(ridge_wf, best_rmse_ridge)\nridge_rmse_fit <- fit(ridge_rmse_final, data = candy_train)\ntidy(ridge_rmse_fit) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term                   |   estimate|  penalty|\n|:----------------------|----------:|--------:|\n|(Intercept)            | 49.8221817| 8.111308|\n|sugarpercent           |  1.4103248| 8.111308|\n|pricepercent           | -0.3480850| 8.111308|\n|chocolate_TRUE.        |  4.2970084| 8.111308|\n|fruity_TRUE.           |  0.1122385| 8.111308|\n|caramel_TRUE.          |  0.8312507| 8.111308|\n|peanutyalmondy_TRUE.   |  2.4787739| 8.111308|\n|nougat_TRUE.           |  0.5721602| 8.111308|\n|crispedricewafer_TRUE. |  1.4495580| 8.111308|\n|hard_TRUE.             | -1.1717589| 8.111308|\n|bar_TRUE.              |  1.4204162| 8.111308|\n|pluribus_TRUE.         |  0.1766225| 8.111308|\n\n\n:::\n:::\n\n\n\n## Visualizing Results: LASSO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_lasso_results)\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n## Selecting Best Model: LASSO\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rmse_lasso <- tuning_lasso_results |> \n  select_best(metric = \"rmse\")\nbest_rmse_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|    2.66|Preprocessor1_Model087 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rsq_lasso <- tuning_lasso_results |> \n  select_best(metric = \"rsq\")\nbest_rsq_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|    8.11|Preprocessor1_Model091 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_rmse_final <- finalize_workflow(lasso_wf, best_rmse_lasso)\nlasso_rmse_fit <- fit(lasso_rmse_final, data = candy_train)\ntidy(lasso_rmse_fit) |> \n  filter(estimate != 0) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term                 |   estimate|  penalty|\n|:--------------------|----------:|--------:|\n|(Intercept)          | 49.8221817| 2.656088|\n|chocolate_TRUE.      |  6.9219213| 2.656088|\n|peanutyalmondy_TRUE. |  0.6011864| 2.656088|\n\n\n:::\n:::\n\n\n\n## Using Parsimony as a Tie-Breaker\n\n-   Good heuristic: **One-Standard Error Rule**\n    +   Use resampling to estimate error metrics\n    +   Compute standard error for error metrics\n    +   Select most parsimonious model that is within one standard error of the best performance metric\n    \n\n\n## Selecting Best Model: Ridge\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_ose_rmse_ridge <- tuning_ridge_results |> \n  select_by_one_std_err(metric = \"rmse\", desc(penalty)) # why are we descending\nbest_ose_rmse_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|   32.75|Preprocessor1_Model096 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbse_ose_rsq_ridge <- tuning_ridge_results |> \n  select_by_one_std_err(metric = \"rsq\", desc(penalty))\nbse_ose_rsq_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|     100|Preprocessor1_Model100 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Visualizing Results: Ridge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_ridge_results)\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_rmse_final <- finalize_workflow(ridge_wf, best_ose_rmse_ridge)\nridge_rmse_fit <- fit(ridge_rmse_final, data = candy_train)\ntidy(ridge_rmse_fit) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term                   |   estimate|  penalty|\n|:----------------------|----------:|--------:|\n|(Intercept)            | 49.8221817| 32.74549|\n|sugarpercent           |  0.7545242| 32.74549|\n|pricepercent           |  0.3689986| 32.74549|\n|chocolate_TRUE.        |  2.0856906| 32.74549|\n|fruity_TRUE.           | -0.6426461| 32.74549|\n|caramel_TRUE.          |  0.5609808| 32.74549|\n|peanutyalmondy_TRUE.   |  1.3698227| 32.74549|\n|nougat_TRUE.           |  0.5030043| 32.74549|\n|crispedricewafer_TRUE. |  0.8753648| 32.74549|\n|hard_TRUE.             | -0.8042963| 32.74549|\n|bar_TRUE.              |  1.0761375| 32.74549|\n|pluribus_TRUE.         | -0.2670056| 32.74549|\n\n\n:::\n:::\n\n\n\n## Visualizing Results: LASSO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_lasso_results)\n```\n\n::: {.cell-output-display}\n![](17-model-tuning_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n\n## Selecting Best Model: Ridge\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbse_ose_rmse_lasso <- tuning_lasso_results |> \n  select_by_one_std_err(metric = \"rmse\", desc(penalty)) # why are we descending\nbse_ose_rmse_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|    3.51|Preprocessor1_Model088 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbse_ose_rsq_lasso <- tuning_lasso_results |> \n  select_by_one_std_err(metric = \"rsq\", desc(penalty)) # why are we descending\nbse_ose_rsq_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config                |\n|-------:|:----------------------|\n|    8.11|Preprocessor1_Model091 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_rmse_final <- finalize_workflow(lasso_wf, bse_ose_rmse_lasso)\nlasso_rmse_fit <- fit(lasso_rmse_final, data = candy_train)\ntidy(lasso_rmse_fit) |> \n  filter(estimate != 0) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term            |  estimate|  penalty|\n|:---------------|---------:|--------:|\n|(Intercept)     | 49.822182| 3.511192|\n|chocolate_TRUE. |  6.304809| 3.511192|\n\n\n:::\n:::\n\n\n\n## Using the Test Set as a Tie Breaker\n\n-   Once you've found you \"best\" candidate from several different classes of model, it's ok to compare on test set\n-   In this case, we have our best ridge and our best lasso model\n-   Main this to avoid... LOTS of comparisons on your test set\n\n## Using the Test Set as a Tie Breaker {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_test_wpreds <- candy_test |> \n  mutate(ridge_preds = predict(ridge_rmse_fit, new_data = candy_test)$.pred,\n         lasso_preds = predict(lasso_rmse_fit, new_data = candy_test)$.pred)\ncandy_test_wpreds |> rmse(estimate = ridge_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        11.6\n```\n\n\n:::\n\n```{.r .cell-code}\ncandy_test_wpreds |> rmse(estimate = lasso_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        11.8\n```\n\n\n:::\n\n```{.r .cell-code}\ncandy_test_wpreds |> rsq(estimate = ridge_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.427\n```\n\n\n:::\n\n```{.r .cell-code}\ncandy_test_wpreds |> rsq(estimate = lasso_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.341\n```\n\n\n:::\n:::\n\n\n\n-   Ridge Wins!\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}