{
  "hash": "082899a003a4a25f1a8dfce4cd25f7ad",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Decision Trees'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n\n\n## Tree-Based Methods {.smaller}\n\n-   Idea: **stratify** or **segment** the predictor space into simple regions\n-   Splitting rules used to segment the predictor space form a tree\n-   Can be used for both classification and regression\n-   Single tree called **decision tree**\n    +   Simple and useful for interpretation\n    +   Not the best in terms of prediction accuracy.\n-   Much more powerful: grow multiple trees and then combine their results\n    +   **bagging** and **boosting** .\n-   Most powerful methods for tabular data are typically tree-based methods\n\n## Terminology for Trees\n\n-   Every split is considered to be a **node**\n-   First node at the top of the tree: **root node** (contains all the training data)\n-   Final nodes at the bottom of tree called **leaves** or **terminal nodes**\n-   Decision trees typically drawn with root at top and leaves at bottom\n-   Nodes in middle of tree called **internal nodes**\n-   The segments of the trees that connect nodes are known as **branches**\n\n\n## Terminology for Trees\n\n::: {#fig-tree1 layout-ncol=2}\n\n![](images/19/depth-3-decision-tree-1.png)\n\n![](images/19/depth-3-decision-tree-2.png)\n\nFrom Hands-On Machine Learning, Boehmke & Greenwell\n:::\n\n\n## Building a Tree {.smaller}\n\n-   Select predictor $X_j$ and cut point $s$ such that splitting the predictor space into the regions\n$\\{X|X_j < s\\}$ and $\\{X|X_j \\geq s\\}$ leads to the greatest possible reduction in performance metric (e.g. SSE)\n-   For any $j$ and $s$, define\n$$R_1 = \\{X|X_j < s\\} \\ \\ \\text{and} \\ \\ R_2 = \\{X|X_j \\geq s\\}$$\n-   Find $j$ and $s$ that minimize\n$$SSE = \\displaystyle \\sum_{i \\in R_1}\\left(y_i - \\hat{y}_{R_1}\\right)^2 + \\sum_{i \\in R_2}\\left(y_i - \\hat{y}_{R_2}\\right)^2$$\n-   Repeat process on each the two new regions\n-   Continue until a stopping criterion is reached\n\n\n\n\n\n## Prediction\n\n-   For new observation that falls in region $R_j$:\n    +   Regression: the mean response of the training set observations in $R_j$\n    +   Classificaiton: majority vote response of the training set observations in $R_j$\n\n\n## Building a Tree and Prediction\n\n::: {#fig-tree2 layout-ncol=2}\n\n![](images/19/decision-stump-1.png)\n\n![](images/19/decision-stump-2.png)\n\nFrom Hands-On Machine Learning, Boehmke & Greenwell\n:::\n\n\n## Building a Tree and Prediction\n\n::: {#fig-tree3 layout-ncol=2}\n\n![](images/19/depth-3-decision-tree-1.png)\n\n![](images/19/depth-3-decision-tree-2.png)\n\nFrom Hands-On Machine Learning, Boehmke & Greenwell\n:::\n\n\n## Building a Tree and Prediction\n\n![From ISLR](images/19/ISLR-tree-1.png)\n\n## Building a Tree {.smaller}\n\n-   Anyone know what a **greedy algorithm** is?\n\n. . .\n\n-   Computationally infeasible to consider every possible partition\n-   Idea: **top-down, greedy** approach known as **recursive binary splitting**.\n    +   **top-down** because it begins at the top of the tree\n    +   **greedy** because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step\n        +   Important for determining whether to use a ordinal encoding or not\n    +   Stop when each terminal node has fewer than some predetermined number of observations\n    \n## Overfitting\n\n-   This process described above is likely to **overfit** the data\n-   One solution: require each split to improve performance by some amount\n    +   Bad Idea: sometimes seemingly meaningless cuts early on enable really good cuts later on\n-   Good solution: **pruning**\n    +   Build big tree and the prune off branches that are unnecessary\n    \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}