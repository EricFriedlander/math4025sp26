{
  "hash": "a309721ae4b1971cd6402d150822c47f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Evaluating Classification Models'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Tips on Gradient Descent HW\n\n-   Make step size small!\n-   May take a while to converge\n-   Try adaptive step size (i.e. backtracking)\n-   Clarification on stopping criteria\n    +   Set tolerance\n    +   Stop when distance from gradient to $(0, 0)$ is below tolerance\n\n\n## Computational Set-Up\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(janitor) # for next contingency tables\nlibrary(kableExtra)\nlibrary(ISLR2)\n\ntidymodels_prefer()\n```\n:::\n\n\n\n\n\n\n## Default Dataset {.smaller}\n\n::: columns\n::: column\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Default) |> kable()  # print first six observations\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|    income|\n|:-------|:-------|---------:|---------:|\n|No      |No      |  729.5265| 44361.625|\n|No      |Yes     |  817.1804| 12106.135|\n|No      |No      | 1073.5492| 31767.139|\n|No      |No      |  529.2506| 35704.494|\n|No      |No      |  785.6559| 38463.496|\n|No      |Yes     |  919.5885|  7491.559|\n\n\n:::\n:::\n\n\n\n:::\n\n::: column\n**Response Variable**: `default`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDefault |> \n  tabyl(default) |>  # class frequencies\n  kable()           # Make it look nice\n```\n\n::: {.cell-output-display}\n\n\n|default |    n| percent|\n|:-------|----:|-------:|\n|No      | 9667|  0.9667|\n|Yes     |  333|  0.0333|\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n## Split the data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\ndefault_split <- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<6000/4000/10000>\n```\n\n\n:::\n\n```{.r .cell-code}\ndefault_train <- training(default_split)\ndefault_test <- testing(default_split)\n```\n:::\n\n\n\n\n## [K-Nearest Neighbors Classifier: Build Model]{.r-fit-text}\n\n-   **Response** ($Y$): `default`\n-   **Predictor** ($X$): `balance`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit <- nearest_neighbor(neighbors = 10) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>  \n  fit(default ~ balance, data = Default)   # fit 10-nn model\n```\n:::\n\n\n\n\n## [K-Nearest Neighbors Classifier: Predictions]{.r-fit-text} {.smaller}\n\n::: panel-tabset\n## Class labels\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(knnfit, new_data = Default, type = \"class\") |> head() |> kable()   # obtain predictions as classes\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n\n\n:::\n:::\n\n\n\n\n## Probabilities\n\n-   Predicts class w/ maximum probability\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(knnfit, new_data = Default, type = \"prob\") |> head() |> kable() # obtain predictions as probabilities\n```\n\n::: {.cell-output-display}\n\n\n| .pred_No| .pred_Yes|\n|--------:|---------:|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n\n\n:::\n:::\n\n\n\n:::\n\n## Fitting a logistic regression\n\nFitting a logistic regression model with `default` as the response and `balance` as the predictor:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogregfit <- logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |> kable()  # obtain results\n```\n\n::: {.cell-output-display}\n\n\n|term        |    estimate| std.error| statistic| p.value|\n|:-----------|-----------:|---------:|---------:|-------:|\n|(Intercept) | -10.6926385| 0.4659035| -22.95033|       0|\n|balance     |   0.0055327| 0.0002841|  19.47329|       0|\n\n\n:::\n:::\n\n\n\n\n## Making predictions in R\n\n::: panel-tabset\n\n## Class Labels\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |> kable()   # obtain class predictions\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n\n\n:::\n:::\n\n\n\n\n## Log-Odds\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |> kable()   # obtain log-odds predictions\n```\n\n::: {.cell-output-display}\n\n\n|         x|\n|---------:|\n| -6.819727|\n\n\n:::\n:::\n\n\n\n\n## Probabilities\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |> kable()  # obtain probability predictions\n```\n\n::: {.cell-output-display}\n\n\n|  .pred_No| .pred_Yes|\n|---------:|---------:|\n| 0.9989092| 0.0010908|\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n# Assessing Performance of Classifiers\n\n## Binary Classifiers\n\n-   Start with binary classification scenarios\n-   With binary classification, designate one category as \"Success/Positive\" and the other as \"Failure/Negative\"\n    +   If relevant to your problem: \"Positive\" should be the thing you're trying to predict/care more about\n    +   Note: \"Positive\" $\\neq$ \"Good\"\n    +   For `default`: \"Yes\" is Positive\n-   Some metrics weight \"Positives\" more and viceversa\n\n## Confusion Matrix\n\n|                                  | Actual Positive/Event | Actual Negative/Non-event |\n|:--------------------------------:|:---------------------:|:-------------------------:|\n|   **Predicted Positive/Event**   |     True Positive (TP)     |      False Positive  (FP)     |\n| **Predicted Negative/Non-event** |    False Negative (FN)    |       True Negative  (TN)     |\n\n## Adding predictions to tibble\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds <- default_test |> \n  mutate(\n    knn_preds = predict(knnfit, new_data = default_test, type = \"class\")$.pred_class,\n    logistic_preds = predict(logregfit, new_data = default_test, type = \"class\")$.pred_class\n  )\n\ndefault_test_wpreds |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income|knn_preds |logistic_preds |\n|:-------|:-------|---------:|--------:|:---------|:--------------|\n|No      |No      |  729.5265| 44361.63|No        |No             |\n|No      |Yes     |  808.6675| 17600.45|No        |No             |\n|No      |Yes     | 1220.5838| 13268.56|No        |No             |\n|No      |No      |  237.0451| 28251.70|No        |No             |\n|No      |No      |  606.7423| 44994.56|No        |No             |\n|No      |No      |  286.2326| 45042.41|No        |No             |\n\n\n:::\n:::\n\n\n\n\n## KNN: Confusion Matrix\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  3854   80\n       Yes   17   49\n```\n\n\n:::\n:::\n\n\n\n\n## KNN: Confusion Matrix (Sexy)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classification-metrics_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n## Logistic Regression: Confusion Matrix\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = logistic_preds) |> \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classification-metrics_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n\n## Classification Metrics {.smaller}\n\n-   Accuracy: proportion of your classes that are correct $$(TP + TN)/Total$$\n-   Recall/Sensitivity: proportion of true positives correct (true positive rate) $$TP/(TP+FN)$$\n-   Precision/Positive Predictive Value (PPV): proportion of predicted positive that are correct $$TP/(TP+FP)$$\n-   Specificity: proportion of true negatives correct (true negative rate) $$TN/(TN+FP)$$\n-   Negative Predictive Value (NPV): proportion of predicted negatives that are correct $$TN/(TN+FN)$$\n\n## KNN: Performance {.smaller}\n\n:::: columns\n::: column\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classification-metrics_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::: column\n-   Accuracy: $(3854+49)/4000 = .976 = 97.6\\%$\n-   Recall/Sensitivity: $49/(49+80) = 0.380 = 38.0\\%$\n-   Precision/Positive Predictive Value (PPV): $49/(49+17) = .742 = 74.2\\%$\n-   Specificity: $3854/(3854+17) = 0.996 = 99.6\\%$\n-   Negative Predictive Value (NPV): $3854/(3854+80) = 98.0$\n:::\n::::\n\n## Logistic Regression: Performance {.smaller}\n\n:::: columns\n::: column\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = logistic_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classification-metrics_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::: column\nCompute the following and write your answers on the board:\n\n-   Accuracy\n-   Recall/Sensitivity\n-   Precision/Positive Predictive Value (PPV)\n-   Specificity\n-   Negative Predictive Value (NPV)\n:::\n::::\n\n## Performance Metrics with `yardstick`\n\n-   `yardstick` is a package that ships with `tidymodels` meant for model evaluation\n-   Typical syntax: `metricname(data, truth, estimate, ...)`\n    +   Bind original data with predicted observations\n    +   Put true response in for `truth` and predicted values in for `estimate`\n    \n## Logistic Regression: Accuracy {.smaller}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  accuracy(truth = default, estimate = logistic_preds) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |binary     |     0.973|\n\n\n:::\n:::\n\n\n\n\n## Two More Metrics {.smaller}\n\n-   Matthews correlation coefficient (MCC): similar to $R^2$ but for classification\n$$\\frac{TP\\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n    +   Good for imbalanced data\n    +   Considers both positives and negatives\n-   F-Measure: harmonic mean of recall and precision\n$$\\frac{2}{recall^{-1} + precision^{-1}} = \\frac{2TP}{2TP+FP+FN}$$\n    +   Focuses more on positives\n    +   bad of imbalanced data\n\n## Metric Sets\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinary_metrics <- metric_set(accuracy, recall, precision, specificity,\n                             npv, mcc, f_meas)\n```\n:::\n\n\n\n\n-   Can apply this to compute a bunch of metrics\n\n\n## KNN: Performance {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  binary_metrics(truth = default, estimate = knn_preds, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric     |.estimator | .estimate|\n|:-----------|:----------|---------:|\n|accuracy    |binary     | 0.9757500|\n|recall      |binary     | 0.3798450|\n|precision   |binary     | 0.7424242|\n|specificity |binary     | 0.9956084|\n|npv         |binary     | 0.9796645|\n|mcc         |binary     | 0.5206828|\n|f_meas      |binary     | 0.5025641|\n\n\n:::\n:::\n\n\n\n\n## Logistic Regression: Performance {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  binary_metrics(truth = default, estimate = logistic_preds, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric     |.estimator | .estimate|\n|:-----------|:----------|---------:|\n|accuracy    |binary     | 0.9730000|\n|recall      |binary     | 0.3023256|\n|precision   |binary     | 0.6842105|\n|specificity |binary     | 0.9953500|\n|npv         |binary     | 0.9771747|\n|mcc         |binary     | 0.4437097|\n|f_meas      |binary     | 0.4193548|\n\n\n:::\n:::\n\n\n\n\n## Discussion\n\n-   For each of the following metrics, brainstorm a situation in which that metric is probably the most important:\n    +   Recall\n    +   Precision\n    +   Accuracy",
    "supporting": [
      "08-classification-metrics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}