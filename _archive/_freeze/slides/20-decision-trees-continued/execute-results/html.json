{
  "hash": "0bc072b07748e968a6302a9d74e7e0ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Decision Trees Continued'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n\n## Building a Tree and Prediction\n\n![From ISLR](images/19/ISLR-tree-1.png)\n\n## Building a Tree {.smaller}\n\n-   Anyone know what a **greedy algorithm** is?\n\n. . .\n\n-   Computationally infeasible to consider every possible partition\n-   Idea: **top-down, greedy** approach known as **recursive binary splitting**.\n    +   **top-down** because it begins at the top of the tree\n    +   **greedy** because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step\n        +   Important for determining whether to use a ordinal encoding or not\n    +   Stop when each terminal node has fewer than some predetermined number of observations\n    \n## Overfitting\n\n-   This process described above is likely to **overfit** the data\n-   One solution: require each split to improve performance by some amount\n    +   Bad Idea: sometimes seemingly meaningless cuts early on enable really good cuts later on\n-   Good solution: **pruning**\n    +   Build big tree and the prune off branches that are unnecessary\n    \n## Tree Pruning\n\n\n![From Hands-On Machine Learning, Boehmke & Greenwell](images/19/pruned-tree-1.png)\n\n\n## Bias-Variance Trade-Off\n\n-   How does the bias-variance trade-off relate to the bias-variance trade-off?\n    +   Would larger trees have high or lower bias?\n    +   What about variance?\n\n\n## Tree Pruning\n\n-   Grow a very large tree, and then **prune** it back to obtain a **subtree**.\n-   Terminology: **cost complexity pruning** or **weakest link pruning**\n-   Consider the following objective function \n$$\n\\begin{aligned}\n&SSE(T) + \\alpha \\times |T|\\\\ \n&\\quad= SSE(T) + \\alpha \\times (\\text{# of terminal nodes of }T)\n\\end{aligned}\n$$\n\n## Cost-Complexity Pruning\n\n-   Fit full tree $T_0$ to minimise $SSE$\n-   Select **sub-tree** $T\\subset T_0$ which minimizes\n$$\n\\begin{aligned}\n&SSE(T) + \\alpha \\times |T|\\\\ \n&\\quad= SSE(T) + \\alpha \\times (\\text{# of terminal nodes of }T)\n\\end{aligned}\n$$\n-   What should happen to the tree as we increase $\\alpha$?\n-   What should happen to the bias and variance as we increase $\\alpha$?\n-   How should we choose $\\alpha$? [Cross Validation]{.fragment}\n\n# Regression Trees in R\n\n## Data: `dcbikeshare` {.smaller}\n\nBike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. As of May 2018, there are about over 1600 bike-sharing programs around the world, providing more than 18 million bicycles for public use. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. [Documentation](https://www.kaggle.com/datasets/marklvl/bike-sharing-dataset)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dcbikeshare)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 731\nColumns: 16\n$ instant    <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, â€¦\n$ dteday     <date> 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05â€¦\n$ season     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,â€¦\n$ yr         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ mnth       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,â€¦\n$ holiday    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,â€¦\n$ weekday    <dbl> 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,â€¦\n$ workingday <dbl> 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,â€¦\n$ weathersit <dbl> 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2,â€¦\n$ temp       <dbl> 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.20â€¦\n$ atemp      <dbl> 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.23â€¦\n$ hum        <dbl> 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261,â€¦\n$ windspeed  <dbl> 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.08â€¦\n$ casual     <dbl> 331, 131, 120, 108, 82, 88, 148, 68, 54, 41, 43, 25, 38, 54â€¦\n$ registered <dbl> 654, 670, 1229, 1454, 1518, 1518, 1362, 891, 768, 1280, 122â€¦\n$ cnt        <dbl> 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 126â€¦\n```\n\n\n:::\n:::\n\n\n\n## Cleaning the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndcbikeshare_clean <- dcbikeshare |> \n  select(-instant, -dteday, -casual, -registered, -yr) |> \n  mutate(\n    season = as_factor(case_when(\n      season == 1 ~ \"winter\",\n      season == 2 ~ \"spring\",\n      season == 3 ~ \"summer\",\n      season == 4 ~ \"fall\"\n    )),\n    mnth = as_factor(mnth),\n    weekday = as_factor(weekday),\n    weathersit = as_factor(weathersit)\n  )\n```\n:::\n\n\n\n## Split the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\nbike_split <- initial_split(dcbikeshare_clean, prop = 0.7, strata = cnt)\nbike_train <- training(bike_split)\nbike_test <- testing(bike_split)\n```\n:::\n\n\n\n\n\n## Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_recipe <- recipe(cnt ~ ., data = bike_train) |>   # set up recipe\n  step_integer(season, mnth, weekday) |>   # numeric conversion of levels of the predictors\n  step_dummy(all_nominal(), one_hot = TRUE)  # one-hot/dummy encode nominal categorical predictors\n```\n:::\n\n\n\n## Define Model Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndec_tree_lowcc <- decision_tree(cost_complexity = 10^(-4)) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\ndec_tree_highcc <- decision_tree(cost_complexity = 0.1) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n```\n:::\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nworkflow() |> \n  add_recipe(bike_recipe) |> \n  add_model(dec_tree_lowcc) |> \n  fit(bike_train) |>\n  extract_fit_engine() |> \n  rpart.plot()\n```\n:::\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-decision-trees-continued_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nworkflow() |> \n  add_recipe(bike_recipe) |> \n  add_model(dec_tree_highcc) |> \n  fit(bike_train) |>\n  extract_fit_engine() |> \n  rpart.plot()\n```\n:::\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-decision-trees-continued_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n\n## Defin Model Workflow with Tuning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndec_tree <- decision_tree(cost_complexity = tune()) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\ndt_wf <- workflow() |> \n  add_recipe(bike_recipe) |> \n  add_model(dec_tree)\n```\n:::\n\n\n\n## Define Folds and Tuning Grid\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_folds <- vfold_cv(bike_train, v = 5, repeats = 10)\n\ncp_grid <- grid_regular(cost_complexity(range = c(-4, -1)), # I had to play around with these \n                             levels = 20)\n```\n:::\n\n\n\n## Tuning CP\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuning_cp_results <- tune_grid(\n  dt_wf,\n  resamples= bike_folds,\n  grid = cp_grid\n)\n```\n:::\n\n\n\n## Plot Results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_cp_results)\n```\n\n::: {.cell-output-display}\n![](20-decision-trees-continued_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Select Best Trees\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tree <- select_best(tuning_cp_results)\nbest_tree |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| cost_complexity|.config               |\n|---------------:|:---------------------|\n|       0.0054556|Preprocessor1_Model12 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nose_tree <- select_by_one_std_err(tuning_cp_results, desc(cost_complexity))\nose_tree |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| cost_complexity|.config               |\n|---------------:|:---------------------|\n|       0.0078476|Preprocessor1_Model13 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n## Fit Best Tree\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tree <- select_best(tuning_cp_results)\nbest_wf <- finalize_workflow(dt_wf, best_tree)\nbest_model <- best_wf |> fit(bike_train)\nbest_model |> \n  extract_fit_engine() |> \n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](20-decision-trees-continued_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n## Fit OSE Tree\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nose_tree <- select_best(tuning_cp_results)\nose_wf <- finalize_workflow(dt_wf, ose_tree)\nose_model <- ose_wf |> fit(bike_train)\nose_model |> \n  extract_fit_engine() |> \n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](20-decision-trees-continued_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n## Questions\n\n-   Why are both models the same but have different RMSE estimates from CV?\n-   What's the difference between encoding `mnth` as an ordinal variable vs. a one-hot encoding?\n\n## Decision Trees {.smaller}\n\n-   Advantages\n    +   Easy to explain and interpret\n    +   Closely mirror human decision-making\n    +   Can be displayed graphically, and are easily interpreted by non-experts\n    +   Does not require standardization of predictors\n    +   Can handle missing data directly\n    +   Can easily capture non-linear patterns\n-   Disadvantages\n    +   Do not have same level of prediction accuracy\n    +   Not very robust\n\n## Classification Trees\n\n-   Predictions: \n    +   Classes: most common class at terminal node\n    +   Probability: proportion of each class at terminal node\n-   Rest of tree: same as regression tree\n\n## Exploring Decision Trees w/ App {.smaller}\n\n-   Dr. F will split you into four groups\n-   On one of your computers connect to a tv and [open this app](https://efriedlander.shinyapps.io/ClassificationMetrics/)\n-   Do the following based on your group number:\n    +   1: Choose plane on the first screen\n    +   2: Choose circle on the first screen\n    +   3: Choose parabola on the first screen\n    +   4: Choose sine curve on the first screen\n-   We will generate data from this population... do you think KNN, logistic regression, or a decision tree will yield a better classifier? Why?\n\n## Exploring Decision Trees w/ App {.smaller}\n\n-   Choose one of the populations\n-   Generate some data\n-   Fit a decision tree to the data and see how the different hyper parameters impact the resulting model:\n    +   **complexity parameter (cp)**: the larger the number the more pruning\n    +   **Minimum leaf size**: the minimum number of observations from the training data that must be contained in a leaf\n    +   **Max depth**: the maximum number of splits before a terminal node\n-   Write down any interesting observations\n\n## Data: Voter Frequency\n\n[Info about data](https://github.com/fivethirtyeight/data/tree/master/non-voters)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoter_data <- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nglimpse(voter_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 5,836\nColumns: 119\n$ RespId         <dbl> 470001, 470002, 470003, 470007, 480008, 480009, 480010,â€¦\n$ weight         <dbl> 0.7516, 1.0267, 1.0844, 0.6817, 0.9910, 1.0591, 1.1512,â€¦\n$ Q1             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ Q2_1           <dbl> 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1â€¦\n$ Q2_2           <dbl> 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1â€¦\n$ Q2_3           <dbl> 2, 2, 2, 1, -1, 3, 2, 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 1, â€¦\n$ Q2_4           <dbl> 4, 3, 2, 3, 1, 4, 3, 2, 3, 1, 2, 4, 2, 3, 1, 1, 1, 1, 2â€¦\n$ Q2_5           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1â€¦\n$ Q2_6           <dbl> 4, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1, 2â€¦\n$ Q2_7           <dbl> 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1â€¦\n$ Q2_8           <dbl> 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 2, 1, 1, 1, 1, 2, 1â€¦\n$ Q2_9           <dbl> 4, 1, 4, 1, 1, 1, 1, 4, 3, 1, 3, 4, 4, 4, 1, 1, 2, 1, 3â€¦\n$ Q2_10          <dbl> 2, 3, 3, 2, 1, 4, 3, 2, 2, 2, 3, 2, 2, 3, 4, 2, 1, 1, 1â€¦\n$ Q3_1           <dbl> 1, 3, 2, 1, 4, 1, 2, 2, 1, 3, 3, 1, 4, 1, 3, 3, 4, 4, 3â€¦\n$ Q3_2           <dbl> 1, 3, 2, 1, -1, 2, 3, 3, 4, 3, 3, 1, 4, 1, 4, 4, 2, 4, â€¦\n$ Q3_3           <dbl> 4, 4, 3, 4, 1, -1, 3, 3, 2, 3, 2, 4, 1, 4, 4, 4, 4, 1, â€¦\n$ Q3_4           <dbl> 4, 3, 3, 4, 1, 2, 2, 1, 1, 2, 2, 4, 1, 1, 1, 1, 4, 1, 2â€¦\n$ Q3_5           <dbl> 3, 3, 2, 2, 2, 2, 2, 1, 2, 3, 2, 3, 1, 1, 2, 2, 3, 3, 2â€¦\n$ Q3_6           <dbl> 2, 2, 2, 1, 4, 2, 2, 2, 1, 2, 3, 1, 4, 1, 1, 2, 4, 3, 2â€¦\n$ Q4_1           <dbl> 2, 2, 2, 1, 1, 4, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2â€¦\n$ Q4_2           <dbl> 1, 2, 2, 2, 1, 3, 1, 1, 2, 3, 2, 1, 1, 2, 2, 1, 1, 1, 1â€¦\n$ Q4_3           <dbl> 2, 2, 3, 2, 1, 3, 1, 2, 2, 3, 3, 1, 1, 2, 2, 1, 1, 1, 1â€¦\n$ Q4_4           <dbl> 2, 3, 3, 2, 1, 3, 2, 2, 4, 3, 3, 2, 3, 4, 4, 2, 1, 1, 2â€¦\n$ Q4_5           <dbl> 2, 3, 2, 2, 1, 4, 1, 2, 3, 2, 3, 1, 2, 2, 2, 1, 1, 2, 3â€¦\n$ Q4_6           <dbl> 2, 1, 3, 2, 1, 2, 1, 3, 3, 3, 3, 2, 4, 2, 1, 2, 1, 1, 3â€¦\n$ Q5             <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1â€¦\n$ Q6             <dbl> 2, 2, 1, 3, 2, 4, 1, 1, 3, 3, 3, 2, 4, 4, 3, 1, 2, 3, 2â€¦\n$ Q7             <dbl> 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1â€¦\n$ Q8_1           <dbl> 3, 2, 3, 3, 1, 3, 2, 4, 3, 2, 2, 4, 1, 4, 1, 1, 4, 1, 3â€¦\n$ Q8_2           <dbl> 4, 3, 2, 2, 3, 3, -1, 4, 4, 3, 2, 3, 4, 4, 3, 4, 2, 3, â€¦\n$ Q8_3           <dbl> 2, 2, 1, 2, 2, 3, 2, 1, 3, 2, 2, 2, 1, 4, 2, 2, 2, 3, 2â€¦\n$ Q8_4           <dbl> 1, 2, 1, 2, 3, 2, 1, 1, 2, 2, 2, 3, 3, 2, 2, 2, 1, 2, 1â€¦\n$ Q8_5           <dbl> 1, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 4, 4, 2, 2, 1, 2, 2â€¦\n$ Q8_6           <dbl> 1, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2â€¦\n$ Q8_7           <dbl> 1, 3, 2, 2, 4, 2, 2, 4, 4, 2, 3, 1, 4, 4, 4, 4, 1, 4, 4â€¦\n$ Q8_8           <dbl> 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 3, 2, 2, 1, 1, 1, 1, 2â€¦\n$ Q8_9           <dbl> 4, 2, 1, 2, 2, 2, 2, 1, 2, 2, 3, 2, 3, 2, 1, 1, 1, 2, 1â€¦\n$ Q9_1           <dbl> 2, 1, 1, 1, 1, -1, 1, 1, 1, 2, 1, 2, 2, 3, 1, 1, 1, 1, â€¦\n$ Q9_2           <dbl> 2, 1, 2, 2, 4, -1, 2, 2, 4, 2, 2, 2, 3, 3, 4, 4, 4, 4, â€¦\n$ Q9_3           <dbl> 4, 3, 4, 4, 3, -1, 2, 3, 4, 3, 3, 4, 2, 3, 3, 3, 4, 4, â€¦\n$ Q9_4           <dbl> 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4â€¦\n$ Q10_1          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q10_2          <dbl> 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q10_3          <dbl> 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2â€¦\n$ Q10_4          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q11_1          <dbl> 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q11_2          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q11_3          <dbl> 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2â€¦\n$ Q11_4          <dbl> 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2â€¦\n$ Q11_5          <dbl> 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2â€¦\n$ Q11_6          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q14            <dbl> 5, 1, 5, 5, 1, -1, 1, 5, 2, 1, 1, 5, 1, 2, 1, 1, 2, 1, â€¦\n$ Q15            <dbl> 1, 1, 2, 1, 5, -1, 3, 1, 4, 5, 1, 2, 5, 2, 3, 5, 1, 4, â€¦\n$ Q16            <dbl> 1, 2, 1, 4, 1, -1, 3, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, â€¦\n$ Q17_1          <dbl> 1, 2, 1, 1, 2, -1, 3, 1, 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, â€¦\n$ Q17_2          <dbl> 1, 2, 3, 1, 2, -1, 2, 1, 1, 1, 2, 1, 4, 3, 1, 1, 1, 1, â€¦\n$ Q17_3          <dbl> 1, 2, 1, 1, 4, -1, 4, 1, 2, 1, 3, 1, 4, 3, 2, 3, 1, 4, â€¦\n$ Q17_4          <dbl> 3, 3, 1, 1, 4, -1, 2, 2, 4, 2, 2, 3, 2, 2, 4, 4, 1, 4, â€¦\n$ Q18_1          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_2          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_3          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_4          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_5          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_6          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_7          <dbl> 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_8          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2â€¦\n$ Q18_9          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q18_10         <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ Q19_1          <dbl> -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, â€¦\n$ Q19_2          <dbl> -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1â€¦\n$ Q19_3          <dbl> 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1â€¦\n$ Q19_4          <dbl> 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -â€¦\n$ Q19_5          <dbl> 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -â€¦\n$ Q19_6          <dbl> 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -â€¦\n$ Q19_7          <dbl> 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -â€¦\n$ Q19_8          <dbl> -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, â€¦\n$ Q19_9          <dbl> -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, â€¦\n$ Q19_10         <dbl> -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -â€¦\n$ Q20            <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1â€¦\n$ Q21            <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1â€¦\n$ Q22            <dbl> NA, NA, NA, NA, NA, 7, NA, NA, NA, NA, NA, NA, 6, NA, Nâ€¦\n$ Q23            <dbl> 2, 1, 2, 2, 1, -1, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 2, 1, â€¦\n$ Q24            <dbl> 1, 3, 1, 1, 3, 4, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 1â€¦\n$ Q25            <dbl> 1, 3, 2, 2, 1, 3, 1, 2, 1, 2, 3, 1, 2, 4, 1, 1, 1, 1, 1â€¦\n$ Q26            <dbl> 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1â€¦\n$ Q27_1          <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1â€¦\n$ Q27_2          <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1â€¦\n$ Q27_3          <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1â€¦\n$ Q27_4          <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1â€¦\n$ Q27_5          <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1â€¦\n$ Q27_6          <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1â€¦\n$ Q28_1          <dbl> 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, NA, -1, 1, -1, 1, â€¦\n$ Q28_2          <dbl> 1, -1, -1, 1, 1, NA, -1, -1, -1, 1, -1, 1, NA, -1, 1, -â€¦\n$ Q28_3          <dbl> 1, -1, -1, -1, 1, NA, 1, -1, 1, -1, -1, -1, NA, -1, 1, â€¦\n$ Q28_4          <dbl> 1, -1, -1, 1, -1, NA, 1, -1, -1, -1, 1, 1, NA, -1, 1, -â€¦\n$ Q28_5          <dbl> -1, -1, -1, -1, 1, NA, 1, -1, -1, -1, -1, 1, NA, -1, -1â€¦\n$ Q28_6          <dbl> -1, 1, -1, -1, -1, NA, -1, 1, -1, -1, -1, 1, NA, -1, 1,â€¦\n$ Q28_7          <dbl> 1, -1, 1, -1, 1, NA, -1, -1, 1, 1, -1, -1, NA, -1, 1, -â€¦\n$ Q28_8          <dbl> -1, -1, -1, -1, -1, NA, -1, -1, -1, -1, -1, -1, NA, 1, â€¦\n$ Q29_1          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q29_2          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, 1, NA, â€¦\n$ Q29_3          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q29_4          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q29_5          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q29_6          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q29_7          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q29_8          <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q29_9          <dbl> NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, -1, NA, â€¦\n$ Q29_10         <dbl> NA, NA, NA, NA, NA, -1, NA, NA, NA, NA, NA, NA, -1, NA,â€¦\n$ Q30            <dbl> 2, 3, 2, 2, 1, 5, 1, 2, 1, 3, 1, 2, 1, 5, 1, 1, 2, 1, 5â€¦\n$ Q31            <dbl> NA, NA, NA, NA, -1, NA, 1, NA, 1, NA, 1, NA, 2, NA, 1, â€¦\n$ Q32            <dbl> 1, NA, 2, 1, NA, NA, NA, 1, NA, NA, NA, 1, NA, NA, NA, â€¦\n$ Q33            <dbl> NA, 1, NA, NA, NA, -1, NA, NA, NA, 1, NA, NA, NA, 1, NAâ€¦\n$ ppage          <dbl> 73, 90, 53, 58, 81, 61, 80, 68, 70, 83, 43, 42, 48, 52,â€¦\n$ educ           <chr> \"College\", \"College\", \"College\", \"Some college\", \"High â€¦\n$ race           <chr> \"White\", \"White\", \"White\", \"Black\", \"White\", \"White\", \"â€¦\n$ gender         <chr> \"Female\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\",â€¦\n$ income_cat     <chr> \"$75-125k\", \"$125k or more\", \"$125k or more\", \"$40-75k\"â€¦\n$ voter_category <chr> \"always\", \"always\", \"sporadic\", \"sporadic\", \"always\", \"â€¦\n```\n\n\n:::\n:::\n\n\n\n## Cleaning Data: Missing Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoter_data |> \n  summarize(across(everything(), ~sum(is.na(.x)))) |> \n  pivot_longer(everything()) |> \n  filter(value > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 22 Ã— 2\n   name  value\n   <chr> <int>\n 1 Q22    5350\n 2 Q28_1   534\n 3 Q28_2   534\n 4 Q28_3   534\n 5 Q28_4   534\n 6 Q28_5   534\n 7 Q28_6   534\n 8 Q28_7   534\n 9 Q28_8   534\n10 Q29_1  4494\n# â„¹ 12 more rows\n```\n\n\n:::\n:::\n\n\n\n## Cleaning the Data: Q28, 29, 31\n\n-   What should we do with question 28?\n-   What should we do with question 29?\n-   What should we do with question 31?\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}