{
  "hash": "e694abcf95f96c6268fcca0a256ed845",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Evaluating Classification Models'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(janitor) # for next contingency tables\nlibrary(kableExtra)\nlibrary(ISLR2)\n\ntidymodels_prefer()\n```\n:::\n\n\n\n\n\n## Default Dataset {.smaller}\n\n::: columns\n::: column\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Default) |> kable()  # print first six observations\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|    income|\n|:-------|:-------|---------:|---------:|\n|No      |No      |  729.5265| 44361.625|\n|No      |Yes     |  817.1804| 12106.135|\n|No      |No      | 1073.5492| 31767.139|\n|No      |No      |  529.2506| 35704.494|\n|No      |No      |  785.6559| 38463.496|\n|No      |Yes     |  919.5885|  7491.559|\n\n\n:::\n:::\n\n\n:::\n\n::: column\n**Response Variable**: `default`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDefault |> \n  tabyl(default) |>  # class frequencies\n  kable()           # Make it look nice\n```\n\n::: {.cell-output-display}\n\n\n|default |    n| percent|\n|:-------|----:|-------:|\n|No      | 9667|  0.9667|\n|Yes     |  333|  0.0333|\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Split the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\ndefault_split <- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<6000/4000/10000>\n```\n\n\n:::\n\n```{.r .cell-code}\ndefault_train <- training(default_split)\ndefault_test <- testing(default_split)\n```\n:::\n\n\n\n## [K-Nearest Neighbors Classifier: Build Model]{.r-fit-text}\n\n-   **Response** ($Y$): `default`\n-   **Predictor** ($X$): `balance`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit <- nearest_neighbor(neighbors = 10) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>  \n  fit(default ~ balance, data = Default)   # fit 10-nn model\n```\n:::\n\n\n\n## [K-Nearest Neighbors Classifier: Predictions]{.r-fit-text} {.smaller}\n\n::: panel-tabset\n## Class labels\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(knnfit, new_data = Default, type = \"class\") |> head() |> kable()   # obtain predictions as classes\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n\n\n:::\n:::\n\n\n\n## Probabilities\n\n-   Predicts class w/ maximum probability\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(knnfit, new_data = Default, type = \"prob\") |> head() |> kable() # obtain predictions as probabilities\n```\n\n::: {.cell-output-display}\n\n\n| .pred_No| .pred_Yes|\n|--------:|---------:|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n\n\n:::\n:::\n\n\n:::\n\n## Fitting a logistic regression\n\nFitting a logistic regression model with `default` as the response and `balance` as the predictor:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogregfit <- logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |> kable()  # obtain results\n```\n\n::: {.cell-output-display}\n\n\n|term        |    estimate| std.error| statistic| p.value|\n|:-----------|-----------:|---------:|---------:|-------:|\n|(Intercept) | -10.6926385| 0.4659035| -22.95033|       0|\n|balance     |   0.0055327| 0.0002841|  19.47329|       0|\n\n\n:::\n:::\n\n\n\n## Making predictions in R\n\n::: panel-tabset\n\n## Class Labels\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |> kable()   # obtain class predictions\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n\n\n:::\n:::\n\n\n\n## Log-Odds\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |> kable()   # obtain log-odds predictions\n```\n\n::: {.cell-output-display}\n\n\n|         x|\n|---------:|\n| -6.819727|\n\n\n:::\n:::\n\n\n\n## Probabilities\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |> kable()  # obtain probability predictions\n```\n\n::: {.cell-output-display}\n\n\n|  .pred_No| .pred_Yes|\n|---------:|---------:|\n| 0.9989092| 0.0010908|\n\n\n:::\n:::\n\n\n\n:::\n\n\n# Assessing Performance of Classifiers\n\n## Binary Classifiers\n\n-   Start with binary classification scenarios\n-   With binary classification, designate one category as \"Success/Positive\" and the other as \"Failure/Negative\"\n    +   If relavent to your problem: \"Positive\" should be the thing you're trying to predict/care more about\n    +   Note: \"Positive\" $\\neq$ \"Good\"\n    +   For `default`: \"Yes\" is Positive\n-   Some metrics weight \"Positives\" more and viceversa\n\n## Confusion Matrix\n\n|                                  | Actual Positive/Event | Actual Negative/Non-event |\n|:--------------------------------:|:---------------------:|:-------------------------:|\n|   **Predicted Positive/Event**   |     True Negative (TN)     |      False Positive  (FP)     |\n| **Predicted Negative/Non-event** |    False Negative (FN)    |       True Positive  (TP)     |\n\n## Adding predictions to tibble\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds <- default_test |> \n  mutate(\n    knn_preds = predict(knnfit, new_data = default_test, type = \"class\")$.pred_class,\n    logistic_preds = predict(logregfit, new_data = default_test, type = \"class\")$.pred_class\n  )\n\ndefault_test_wpreds |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income|knn_preds |logistic_preds |\n|:-------|:-------|---------:|--------:|:---------|:--------------|\n|No      |No      |  729.5265| 44361.63|No        |No             |\n|No      |Yes     |  808.6675| 17600.45|No        |No             |\n|No      |Yes     | 1220.5838| 13268.56|No        |No             |\n|No      |No      |  237.0451| 28251.70|No        |No             |\n|No      |No      |  606.7423| 44994.56|No        |No             |\n|No      |No      |  286.2326| 45042.41|No        |No             |\n\n\n:::\n:::\n\n\n\n## KNN: Confusion Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  3854   80\n       Yes   17   49\n```\n\n\n:::\n:::\n\n\n\n## KNN: Confusion Matrix (Sexy)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Logistic Regression: Confusion Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = logistic_preds) |> \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n## Classification Metrics {.smaller}\n\n-   Accuracy: proportion of your classes that are correct $$(TP + TN)/Total$$\n-   Recall/Sensitivity: proportion of true positives correct (true positive rate) $$TP/(TP+FN)$$\n-   Precision/Positive Predictive Value (PPV): proportion of predicted positive that are correct $$TP/(TP+FP)$$\n-   Specificity: proportion of true negatives correct (true negative rate) $$TN/(TN+FP)$$\n-   Negative Predictive Value (NPV): proportion of predicted negatives that are correct $$TN/(TN+FN)$$\n\n## KNN: Performance {.smaller}\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: column\n-   Accuracy: $(3854+49)/4000 = .976 = 97.6\\%$\n-   Recall/Sensitivity: $49/(49+80) = 0.380 = 38.0\\%$\n-   Precision/Positive Predictive Value (PPV): $49/(49+17) = .742 = 74.2\\%$\n-   Specificity: $3854/(3854+17) = 0.996 = 99.6\\%$\n-   Negative Predictive Value (NPV): $3854/(3854+80) = 98.0$\n:::\n::::\n\n## Logistic Regression: Performance {.smaller}\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = logistic_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: column\nCompute the following and write your answers on the board:\n\n-   Accuracy\n-   Recall/Sensitivity\n-   Precision/Positive Predictive Value (PPV)\n-   Specificity\n-   Negative Predictive Value (NPV)\n:::\n::::\n\n## Performance Metrics with `yardstick`\n\n-   `yardstick` is a package that ships with `tidymodels` meant for model evaluation\n-   Typical syntax: `metricname(data, truth, estimate, ...)`\n    +   Bind original data with predicted observations\n    +   Put true response in for `truth` and predicted values in for `estimate`\n    \n## Logistic Regression: Accuracy {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  accuracy(truth = default, estimate = logistic_preds) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |binary     |     0.973|\n\n\n:::\n:::\n\n\n\n## Two More Metrics {.smaller}\n\n-   Matthews correlation coefficient (MCC): similar to $R^2$ but for classification\n$$\\frac{TP\\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n    +   Good for imbalanced data\n    +   Considers both positives and negatives\n-   F-Measure: harmonic mean of recall and precision\n$$\\frac{2}{recall^{-1} + precision^{-1}} = \\frac{2TP}{2TP+FP+FN}$$\n    +   Focuses more on positives\n    +   bad of imbalanced data\n\n## Metric Sets\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinary_metrics <- metric_set(accuracy, recall, precision, specificity,\n                             npv, mcc, f_meas)\n```\n:::\n\n\n\n-   Can apply this to compute a bunch of metrics\n\n\n## KNN: Performance {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  binary_metrics(truth = default, estimate = knn_preds, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric     |.estimator | .estimate|\n|:-----------|:----------|---------:|\n|accuracy    |binary     | 0.9757500|\n|recall      |binary     | 0.3798450|\n|precision   |binary     | 0.7424242|\n|specificity |binary     | 0.9956084|\n|npv         |binary     | 0.9796645|\n|mcc         |binary     | 0.5206828|\n|f_meas      |binary     | 0.5025641|\n\n\n:::\n:::\n\n\n\n## Logistic Regression: Performance {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  binary_metrics(truth = default, estimate = logistic_preds, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric     |.estimator | .estimate|\n|:-----------|:----------|---------:|\n|accuracy    |binary     | 0.9730000|\n|recall      |binary     | 0.3023256|\n|precision   |binary     | 0.6842105|\n|specificity |binary     | 0.9953500|\n|npv         |binary     | 0.9771747|\n|mcc         |binary     | 0.4437097|\n|f_meas      |binary     | 0.4193548|\n\n\n:::\n:::\n\n\n\n## Discussion\n\n-   For each of the following metrics, brainstorm a situation in which that metric is probably the most important:\n    +   Recall\n    +   Precision\n    +   Accuracy\n\n# Thresholding\n\n\n## Using a threshold {.smaller}\n\n- Step 1: Predict **probabilities** for all observations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wprobs <- default_test |> \n  mutate(\n    knn_probs = predict(knnfit, new_data = default_test, type = \"prob\")$.pred_Yes,\n    logistic_probs = predict(logregfit, new_data = default_test, type = \"prob\")$.pred_Yes\n  )\n\ndefault_test_wprobs |> head() |> kable()   # obtain probability predictions\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income| knn_probs| logistic_probs|\n|:-------|:-------|---------:|--------:|---------:|--------------:|\n|No      |No      |  729.5265| 44361.63|         0|      0.0012842|\n|No      |Yes     |  808.6675| 17600.45|         0|      0.0019883|\n|No      |Yes     | 1220.5838| 13268.56|         0|      0.0190870|\n|No      |No      |  237.0451| 28251.70|         0|      0.0000843|\n|No      |No      |  606.7423| 44994.56|         0|      0.0006514|\n|No      |No      |  286.2326| 45042.41|         0|      0.0001107|\n\n\n:::\n:::\n\n\n\n## Using a threshold {.smaller}\n\n- Step 1: Predict **probabilities** for all observations\n- Step 2: Set a threshold to obtain **class labels** (0.5 below)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.5   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs > threshold, \"Yes\", \"No\"))\n  )\n\ndefault_test_wprobs |> head() |> kable()      \n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income| knn_probs| logistic_probs|knn_preds |logistic_preds |\n|:-------|:-------|---------:|--------:|---------:|--------------:|:---------|:--------------|\n|No      |No      |  729.5265| 44361.63|         0|      0.0012842|No        |No             |\n|No      |Yes     |  808.6675| 17600.45|         0|      0.0019883|No        |No             |\n|No      |Yes     | 1220.5838| 13268.56|         0|      0.0190870|No        |No             |\n|No      |No      |  237.0451| 28251.70|         0|      0.0000843|No        |No             |\n|No      |No      |  606.7423| 44994.56|         0|      0.0006514|No        |No             |\n|No      |No      |  286.2326| 45042.41|         0|      0.0001107|No        |No             |\n\n\n:::\n:::\n\n\n\n## Using a threshold {.smaller}\n\n- Step 1: Predict **probabilities** for all observations\n- Step 2: Set a threshold to obtain **class labels** (0.5 below)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.5   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs > threshold, \"Yes\", \"No\"))\n  )\n\ndefault_test_wprobs |> head() |> kable()      \n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income| knn_probs| logistic_probs|knn_preds |logistic_preds |\n|:-------|:-------|---------:|--------:|---------:|--------------:|:---------|:--------------|\n|No      |No      |  729.5265| 44361.63|         0|      0.0012842|No        |No             |\n|No      |Yes     |  808.6675| 17600.45|         0|      0.0019883|No        |No             |\n|No      |Yes     | 1220.5838| 13268.56|         0|      0.0190870|No        |No             |\n|No      |No      |  237.0451| 28251.70|         0|      0.0000843|No        |No             |\n|No      |No      |  606.7423| 44994.56|         0|      0.0006514|No        |No             |\n|No      |No      |  286.2326| 45042.41|         0|      0.0001107|No        |No             |\n\n\n:::\n:::\n\n\n\n## Performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_metrics <- metric_set(accuracy, sensitivity, specificity)\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.976\n2 sensitivity binary         0.996\n3 specificity binary         0.380\n```\n\n\n:::\n:::\n\n\n\n## Low Threshold\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.1   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\"))\n  )\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.923\n2 sensitivity binary         1    \n3 specificity binary         0.921\n```\n\n\n:::\n:::\n\n\n\n## High Threshold\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.9   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\"))\n  )\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary        0.968 \n2 sensitivity binary        0.0233\n3 specificity binary        1     \n```\n\n\n:::\n:::\n\n\n\n## ROC Curve and AUC\n\n- **ROC (Receiver Operating Characteristics) curve**: popular graphic for comparing different classifiers across all possible thresholds\n  + Plots the (1-Specificity) along the x-axis and the Sensitivity (true positive rate) along the y-axis\n- **AUC**: area under the AUC curve\n  + Ideal ROC curve will hug the top left corner\n\n## ROC Curve\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |> \n  head() |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n| .threshold| specificity| sensitivity|\n|----------:|-----------:|-----------:|\n|       -Inf|   0.0000000|           1|\n|     0.0000|   0.0000000|           1|\n|     0.0145|   0.8915009|           1|\n|     0.0415|   0.8997675|           1|\n|     0.0560|   0.9062258|           1|\n|     0.0655|   0.9064841|           1|\n\n\n:::\n:::\n\n\n\n## ROC Curve: Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n## ROC Curve\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_auc(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |binary     | 0.9838903|\n\n\n:::\n:::\n\n\n\n## ROC AUC\n\n-   Measures how good your model is at separating categories\n-   Only for binary classification\n-   What should be the minimum AUC?\n-   What should be that maximum possible AUC?",
    "supporting": [
      "08-classifiction-metrics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}