{
  "hash": "220ddfbae95754e89c3a1df7057ea08b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Class Imbalance'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: false\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n    \n## Exploring with App {.smaller}\n\n-   [App](https://efriedlander.shinyapps.io/ClassificationMetrics/)\n    +   Break into groups\n    +   Investigate how your performance metrics change between balanced data and unbalanced data\n    +   Additional Considerations:\n        +   Impact of boundaries/models?\n        +   Impact of sample size?\n        +   Impact of noise level?\n    +   Please write down observations so we can discuss\n    \n# Dealing with Class-Imbalance\n\n##  Class-Imbalance\n\n-   Class-imbalance occurs where your the classes in your response greatly differ in terms of how common they are\n-   Occurs frequently:\n    +   Medicine: survival/death\n    +   Admissions: enrollment/non-enrollment\n    +   Finance: repaid loan/defaulted\n    +   Tech: Clicked on ad/Didn't click\n    +   Tech: Churn rate\n    +   Finance: Fraud\n\n## Data: `haberman` {.smaller}\n\nStudy conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\n\nGoal: predict whether a patient survived after undergoing surgery for breast cancer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhaberman <- read_csv(\"../data/haberman.data\",\n                     col_names = c(\"Age\", \"OpYear\", \"AxNodes\", \"Survival\"))\nhaberman |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| Age| OpYear| AxNodes| Survival|\n|---:|------:|-------:|--------:|\n|  30|     64|       1|        1|\n|  30|     62|       3|        1|\n|  30|     65|       0|        1|\n|  31|     59|       2|        1|\n|  31|     65|       4|        1|\n|  33|     58|      10|        1|\n\n\n:::\n:::\n\n\n\n## Quick Clean\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhaberman <- haberman |> \n  mutate(Survival = factor(if_else(Survival == 1, \"Survived\", \"Died\"),\n                           levels = c(\"Died\", \"Survived\")))\nhaberman |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| Age| OpYear| AxNodes|Survival |\n|---:|------:|-------:|:--------|\n|  30|     64|       1|Survived |\n|  30|     62|       3|Survived |\n|  30|     65|       0|Survived |\n|  31|     59|       2|Survived |\n|  31|     65|       4|Survived |\n|  33|     58|      10|Survived |\n\n\n:::\n:::\n\n\n\n## Split Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\nhab_splits <- initial_split(haberman, prop = 0.75, strata = Survival)\nhab_train <- training(hab_splits)\nhab_test <- testing(hab_splits)\n```\n:::\n\n\n\n## Visualizing Response\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_train |> \n  ggplot(aes(y = Survival)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n## Fitting Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_model <- logistic_reg() |> \n  set_engine(\"glm\")\n\nlr_fit <- lr_model |> \n  fit(Survival ~ . , data = hab_train)\n```\n:::\n\n\n\n## Confusion Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_fit |> augment(new_data = hab_test) |> \n  conf_mat(truth = Survival, estimate = .pred_class) |> autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n## Performance Metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_metrics <- metric_set(accuracy, precision, recall)\n\nlr_fit |> augment(new_data = hab_test) |> \n  roc_auc(truth = Survival, .pred_Died) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |binary     | 0.7284879|\n\n\n:::\n\n```{.r .cell-code}\nlr_fit |> augment(new_data = hab_test) |> \n  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator | .estimate|\n|:---------|:----------|---------:|\n|accuracy  |binary     | 0.7692308|\n|precision |binary     | 0.8000000|\n|recall    |binary     | 0.1904762|\n\n\n:::\n:::\n\n\n\n## Recall is BAD!\n\n-   Since there are so few deaths, model always predicts a low probability of death\n-   Idea: just because you you have a HIGHER probability of death doesn't mean have a HIGH probability of death\n\n## What do we do?\n\n-   Depends on what your goal is...\n-   Ask yourself: What is most important to my problem?\n    +   Accurate probabilities?\n    +   Overall accuracy?\n    +   Effective identification of a specific class (e.g. positives)?\n    +   Low false-positive rate?\n-   Discussion: Let's think of scenarios where each one of these is the most important.\n\n## Solutions to Class Imbalance {.smaller}\n\n-   Adjust probability threshold (we've already done this)\n    +   If you wanted to increase your recall would you increase or decrease your threshold?\n-   Sampling-based solutions (done during pre-processing)\n    +   Over-sample minority class\n    +   Under-sample majority class\n    +   Combination of both (e.g. SMOTE)\n-   Weight class/objective function\n\n## Over-sampling minority class\n\n-   Upsample: think bootstrapping for final sample is larger than original\n-   Idea: upsample minority class until it is same size(ish) as majority class\n\n## Visualizing Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_train |> ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n## Upsample Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(themis)\nupsample_recipe <- recipe(Survival ~ ., data = hab_train) |> \n  step_upsample(Survival, over_ratio = 1)\n\nhab_upsample <- upsample_recipe |> prep(hab_train) |> bake(new_data = NULL)\n```\n:::\n\n\n\n## Upsampled Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_upsample |>  ggplot(aes(x = Survival)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing Upsampled Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_upsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing Upsampled Data: No Jitter\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_upsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n## Performance consideration\n\n-   Pro:\n    +   Preserves all information in the data set\n-   Con:\n    +   Models will probably over-align to the noise in the minority class\n    \n## Under-sampling majority class\n\n-   Downsample: collect a random sample smaller than the original sample\n-   Idea: down sample majority class until it is same size(ish) as minority class\n\n## Visualizing Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_train |> ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Downsample Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndownsample_recipe <- recipe(Survival ~ ., data = hab_train) |> \n  step_downsample(Survival, under_ratio = 1)\n\nhab_downsample <- downsample_recipe |> prep(hab_train) |> bake(new_data = NULL)\n```\n:::\n\n\n\n## Downsample Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_downsample |>  ggplot(aes(x = Survival)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing Downsampled Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_downsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing Downsampled Data: No Jitter\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_downsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n## Performance considerations\n\n-   Pro:\n    +   Model doesn't over-align to noise in minority class\n-   Con:\n    +   Lose information from majority class\n    \n## SMOTE\n\n-   Basic idea:\n    +   Both upsample minority and downsample majority (Tidymodel implementation only upsamples)\n-   Better Upsampling: Instead of just randomly replicating minority observations\n    +   Find (minority) nearest neighbors of each minority observation\n    +   Interpolate line between them\n    +   Upsample by randomly generating points in interpolated lines\n\n## Visualizing Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_train |> ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n## SMOTE Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmote_recipe <- recipe(Survival ~ ., data = hab_train) |> \n  step_normalize(all_numeric_predictors()) |> \n  step_smote(Survival, over_ratio = 1, neighbors = 5)\n\nhab_smote <- smote_recipe |> prep(hab_train) |> bake(new_data = NULL)\n```\n:::\n\n\n\n## SMOTE Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_smote |>  ggplot(aes(x = Survival)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing SMOTE Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_smote |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_jitter()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing SMOTE Data: No Jitter\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_smote |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n\n## Performance considerations\n\n-   Pro:\n    +   Model doesn't over-align (as much) to noise in minority class\n    +   Don't lose (as much) information from majority class\n-   Con:\n    +   Creating new information out of nowhere\n    \n## Fitting models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noversamp_fit <- workflow() |> add_recipe(upsample_recipe) |> \n  add_model(lr_model) |> fit(hab_train)\ndownsamp_fit <- workflow() |> add_recipe(downsample_recipe) |> \n  add_model(lr_model)  |> fit(hab_train)\nsmote_fit <- workflow() |> add_recipe(smote_recipe) |> \n  add_model(lr_model)  |> fit(hab_train)\n```\n:::\n\n\n\n## Evaluate Performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noversamp_fit |> augment(new_data = hab_test) |> \n  roc_auc(truth = Survival, .pred_Died) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |binary     | 0.7343358|\n\n\n:::\n\n```{.r .cell-code}\ndownsamp_fit |> augment(new_data = hab_test) |> \n  roc_auc(truth = Survival, .pred_Died) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |binary     | 0.7243108|\n\n\n:::\n\n```{.r .cell-code}\nsmote_fit |> augment(new_data = hab_test) |> \n  roc_auc(truth = Survival, .pred_Died) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |binary     | 0.7251462|\n\n\n:::\n:::\n\n\n\n## Evaluate Performance {.smaller}\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\noversamp_fit |> augment(new_data = hab_test) |> \n  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator | .estimate|\n|:---------|:----------|---------:|\n|accuracy  |binary     | 0.7692308|\n|precision |binary     | 0.5714286|\n|recall    |binary     | 0.5714286|\n\n\n:::\n:::\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\ndownsamp_fit |> augment(new_data = hab_test) |> \n  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator | .estimate|\n|:---------|:----------|---------:|\n|accuracy  |binary     | 0.7435897|\n|precision |binary     | 0.5217391|\n|recall    |binary     | 0.5714286|\n\n\n:::\n:::\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nsmote_fit |> augment(new_data = hab_test) |> \n  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator | .estimate|\n|:---------|:----------|---------:|\n|accuracy  |binary     | 0.7435897|\n|precision |binary     | 0.5217391|\n|recall    |binary     | 0.5714286|\n\n\n:::\n:::\n\n\n\n# Weighting Observations/Objective Function\n\n## Creating Importance Weights\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(hardhat)\nhab_train <- hab_train |> \n  mutate(weights = ifelse(Survival == \"Died\", 4, 1),\n         weights = importance_weights(weights))\n\nhab_train |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| Age| OpYear| AxNodes|Survival | weights|\n|---:|------:|-------:|:--------|-------:|\n|  34|     59|       0|Died     |       4|\n|  34|     66|       9|Died     |       4|\n|  38|     69|      21|Died     |       4|\n|  39|     66|       0|Died     |       4|\n|  41|     67|       0|Died     |       4|\n|  42|     69|       1|Died     |       4|\n\n\n:::\n:::\n\n\n\n## Weighted Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted_wf <- workflow() |> \n  add_model(lr_model) |> \n  add_recipe(recipe(Survival ~ ., data = hab_train)) |> \n  add_case_weights(weights)\n\nweighted_fit <- weighted_wf |> \n  fit(hab_train)\n```\n:::\n\n\n\n## Model Performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted_fit |> augment(new_data = hab_test) |> \n  conf_mat(truth = Survival, estimate = .pred_class) |> autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](30-imbalance-2_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n\n## Model Performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweighted_fit |> augment(new_data = hab_test) |> \n  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator | .estimate|\n|:---------|:----------|---------:|\n|accuracy  |binary     | 0.5256410|\n|precision |binary     | 0.3461538|\n|recall    |binary     | 0.8571429|\n\n\n:::\n\n```{.r .cell-code}\nweighted_fit |> augment(new_data = hab_test) |> \n  roc_auc(truth = Survival, .pred_Died) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |binary     | 0.7142857|\n\n\n:::\n:::\n\n\n\n# Final Note / Alternative Metrics\n\n## Model vs Decisions {.smaller}\n\n-   Helpful framework for thinking about this:\n    -   Divide model predictions from decisions\n    -   Usually, model predicts a probability, then you make a classification based on that probability\n    -   Choosing the best model probably means (1) calibrating your probabilities correctly, then (2) making classifications/decisions to optimze your use-case\n\n## Scoring Rules {.smaller}\n\n-   Scoring rule: metric that evaluates probabilities\n-   Notation:\n    +   $\\hat{p}_{ik}$: predicted probability observation $i$ is in class $k$\n    +   $y_{ik}$: 1 if observation $i$ is in class $k$, 0 otherwise\n    +   $K$: number of classes\n    +   $N$: number of observations\n-   Brier Score: think MSE for probabilities \n    +   Binary: $\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{p}_i - y_i)^2$\n    +   Multiclass: $\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} (\\hat{p}_{ik} - y_{ik})^2$\n-   Logorithmic Score:\n    +   Binary: $-\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]$\n    +   Multi-class: $-\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{p}_{ik})$\n\n## Scoring our models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhab_scores <- metric_set(brier_class, mn_log_loss, roc_auc)\nall_scores <- lr_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = \"Logistic\") |> \n  bind_rows(oversamp_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = \"Oversample\")) |> \n  bind_rows(downsamp_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = \"Undersample\")) |> \n  bind_rows(smote_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = \"SMOTE\")) |> \n  bind_rows(weighted_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = \"Weighted\"))\n```\n:::\n\n\n\n## Scores\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_scores |>\n  select(-.estimator) |> \n  pivot_wider(names_from = .metric, values_from = .estimate) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|model       | brier_class| mn_log_loss|   roc_auc|\n|:-----------|-----------:|-----------:|---------:|\n|Logistic    |   0.1678374|   0.5161121| 0.7284879|\n|Oversample  |   0.2115716|   0.6217420| 0.7343358|\n|Undersample |   0.2130135|   0.6298389| 0.7243108|\n|SMOTE       |   0.2169736|   0.6304324| 0.7251462|\n|Weighted    |   0.2599917|   0.7217460| 0.7142857|\n\n\n:::\n:::\n\n\n\n## Conclusion\n\n-   Many different approaches and strategies depending on data\n-   First strategy: tresholding\n-   Many times method depends on model algorithm\n-   Make sure to ask \"Is imbalance really a problem here?\"\n",
    "supporting": [
      "30-imbalance-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}