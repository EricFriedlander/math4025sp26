{
  "hash": "24a0c262f00b8c25e7363c27b8e5eca9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Boosting'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rpart.plot)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n## Exploring Bagging Using App\n\n-   [App](https://efriedlander.shinyapps.io/BVTappRegression/)\n\n## Ensemble Methods\n\n-   Single regression or classification trees usually have poor predictive performance.\n-   **Ensemble Methods**: use a collection of models (in this case, decision trees) to improve the predictive performance\n    -   Downside: Interpretability\n-   Last Time:\n    -   Bagging\n    -   Random Forests\n-   This Time:\n    -   Boosting\n    \n## Flexibility vs. Interpretability\n\n![Adapted from ISLR, James et al.](images/25/2_7-1.png)\n\n\n# Boosting\n\n## Boosting {.smaller}\n\n-   Still an ensemble method in that it creates a bunch of models (trees in this case)\n-   Trees are created *sequentially*, and fit to the *residuals* of the previous trees\n-   Idea: each additional tree to improve upon the previous trees by focusing on places where the previous trees perform poorly\n-   Each model in the process is a weak model, referred to as a **base learner**\n-   **learning slowly**: as more trees are fit, the overall ensemble gradually improves\n\n. . .\n\n-   Remind me: What is a residual?\n\n## Boosting Algorithm {.smaller}\n\nLet $B$ be the number of trees you want to fit and $d$ the maximum number of splits\n\n1.    Set $\\hat{f}(x) = 0$ and $r_i = y_i$ for all training set\n2.    For $b = 1,\\ldots, B$:\n        a.    Fit tree $\\hat{f}_b$ with $d$ splits ($d+1$ leaves) using psuedo-residuals $r$ as response and features $X$ as predictors.\n        b.    Update full model $\\hat{f}$: $\\hat{f} = \\hat{f} + \\lambda\\hat{f}_b$\n        c.    Update pseudo-residuals: $r_i = r_i - \\lambda\\hat{f}_b(x_i)$\n3. Output final model:\n$$\\hat{f}(x) = \\displaystyle \\sum_{b=1}^{B} \\lambda \\ \\hat{f}_b(x)$$\n-   It may be hard to see, but this is similar to gradient descent and so is called **gradient boosting**\n\n\n\n## Gradient Boosting\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](26-boosting_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Many Implementations of Boosting\n\n-   Any implementation of boosting you use will like be a tweak of this\n    +   AdaBoost\n    +   Catboost\n    +   LightGBM\n    +   **XGBoost**\n\n## Gradient Boosting Tuning Parameters\n\n-   [Documentation](https://parsnip.tidymodels.org/reference/boost_tree.html)\n\n# Gradient Boosting in R\n\n## Data: Voter Frequency\n\n-   [Info about data](https://github.com/fivethirtyeight/data/tree/master/non-voters)\n-   Goal: Identify individuals who are unlikely to vote to help organization target \"get out the vote\" effort.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoter_data <- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nvoter_clean <- voter_data |> \n  select(-RespId, -weight, -Q1) |>\n  mutate(\n    educ = factor(educ, levels = c(\"High school or less\", \"Some college\", \"College\")),\n    income_cat = factor(income_cat, levels = c(\"Less than $40k\", \"$40-75k \",\n                                               \"$75-125k\", \"$125k or more\")),\n    voter_category = factor(voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n  ) |> \n  filter(Q22 != 5 | is.na(Q22)) |> \n  mutate(Q22 = as_factor(Q22),\n         Q22 = if_else(is.na(Q22), \"Not Asked\", Q22),\n         across(Q28_1:Q28_8, ~if_else(.x == -1, 0, .x)),\n         across(Q28_1:Q28_8, ~ as_factor(.x)),\n         across(Q28_1:Q28_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n         across(Q29_1:Q29_10, ~if_else(.x == -1, 0, .x)),\n         across(Q29_1:Q29_10, ~ as_factor(.x)),\n         across(Q29_1:Q29_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n        Party_ID = as_factor(case_when(\n          Q31 == 1 ~ \"Strong Republican\",\n          Q31 == 2 ~ \"Republican\",\n          Q32 == 1  ~ \"Strong Democrat\",\n          Q32 == 2 ~ \"Democrat\",\n          Q33 == 1 ~ \"Lean Republican\",\n          Q33 == 2 ~ \"Lean Democrat\",\n          TRUE ~ \"Other\"\n        )),\n        Party_ID = factor(Party_ID, levels =c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n                                                \"Other\", \"Lean Democrat\", \"Democrat\", \"Strong Democrat\")),\n        across(!ppage, ~as_factor(if_else(.x == -1, NA, .x))))\n```\n:::\n\n\n\n## Split Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\nvoter_splits <- initial_split(voter_clean, prop = 0.7, strata = voter_category)\nvoter_train <- training(voter_splits)\nvoter_test <- testing(voter_splits)\n```\n:::\n\n\n\n## Define Model\n\n-   `trees`: VERY MUCH A TUNING PARAMETER NOW!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_model <- boost_tree(trees = tune(), learn_rate = tune(), tree_depth = tune(),\n                        min_n = tune()) |>\n  set_engine(\"xgboost\") |> # dont need importance\n  set_mode(\"classification\")\n```\n:::\n\n\n\n## Define Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_recipe <- recipe(voter_category ~ . , data = voter_train) |>\n  step_indicate_na(all_predictors()) |>\n  step_zv(all_predictors()) |>\n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |>\n  step_impute_median(all_numeric_predictors()) |>\n  step_impute_mode(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n```\n:::\n\n\n\n## Define Workflow and Fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_wf <- workflow() |>\n  add_model(gbm_model) |>\n  add_recipe(gbm_recipe)\n```\n:::\n\n\n\n## Creating Hyperparameter Grid\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_grid <- grid_latin_hypercube(trees(range = c(20, 200)),\n                                  tree_depth(range = c(1, 15)),\n                                  learn_rate(range = c(-10,-1)),\n                                  min_n(range = c(2, 40)), size = 50)\nvoter_folds = vfold_cv(voter_train, v = 5, repeats = 10, strata = voter_category)\n```\n:::\n\n\n\n## Tuning Hyperparameters in Parallel\n-   Check out [Chapter 10.4 of TMWR](https://www.tmwr.org/resampling#parallel)\n    +   Implementation depends on operating system\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doParallel)\n\ncl <- makePSOCKcluster(9)\nregisterDoParallel(cl)\n\ntuning_results <- tune_grid(\n  gbm_wf,\n  resamples= voter_folds,\n  grid = gbm_grid\n)\n\nstopCluster(cl)\n```\n:::\n\n\n\n## Results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_results)\n```\n\n::: {.cell-output-display}\n![](26-boosting_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n## Which one was best?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(tuning_results, metric = \"accuracy\") |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| trees| min_n| tree_depth| learn_rate|.config               |\n|-----:|-----:|----------:|----------:|:---------------------|\n|   107|    25|          7|   0.057429|Preprocessor1_Model24 |\n\n\n:::\n:::\n\n\n\n## Evaluate performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_fit <-  gbm_wf |>\n  finalize_workflow(select_best(tuning_results, metric = \"accuracy\")) |>\n  fit(voter_train)\n\naugment(gbm_fit, new_data = voter_test) |>\n  accuracy(truth = voter_category, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.653\n```\n\n\n:::\n:::\n\n\n\n## Variable Importance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\nvip(gbm_fit)\n```\n\n::: {.cell-output-display}\n![](26-boosting_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}