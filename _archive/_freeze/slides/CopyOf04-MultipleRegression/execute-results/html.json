{
  "hash": "dee30781a158d78e4d695f4ba4d370f3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MAT-427: Multiple Linear Regression + Data Splitting'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(modeldata)\nlibrary(knitr)\n\ntidymodels_prefer()\n```\n:::\n\n\n\n## Multiple Linear Regression\n\n- Response: $Y$\n- Predictor Variables: $X_1, X_2, \\ldots, X_p$\n- Assume true relationship:\n\n$$\n\\begin{aligned}\nY&=f(\\mathbf{X}) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\n\\end{aligned}\n$$\nwhere $\\beta_j$ quantifies the association between the $j^{th}$ predictor and the response.\n\n\n\n## [Multiple Linear Regression: Estimating Parameters]{.r-fit-text} {.smaller}\n\n- Suppose $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are estimates of $\\beta_0, \\beta_1, \\ldots, \\beta_p$\n- Training Data:\n  + Observed response: $y_i$ for $i=1,\\ldots,n$\n  + Observed predictors: $x_{1i}, x_{2i}, \\ldots x_{pi}$ for $i=1,\\ldots, n$\n- Predicted response: \n$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\ldots + \\hat{\\beta}_px_{pi} \\text{ for } i=1, \\ldots, n$$\n- Residuals: $e_i = \\hat{y}_i - y_i$ for $i=1, \\ldots, n$\n- Mean Squared Error (MSE): $MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}$\n\n## [Multiple Linear Regression: Estimating Parameters]{.r-fit-text}\n\n- **Goal:** Use *training data* to find $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ that minimizes MSE\n  + $\\hat{\\beta}_i$'s called **least-squares estimators**\n  + Since minimizing MSE $\\implies$ MSE is called **cost/loss function**\n- Can use calculus or gradient descent to find $\\hat{\\beta}_i$'s\n\n\n\n## House Prices dataset {.smaller}\n\n- `size` is in square feet\n- `num_bedrooms` is a count\n- `price` is in $1,000's\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouse_prices <- readRDS(\"../data/house_prices.rds\")   # load dataset\nhead(house_prices, 6) |> kable()  # print first 6 observations\n```\n\n::: {.cell-output-display}\n\n\n| size| num_bedrooms| price|\n|----:|------------:|-----:|\n| 2104|            3| 399.9|\n| 1600|            3| 329.9|\n| 2400|            3| 369.0|\n| 1416|            2| 232.0|\n| 3000|            4| 539.9|\n| 1985|            4| 299.9|\n\n\n:::\n:::\n\n\n\n\n## Multiple Linear Regression\n\nSome Exploratory Data Analysis (EDA)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GGally)\nggpairs(data = house_prices)   # correlation plot\n```\n\n::: {.cell-output-display}\n![](CopyOf04-MultipleRegression_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n## Multiple Linear Regression in R {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_model <- linear_reg() |> \n  set_engine(\"lm\")\n\nhouse_price_mlr <- mlr_model |> \n  fit(price ~ size + num_bedrooms, data = house_prices)   # fit the model\n\nhouse_price_mlr |> \n  tidy() |>   # produce result summaries of the model\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term         |   estimate|  std.error|  statistic|   p.value|\n|:------------|----------:|----------:|----------:|---------:|\n|(Intercept)  | 89.5977660| 41.7674230|  2.1451591| 0.0374991|\n|size         |  0.1392106|  0.0147951|  9.4092391| 0.0000000|\n|num_bedrooms | -8.7379154| 15.4506975| -0.5655353| 0.5745825|\n\n\n:::\n:::\n\n\n\n## [Multiple Linear Regression: Interpreting Parameters]{.r-fit-text} {.smaller}\n\n- $\\hat{\\beta}_0=89.5978$: The intercept $\\implies$ a house with 0 square feet and 0 bedrooms would cost approximately \\$89,598.80. Is this meaningful in context? [Not really]{.fragment .fade-in}\n- $\\hat{\\beta}_1=0.1392$: With `num_bedrooms` remaining fixed, an additional 1 square foot of `size` leads to an increase in `price` by approximately \\$139.20.\n- $\\hat{\\beta}_2=-8.7379$: With `size` remaining fixed, an additional bedroom leads to an decrease in `price` of approximately \\$8,737.90.\n\n. . .\n\n- Hmm.... that's a little weird...\n- **Simpson's Paradox:** when relationship between two variables disappears or reverses when controlling for a third, **confounding variable**\n\n## [Multiple Linear Regression: Interpreting Parameters]{.r-fit-text}\n\n:::{.incremental}\n- Write down our model in mathematical notation\n- $\\text{price} = 89.5978 + 0.1392\\times\\text{size} - 8.7379\\times\\text{num_bedrooms}$\n- $Y = 89.5978 + 0.1392X_1 - 8.7379X_2$\n:::\n\n\n## Multiple Linear Regression: Prediction {.smaller}\n\n- Prediction of `price` when `size` is 2000 square feet for a house with 3 bedrooms\n- $\\text{sales} = 89.5978 + 0.1392\\times2000 - 8.7379\\times3 = 341.7841$\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(house_price_mlr, new_data = tibble(size = 2000, num_bedrooms = 3))   # obtain prediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  .pred\n  <dbl>\n1  342.\n```\n\n\n:::\n:::\n\n\n- Why don't these match exactly? [**rounding**]{.fragment .fade-in}\n\n\n## Linear Regression: Comparing Models {.smaller}\n\n:::{.fragment}\n:::{.incremental}\n- Many methods for comparing regression models from your regression course\n- Today: Data splitting\n- First: New Data\n:::\n:::\n:::{.fragment}\n- **ames housing data**\n  + Many variables\n  + Focus on:\n    * `Sale_Price`: in dollars\n    * `Gr_Liv_Area`: size in square feet\n    * `Bedroom_AbvGr`: number of bedrooms above grade\n:::\n\n\n\n## Comparing Models: Data Splitting {.smaller}\n\n- Split `ames` data set into two parts\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Test set: randomly selected proportion $1-p$ of data used for estimating prediction error\n- If comparing A LOT of models, split into *three* parts to prevent **information leakage**\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Validation set: randomly selected proportion $q$ (typically 20-30%) of data used to choosing tuning parameters\n  + Test set: randomly selected proportion $1-p-q$ of data used for estimating prediction error\n- Idea: use data your model hasn't seen to get more accurate estimate of error and prevent overfitting\n\n## Comparing Models: Data Splitting with `tidymodels` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427) # Why?\n\names_split <- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<2049/881/2930>\n```\n\n\n:::\n\n```{.r .cell-code}\names_train <- training(ames_split) # get training data\names_test <- testing(ames_split) # get test data\n```\n:::\n\n\n\n- `strata` not necessary but good practice\n  + `strata` will use *stratified sampling* on the variable you specify (very little downside) \n\n## Linear Regression: Comparing Models {.smaller}\n\n- Let's create three models with `Sale_Price` as the response:\n  + **fit1**: a linear regression model with `Bedroom_AbvGr`  as the only predictor\n  + **fit2**: a linear regression model with `Gr_Liv_Area` as the only predictor\n  + **fit3** (similar to model in previous slides): a multiple regression model with `Gr_Liv_Area` and `Bedroom_AbvGr` as predictors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- mlr_model |> fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set\nfit2 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)\nfit3 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)\nfit4 <- mlr_model |> fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)\n```\n:::\n\n\n\n## Computing MSE {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 1\nfit1_train_mse <- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)\nfit1_test_mse <- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)\n\n# Fit 2\nfit2_train_mse <- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)\nfit2_test_mse <- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit3_train_mse <- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)\nfit3_test_mse <- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit4_train_mse <- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)\nfit4_test_mse <- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)\n```\n:::\n\n\n\n## [Question]{style=\"color:blue\"}\n\nWithout looking at the numbers\n\n1. Do we know which of the following is the smallest: `fit1_train_mse`, `fit2_train_mse`, `fit3_train_mse`, `fit4_train_mse`? [Yes, `fit4_train_mse`]{.fragment .fade-in}\n2. Do we know which of the following is the smallest: `fit1_test_mse`, `fit2_test_mse`, `fit3_test_mse`, `fit4_test_mse`? [No]{.fragment .fade-in}\n\n## Choosing a Model {.smaller}\n\n::::{.columns}\n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training Errors\nc(fit1_train_mse, fit2_train_mse, \n  fit3_train_mse, fit4_train_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6213135279 3188099910 2781293767 2472424544\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_train_mse, fit2_train_mse, \n            fit3_train_mse, fit4_train_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# test Errors\nc(fit1_test_mse, fit2_test_mse, \n  fit3_test_mse, fit4_test_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.329031e+09 3.203895e+09 2.732389e+09 2.726084e+12\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_test_mse, fit2_test_mse, \n            fit3_test_mse, fit4_test_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n:::\n:::{.column}\n- `fit4` has the lowest training MSE (to be expected)\n- `fit3` has the lowest test MSE\n  + We would choose `fit3`\n- Anything else interesting we see?\n:::\n::::\n\n# K-Nearest Neighbors\n\n## Regression: Conditional Averaging {.smaller}\n\n**Restaurant Outlets Profit dataset**\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](CopyOf04-MultipleRegression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\nWhat is a good value of $\\hat{f}(x)$ (expected profit), say at $x=6$?\n\nA possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.\n\n\n## K-Nearest Neighbors (KNN) Regression  {.smaller}\n\n- Non-parametric approach\n- Formally: Given a value for $K$ and a test data point $x_0$,\n$$\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) $$\nwhere $\\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.\n- Informally, average together the $K$ \"closest\" observations in your training set\n- \"Closeness\": usually use the **Euclidean metric** to measure distance\n- Euclidean distance between $\\mathbf{X}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})$ and $\\mathbf{x}_j=(x_{j1}, x_{j2}, \\ldots, x_{jp})$:\n$$||\\mathbf{x}_i-\\mathbf{x}_j||_2 = \\sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \\ldots + (x_{ip}-x_{jp    })^2}$$\n\n## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}\n\n::::{.columns}\n:::{.column}\n**$K=1$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit1 <- nearest_neighbor(mode = \"regression\", neighbors = 1) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|   .pred|\n|-------:|\n| 0.92695|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](CopyOf04-MultipleRegression_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n:::{.column}\n**$K=5$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit5 <- nearest_neighbor(mode = \"regression\", neighbors = 5) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|    .pred|\n|--------:|\n| 4.113736|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](CopyOf04-MultipleRegression_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n## Regression Methods: Comparison\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](CopyOf04-MultipleRegression_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n## <span style=\"color:blue\">Question!!!</span>\n\nAs $K$ in KNN regression increases:\n\n- the flexibility of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red} /decreases)\n- the bias of the fit $\\underline{\\hspace{5cm}}$ (increases/[decreases]{.fragment .highlight-red} )\n- the variance of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red}/decreases)\n\n\n## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}\n\n- Let's look at the `house_prices` data\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 3\n  Sale_Price Gr_Liv_Area Bedroom_AbvGr\n       <int>       <int>         <int>\n1     215000        1656             3\n2     105000         896             2\n3     172000        1329             3\n4     244000        2110             3\n5     189900        1629             3\n6     195500        1604             3\n```\n\n\n:::\n:::\n\n\n:::{.fragment}\n:::{.incremental}\n- Should 1 square foot count the same as 1 bedroom?\n- Need to **center and scale** (freq. just say scale)\n  + subtract mean from each predictor\n  + divide by standard deviation of each predictor\n  + compares apples-to-apples\n:::\n:::\n\n## Scaling in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# scale predictors\names_scaled <- tibble(size_scaled = scale(ames$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),\n                                  price = ames$Sale_Price)\n\nhead(ames_scaled)   # first six observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 3\n  size_scaled[,1] num_bedrooms_scaled[,1]  price\n            <dbl>                   <dbl>  <int>\n1           0.309                   0.176 215000\n2          -1.19                   -1.03  105000\n3          -0.338                   0.176 172000\n4           1.21                    0.176 244000\n5           0.256                   0.176 189900\n6           0.206                   0.176 195500\n```\n\n\n:::\n:::\n\n\n\n\n## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nknnfit10 <- knnreg(price ~ size_scaled + num_bedrooms_scaled, data = ames_scaled, k = 10)   # 10-nn regression\n```\n:::\n\n\n\n- Must also scale test data points **using mean and sd from training set!!!!**\n- Test Point: `size` = 2000 square feet, and `num_bedrooms` = 3, then\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtain 10-nn prediction\n\npredict(knnfit10, newdata = tibble(size_scaled = (2000 - mean(ames$Gr_Liv_Area))/sd(ames$Gr_Liv_Area),\n                                     num_bedrooms_scaled = (3 - mean(ames$Bedroom_AbvGr))/sd(ames$Bedroom_AbvGr)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 259490\n```\n\n\n:::\n:::\n\n\n\n\n## [Linear Regression vs K-Nearest Neighbors]{.r-fit-text} {.smaller}\n\n- Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.\n- Linear regression works for regression problems ($Y$ numerical), KNN can be used for both regression and classification - i.e. $Y$ qualitative (next lesson)\n- Linear regression is interpretable, KNN is not.\n- Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well while KNN does not allow for qualitative predictors\n- Performance: KNN can be pretty good for small $p$, that is, $p \\le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - *curse of dimensionality*\n\n## Classification Problems {.smaller}\n\n- Response $Y$ is qualitative (categorical).\n- Objective: build a classifier $\\hat{Y}=\\hat{C}(\\mathbf{X})$\n  + assigns class label to a future unlabeled (unseen) observations\n  + understand the relationship between the predictors and response\n- Two ways to make predictions\n  + Class probabilities\n  + Class labels\n\n## Classification Problems: Example\n\n**Default dataset**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)   # load library\ndata(\"Default\")   # load dataset\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Default)   # print first six observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(Default$default)   # class frequencies\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  No  Yes \n9667  333 \n```\n\n\n:::\n:::\n\n\n\n\n**We will consider `default` as the response variable.**\n\n\n## Classification Problems: Example\n\nFor some algorithms, we might need to convert the categorical response to numeric (0/1) values.\n\n**Default dataset**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDefault$default_id <- ifelse(Default$default == \"Yes\", 1, 0)   # create 0/1 variable\n\nhead(Default, 10)   # print first ten observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   default student   balance    income default_id\n1       No      No  729.5265 44361.625          0\n2       No     Yes  817.1804 12106.135          0\n3       No      No 1073.5492 31767.139          0\n4       No      No  529.2506 35704.494          0\n5       No      No  785.6559 38463.496          0\n6       No     Yes  919.5885  7491.559          0\n7       No      No  825.5133 24905.227          0\n8       No     Yes  808.6675 17600.451          0\n9       No      No 1161.0579 37468.529          0\n10      No      No    0.0000 29275.268          0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## K-Nearest Neighbors Classifier\n\nGiven a value for $K$ and a test data point $x_0$,\n$$P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)$$\n\nwhere $\\mathcal{N}_0$ is known as the **neighborhood** of $x_0$.\n\n\nFor classification problems, the predictions are obtained in terms of **majority vote** (unlike in regression where predictions are obtained by averaging).\n\n\n## K-Nearest Neighbors Classifier: Build Model\n\n**Default dataset**\n\nresponse ($Y$): `default` and predictor ($X$): `balance`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit <- knn3(default ~ balance, data = Default, k = 10)   # fit 10-nn model\n```\n:::\n\n\n\n\n## K-Nearest Neighbors Classifier: Predictions\n\n**Default dataset**\n\n* One can directly obtain the class label predictions as below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_class_preds_1 <- predict(knnfit, newdata = Default, type = \"class\")   # obtain default class label predictions\n```\n:::\n\n\n\n\n* Otherwise, one can first obtain predictions in terms of probabilities and then convert them into class label predictions based on a threshold.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_prob_preds <- predict(knnfit, newdata = Default, type = \"prob\")   # obtain predictions as probabilities\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.5   # set threshold\n\nknn_class_preds_2 <- factor(ifelse(knn_prob_preds[,2] > threshold, \"Yes\", \"No\"))   # obtain predictions as class labels\n```\n:::\n\n\n\n\n## K-Nearest Neighbors Classifier: Performance  {.smaller}\n\n**Default dataset**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create confusion matrix\n\n# use the following code only when all predictions are from the same class\n# levels(knn_class_preds_1) = c(\"No\", \"Yes\")\n\nconfusionMatrix(data = knn_class_preds_1, reference = Default$default, positive = \"Yes\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  9618  210\n       Yes   49  123\n                                          \n               Accuracy : 0.9741          \n                 95% CI : (0.9708, 0.9771)\n    No Information Rate : 0.9667          \n    P-Value [Acc > NIR] : 1.076e-05       \n                                          \n                  Kappa : 0.4752          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.3694          \n            Specificity : 0.9949          \n         Pos Pred Value : 0.7151          \n         Neg Pred Value : 0.9786          \n             Prevalence : 0.0333          \n         Detection Rate : 0.0123          \n   Detection Prevalence : 0.0172          \n      Balanced Accuracy : 0.6822          \n                                          \n       'Positive' Class : Yes             \n                                          \n```\n\n\n:::\n:::\n",
    "supporting": [
      "CopyOf04-MultipleRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}