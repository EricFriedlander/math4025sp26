{
  "hash": "80502c6e4ac78aa58ebc53511469f4f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Feature Selection and Regularization'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(fivethirtyeight) # for candy rankings data\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n## Data: Candy {.smaller}\n\nThe data for this lecture comes from the article FiveThirtyEight [*The Ultimate Halloween Candy Power Ranking*](https://fivethirtyeight.com/features/the-ultimate-halloween-candy-power-ranking) by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g. Reese's vs. Skittles). Click [here](http://walthickey.com/2017/10/18/whats-the-best-halloween-candy/) to check out some of the match ups.\n\nThe data set contains 12 characteristics and win percentage from 85 candies in the experiment.\n\n## Data: Candy\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 13\n$ competitorname   <chr> \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarterâ€¦\n$ chocolate        <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31â€¦\n$ pricepercent     <dbl> 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n## Data Cleaning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_rankings_clean <- candy_rankings |> \n  select(-competitorname) |> \n  mutate(sugarpercent = sugarpercent*100, # convert proportions into percentages\n         pricepercent = pricepercent*100, # convert proportions into percentages\n         across(where(is.logical), ~ factor(.x, levels = c(\"FALSE\", \"TRUE\")))) # convert logicals into factors\n```\n:::\n\n\n\n## Data Cleaning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 12\n$ chocolate        <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <fct> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <fct> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <fct> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 73.2, 60.4, 1.1, 1.1, 90.6, 46.5, 60.4, 31.3, 90.6, 6â€¦\n$ pricepercent     <dbl> 86.0, 51.1, 11.6, 51.1, 51.1, 76.7, 76.7, 51.1, 32.5,â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n## Data Splitting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_split <- initial_split(candy_rankings_clean, prop = 0.75, strata = winpercent)\ncandy_train <- training(candy_split)\ncandy_test <- testing(candy_split)\n```\n:::\n\n\n\n\n# Feature Selection for Linear Regression\n\n## What is feature selection? {.smaller}\n\n-   How do we choose what variables to include in our model?\n-   Up to now... include all of them... probably not the best\n-   Advantage of linear regression: interpretability\n-   Including every feature decreases interpretability\n-   Reasons for feature selection:\n    +   Improve model performance\n    +   Improve model interpretability\n-   Parsimony: simpler models are called more *parsimonious*\n-   Occam's Razor: more parsimonious models are better than less parsimonious models, holding all else constant\n\n## Types of feature selection\n\n-   Subset selection: Forward/Backward/Best-Subset Selection\n-   Shrinkage-based methods: LASSO and Ridge Regression\n-   Dimension reduction: consider linear combinations of predictors\n\n# Subset Selection\n\n## Exercise\n\n-   With your group, write out the steps for the following algorithms on the board\n    +   Group 1: Forward selection\n    +   Group 2: Backward elimination\n    +   Group 3: Step-wise selection\n    +   Group 4: Best-subset selection\n\n## Subset Selection in R {.smaller}\n\n-   `tidymodels` does not have an implementation for any subset selection techniques\n-   regularization (shrinkage-based) methods almost always perform better\n-   [`colino` package](https://stevenpawley.github.io/colino/) provides `tidymodels` implementation\n-   Other options\n    +   `caret` package\n    +   `olsrr` and `blorr` packages if you don't care about cross-validation\n    +   implement yourself\n    \n## Feature Selection in R\n\n-   When creating your recipe, don't need to always include all variables in your `recipe`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_recipe <- recipe(resp ~ var1 + var2 + var1*var2, data = training_data) |> \n  step_x(...)\n```\n:::\n\n\n\n## Re-using Recipe but changing formula\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnoint_recipe2 <- new_recipe |> \n  remove_formula() |> \n  add_formula(resp ~ var1 + var2)\n```\n:::\n\n\n\n# Shrinkage/Penalized/Regularization Methods\n\n## Question\n\n-   What criteria do we use to fit a linear regression model? Write down an expression with $\\hat{\\beta_i}$'s, $x_{ij}$'s, and $y_j$'s in it.\n\n## OLS {.smaller}\n\n-   Ordinary Least Squares Regression:\n\n$$\n\\begin{aligned}\n\\hat{\\beta} =\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}SSE(\\hat{\\beta}) &= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{y}_j)^2\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2\n\\end{aligned}\n$$\n\n-   $\\hat{\\beta} = (\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p)$ is the vector of all my coefficients\n-   $\\operatorname{argmin}$ is a function (operator) that returns the *arguments* that minimize the quantity it's being applied to\n\n## Ridge Regression {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_2^2\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\n\\end{aligned}\n$$\n\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients? [[THEY SHRINK!]{.span style=\"color:red;\"}]{.fragment .fade-in}\n-   $\\|\\cdot\\|_2$ is called the *$L_2$-norm*\n\n## But... but... but why?!? {.smaller}\n\n::: incremental\n-   Recall the Bias-Variance Trade-Off\n-   Our reducible error can partitioned into:\n    +   Bias: how much $\\hat{f}$ misses $f$ by *on average*\n    +   Variance: how much $\\hat{f}$ moves around from sample to sample\n-   Ridge: increase bias a little bit in exchange for large decrease in variance\n-   As we increase $\\lambda$ do we increase or decrease the penalty for large coefficients?\n-   As we increase $\\lambda$ do we increase or decrease the *flexibility* of our model?\n:::\n\n## LASSO {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_1\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\n\\end{aligned}\n$$\n\n-   LASSO: **L**east **A**bsolute **S**hrinkage and **S**election **O**perator\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients [THEY SHRINK!]{.span style=\"color:red;\"}\n-   $\\|\\cdot\\|_1$ is called the *$L_1$-norm*\n\n## Question\n\n-   What should happen to our coefficients as we *increase* $\\lambda$?\n\n## Ridge vs. LASSO\n\n::: incremental\n-   Ridge has a *closed-form solution*... how might we calculate it?\n-   Ridge has some nice linear algebra properties that makes it EXTREMELY FAST to fit\n-   LASSO has no *closed-form solution*... why?\n-   LASSO coefficients estimated *numerically*... how?\n    +   Gradient descent works (kind of) but something called **coordinate descent** is typically better\n-   MOST IMPORTANT PROPERTY OF LASSO: it *induces sparsity* while Ridge does not\n:::\n\n## Sparsity in Applied Math {.smaller}\n\n-   **sparse** typically means \"most things are zero\"\n-   Example: sparse matrices are matrices where most entries are zero\n    +   for large matrices this can provide HUGE performance gains\n-   LASSO *induces sparsity* by setting most of the parameter estimates to zero\n    +   this means it fits the model and does feature selection SIMULTANEOUSLY\n-   Let's do some board work to see why this is...\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}