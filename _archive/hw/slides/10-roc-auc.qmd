---
title: 'MATH 427: ROC and AUC'
author: Eric Friedlander
footer: "[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---

## Computational Set-Up

```{r}
library(tidyverse)
library(tidymodels)
library(knitr)
library(janitor) # for contingency tables
library(ISLR2)
library(ggforce) # sina plots

tidymodels_prefer()

set.seed(427)
```



## Default Dataset {.smaller}

::: columns
::: column
A simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.

```{r}
#| message: FALSE
head(Default) |> kable()  # print first six observations
```
:::

::: column
**Response Variable**: `default`

```{r}
Default |> 
  tabyl(default) |>  # class frequencies
  kable()           # Make it look nice
```
:::
:::

## Split the data

```{r}
set.seed(427)

default_split <- initial_split(Default, prop = 0.6, strata = default)
default_split

default_train <- training(default_split)
default_test <- testing(default_split)
```

## [K-Nearest Neighbors Classifier: Build Model]{.r-fit-text}

-   **Response** ($Y$): `default`
-   **Predictor** ($X$): `balance`

```{r}
knnfit <- nearest_neighbor(neighbors = 10) |> 
  set_engine("kknn") |> 
  set_mode("classification") |>  
  fit(default ~ balance, data = default_train)   # fit 10-nn model
```

## [K-Nearest Neighbors Classifier: Predictions]{.r-fit-text} {.smaller}

::: panel-tabset
## Class labels

```{r}
predict(knnfit, new_data = default_test, type = "class") |> head() |> kable()   # obtain predictions as classes
```

## Probabilities

-   Predicts class w/ maximum probability

```{r}
predict(knnfit, new_data = default_test, type = "prob") |> head() |> kable() # obtain predictions as probabilities
```
:::

## Fitting a logistic regression

Fitting a logistic regression model with `default` as the response and `balance` as the predictor:

```{r}
logregfit <- logistic_reg() |> 
  set_engine("glm") |> 
  fit(default ~ balance, data = default_train)   # fit logistic regression model

tidy(logregfit) |> kable()  # obtain results
```

## Making predictions in R

::: panel-tabset

## Class Labels

```{r}
predict(logregfit, new_data = tibble(balance = 700), type = "class") |> kable()   # obtain class predictions
```

## Log-Odds

```{r}
predict(logregfit, new_data = tibble(balance = 700), type = "raw") |> kable()   # obtain log-odds predictions
```

## Probabilities

```{r}
predict(logregfit, new_data = tibble(balance = 700), type = "prob") |> kable()  # obtain probability predictions
```

:::

## Binary Classifiers

-   Start with binary classification scenarios
-   With binary classification, designate one category as "Success/Positive" and the other as "Failure/Negative"
    +   If relevant to your problem: "Positive" should be the thing you're trying to predict/care more about
    +   Note: "Positive" $\neq$ "Good"
    +   For `default`: "Yes" is Positive
-   Some metrics weight "Positives" more and viceversa

## Last Time {.smaller}

-   Confusion Matrix
-   Metrics based on confusion matrix
    +   Accuracy
    +   Recall/Sensitivity
    +   Precision/PPV
    +   Specificity
    +   NPV
    +   MCC
    +   F-Measure
-   Today: ROC and AUC
    
# Thresholding


## Using a threshold {.smaller}

- Step 1: Predict **probabilities** for all observations

```{r}
default_test_wprobs <- default_test |>
  mutate(
    knn_probs = predict(knnfit, new_data = default_test, type = "prob") |> pull(.pred_Yes),
    logistic_probs = predict(logregfit, new_data = default_test, type = "prob") |> pull(.pred_Yes)
  )

default_test_wprobs |> head() |> kable()   # obtain probability predictions
```

## Using a threshold {.smaller}

- Step 1: Predict **probabilities** for all observations
- Step 2: Set a threshold to obtain **class labels** (0.5 below)

```{r}
threshold <- 0.5   # set threshold
default_test_wprobs <- default_test_wprobs |>
  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, "Yes", "No")),
         logistic_preds = as_factor(if_else(logistic_probs > threshold, "Yes", "No"))
  )

default_test_wprobs |> head() |> kable()
```

## Using a threshold {.smaller}

- Step 1: Predict **probabilities** for all observations
- Step 2: Set a threshold to obtain **class labels** (0.5 below)

```{r}
threshold <- 0.5   # set threshold
default_test_wprobs <- default_test_wprobs |>
  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, "Yes", "No")),
         logistic_preds = as_factor(if_else(logistic_probs > threshold, "Yes", "No")))

default_test_wprobs |> head() |> kable()
```

## Performance

```{r}
roc_metrics <- metric_set(accuracy, sensitivity, specificity)
roc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = "second") |> kable()
```

## Low Threshold

```{r}
threshold <- 0.1   # set threshold
default_test_wprobs <- default_test_wprobs |>
  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, "Yes", "No")))

roc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = "second")  |> kable()
```

## High Threshold

```{r}
threshold <- 0.9   # set threshold
default_test_wprobs <- default_test_wprobs |>
  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, "Yes", "No"))
  )

roc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = "second") |> kable()
```

## Question

-   If I want to improve Recall/Sensitivity should I increase or decrease my threshold?
-   If I want to improve my Precision/PPV should I increase or decrease my threshold?

# ROC Curve

## ROC Curve and AUC

- **ROC (Receiver Operating Characteristics) curve**: popular graphic for comparing different classifiers across all possible thresholds
  + Plots the (1-Specificity) along the x-axis and the Sensitivity (true positive rate) along the y-axis
- **AUC**: area under the AUC curve
  + Ideal ROC curve will hug the top left corner
- Idea: How well is my classifier separating positives from negatives

## ROC Curve

```{r}
roc_curve(default_test_wprobs, truth = default, knn_probs, event_level = "second") |>
  head() |>
  kable()
```

## ROC Curve: Plot

```{r}
roc_curve(default_test_wprobs, truth = default, knn_probs, event_level = "second") |>
  autoplot()
```


## AUC

-   AUC: Area under the curve (ROC Curve that is)
-   Measures how good your model is at separating categories
-   Only for binary classification

## AUC in R


```{r}
roc_auc(default_test_wprobs, truth = default, knn_probs, event_level = "second") |>
  kable()
```


## Pathological Example 1

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE
set.seed(427)
example1 <- tibble(prob = c(runif(100, 0, 0.1), runif(100, 0.9, 1)), class = c(rep("Negative", 100), rep("Positive", 100))) |>
  mutate(class = factor(class))

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%"))
```
:::
::::

## Pathological Example 1 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 1, y = 1, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{100}{100} = 1$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{100}{100} = 1$$

## Pathological Example 1 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0.05

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0.55, y = 1, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{100}{100} = 1$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{55}{100} = 0.55$$

## Pathological Example 1 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0.5

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0, y = 1, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{100}{100} = 1$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{0}{100} = 0$$

## Pathological Example 1 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0.95
example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0, y = 0.56, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{56}{100} = 0.56$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{0}{100} = 0$$

## Pathological Example 1 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 1

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0, y = 0, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{0}{100} = 0$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{0}{100} = 0$$

## Pathological Example 2

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE
set.seed(427)
example1 <- tibble(prob = c(runif(100, 0, 0.5), runif(100, 0.5, 1)), class = c(rep("Negative", 100), rep("Positive", 100))) |>
  mutate(class = factor(class))

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%"))
```
:::
::::

## Pathological Example 3

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE
set.seed(427)
example1 <- tibble(prob = c(runif(100, 0, 0.7), runif(100, 0.3, 1)), class = c(rep("Negative", 100), rep("Positive", 100))) |>
  mutate(class = factor(class))

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot()  +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%"))
```
:::
::::

## Pathological Example 3 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 1, y = 1, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{100}{100} = 1$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{100}{100} = 1$$

## Pathological Example 3 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0.25

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0.62, y = 1, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{100}{100} = 1$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{62}{100} = 0.62$$

## Pathological Example 3 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0.31
example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0.58, y = 0.99, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{99}{100} = 0.99$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{58}{100} = 0.58$$

## Pathological Example 3 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0.5

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0.29, y = 0.75, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{75}{100} = 0.75$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{29}{100} = 0.29$$

## Pathological Example 3 {.smaller}

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE

threshold <- 0.75

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  geom_hline(yintercept = threshold, color = "red") +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%")) +
  annotate("point", x = 0, y = 0.37, color = "red")
```
:::
::::

$$\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Greens Above Line}}{\text{All Greens}} = \frac{37}{100} = 0.37$$
$$1-\text{Specificity} = 1-\frac{TN}{TN + FP} = \frac{FP}{TN + FP} = \frac{\text{Reds Above Line}}{\text{All Reds}} = \frac{0}{100} = 0$$

## Pathological Example 4

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE
example1 <- tibble(prob = c(runif(100, 0, 1), runif(100, 0, 1)), class = c(rep("Negative", 100), rep("Positive", 100))) |>
  mutate(class = factor(class))

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot()
```
:::
::::

## Pathological Example 5

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE
multiplier <- 1
a <- 1 * multiplier
b <- 3 * multiplier
example1 <- tibble(prob = c(rbeta(100, a, b), rbeta(100, b, a)), class = c(rep("Negative", 100), rep("Positive", 100))) |>
  mutate(class = factor(class))

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%"))
```
:::
::::

## Pathological Example 6

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE
multiplier <- 1
a <- 2 * multiplier
b <- 3 * multiplier
example1 <- tibble(prob = c(rbeta(100, a, b), rbeta(100, b, a)), class = c(rep("Negative", 100), rep("Positive", 100))) |>
  mutate(class = factor(class))

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%"))
```
::: 
:::: 

## Pathological Example 7

:::: columns
:::: {.column width="40%"}
```{r}
#| echo: FALSE
multiplier <- 20
a <- 2 * multiplier
b <- 3 * multiplier
example1 <- tibble(prob = c(rbeta(100, a, b), rbeta(100, b, a)), class = c(rep("Negative", 100), rep("Positive", 100))) |>
  mutate(class = factor(class))

example1 |>
  ggplot(aes(y = prob, x = class, color = class)) +
  geom_sina() +
  ylab("Predicted Probability of Positive")
```
:::

:::: {.column width="60%"}
```{r}
#| echo: FALSE
example1 |>
  roc_curve(truth = class, prob, event_level = "second") |>
  autoplot() +
  annotate("text",
           x = 0.75, y = 0.25,
           label = paste0("AUC: ", roc_auc(example1, truth = class, prob, event_level = "second") |> pull(.estimate)*100, "%"))
```
:::
::::

## AUC Questions

-   What should be the minimum AUC?
-   What should be that maximum possible AUC?

# Review of Classification Metrics

## Review: Logistic Regression vs KNN {.smaller}

- Logistic regression $\implies$ parametric , KNN $\implies$ non-parametric.
- Logistic regression $\implies$ only for classification problems ($Y$ categorical), KNN $\implies$ both regression and classification.
- Logistic regression is (more) interpretable, KNN is not.
- Logistic regression allows qualitative predictors. Euclidean distance with KNN does not allow for qualitative predictors.
- Prediction: KNN can be pretty good for small $p$, that is, $p \le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - curse of dimensionality.

## Classification Metrics App {.smaller}

-   Dr. F will split you into four groups
-   On one of your computers connect to a tv and [open this app](https://efriedlander.shinyapps.io/ClassificationMetrics/)
-   Do the following based on your group number:
    +   1: Choose plane on the first screen
    +   2: Choose circle on the first screen
    +   3: Choose parabola on the first screen
    +   4: Choose sine curve on the first screen
-   We will generate data from this population... do you think KNN or logistic regression will yield a better classifier? Why?

## Classification Metrics App {.smaller}

-   On the second tab generate a small test and training set
-   On the third tab fit a KNN model with 5 neighbors and then a logistic regression model
-   Which model do you think will perform better based on the plots you see?
-   Choose the better model, click fit, and click on the fourth tab

## Questions

Using the app, try and answer the following questions:

- Which of the metrics are most and least impacted by:
  - Sample size
  - Imbalanced data (i.e. proportion positive near 0 or 1)
  - High noise

