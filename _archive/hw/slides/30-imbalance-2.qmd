---
title: 'MATH 427: Class Imbalance'
author: Eric Friedlander
footer: "[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---


## Computational Set-Up

```{r}
library(tidyverse)
library(tidymodels)
library(knitr)
library(kableExtra)

tidymodels_prefer()

set.seed(427)
```

    
## Exploring with App {.smaller}

-   [App](https://efriedlander.shinyapps.io/ClassificationMetrics/)
    +   Break into groups
    +   Investigate how your performance metrics change between balanced data and unbalanced data
    +   Additional Considerations:
        +   Impact of boundaries/models?
        +   Impact of sample size?
        +   Impact of noise level?
    +   Please write down observations so we can discuss
    
# Dealing with Class-Imbalance

##  Class-Imbalance

-   Class-imbalance occurs where your the classes in your response greatly differ in terms of how common they are
-   Occurs frequently:
    +   Medicine: survival/death
    +   Admissions: enrollment/non-enrollment
    +   Finance: repaid loan/defaulted
    +   Tech: Clicked on ad/Didn't click
    +   Tech: Churn rate
    +   Finance: Fraud

## Data: `haberman` {.smaller}

Study conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.

Goal: predict whether a patient survived after undergoing surgery for breast cancer.

```{r}
haberman <- read_csv("../data/haberman.data",
                     col_names = c("Age", "OpYear", "AxNodes", "Survival"))
haberman |> head() |> kable()
```

## Quick Clean

```{r}
haberman <- haberman |> 
  mutate(Survival = factor(if_else(Survival == 1, "Survived", "Died"),
                           levels = c("Died", "Survived")))
haberman |> head() |> kable()
```

## Split Data

```{r}
set.seed(427)
hab_splits <- initial_split(haberman, prop = 0.75, strata = Survival)
hab_train <- training(hab_splits)
hab_test <- testing(hab_splits)
```

## Visualizing Response

```{r}
hab_train |> 
  ggplot(aes(y = Survival)) +
  geom_bar()
```

## Fitting Model

```{r}
lr_model <- logistic_reg() |> 
  set_engine("glm")

lr_fit <- lr_model |> 
  fit(Survival ~ . , data = hab_train)
```

## Confusion Matrix

```{r}
lr_fit |> augment(new_data = hab_test) |> 
  conf_mat(truth = Survival, estimate = .pred_class) |> autoplot("heatmap")
```

## Performance Metrics

```{r}
hab_metrics <- metric_set(accuracy, precision, recall)

lr_fit |> augment(new_data = hab_test) |> 
  roc_auc(truth = Survival, .pred_Died) |> kable()

lr_fit |> augment(new_data = hab_test) |> 
  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()
```

## Recall is BAD!

-   Since there are so few deaths, model always predicts a low probability of death
-   Idea: just because you you have a HIGHER probability of death doesn't mean have a HIGH probability of death

## What do we do?

-   Depends on what your goal is...
-   Ask yourself: What is most important to my problem?
    +   Accurate probabilities?
    +   Overall accuracy?
    +   Effective identification of a specific class (e.g. positives)?
    +   Low false-positive rate?
-   Discussion: Let's think of scenarios where each one of these is the most important.

## Solutions to Class Imbalance {.smaller}

-   Adjust probability threshold (we've already done this)
    +   If you wanted to increase your recall would you increase or decrease your threshold?
-   Sampling-based solutions (done during pre-processing)
    +   Over-sample minority class
    +   Under-sample majority class
    +   Combination of both (e.g. SMOTE)
-   Weight class/objective function

## Over-sampling minority class

-   Upsample: think bootstrapping for final sample is larger than original
-   Idea: upsample minority class until it is same size(ish) as majority class

## Visualizing Data

```{r}
hab_train |> ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_jitter()
```

## Upsample Recipe

```{r}
library(themis)
upsample_recipe <- recipe(Survival ~ ., data = hab_train) |> 
  step_upsample(Survival, over_ratio = 1)

hab_upsample <- upsample_recipe |> prep(hab_train) |> bake(new_data = NULL)
```

## Upsampled Data

```{r}
hab_upsample |>  ggplot(aes(x = Survival)) +
  geom_bar()
```

## Visualizing Upsampled Data

```{r}
hab_upsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_jitter()
```

## Visualizing Upsampled Data: No Jitter

```{r}
hab_upsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_point()
```

## Performance consideration

-   Pro:
    +   Preserves all information in the data set
-   Con:
    +   Models will probably over-align to the noise in the minority class
    
## Under-sampling majority class

-   Downsample: collect a random sample smaller than the original sample
-   Idea: down sample majority class until it is same size(ish) as minority class

## Visualizing Data

```{r}
hab_train |> ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_jitter()
```

## Downsample Recipe

```{r}
downsample_recipe <- recipe(Survival ~ ., data = hab_train) |> 
  step_downsample(Survival, under_ratio = 1)

hab_downsample <- downsample_recipe |> prep(hab_train) |> bake(new_data = NULL)
```

## Downsample Data

```{r}
hab_downsample |>  ggplot(aes(x = Survival)) +
  geom_bar()
```

## Visualizing Downsampled Data

```{r}
hab_downsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_jitter()
```

## Visualizing Downsampled Data: No Jitter

```{r}
hab_downsample |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_point()
```

## Performance considerations

-   Pro:
    +   Model doesn't over-align to noise in minority class
-   Con:
    +   Lose information from majority class
    
## SMOTE

-   Basic idea:
    +   Both upsample minority and downsample majority (Tidymodel implementation only upsamples)
-   Better Upsampling: Instead of just randomly replicating minority observations
    +   Find (minority) nearest neighbors of each minority observation
    +   Interpolate line between them
    +   Upsample by randomly generating points in interpolated lines

## Visualizing Data

```{r}
hab_train |> ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_jitter()
```

## SMOTE Recipe

```{r}
smote_recipe <- recipe(Survival ~ ., data = hab_train) |> 
  step_normalize(all_numeric_predictors()) |> 
  step_smote(Survival, over_ratio = 1, neighbors = 5)

hab_smote <- smote_recipe |> prep(hab_train) |> bake(new_data = NULL)
```

## SMOTE Data

```{r}
hab_smote |>  ggplot(aes(x = Survival)) +
  geom_bar()
```

## Visualizing SMOTE Data

```{r}
hab_smote |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_jitter()
```

## Visualizing SMOTE Data: No Jitter

```{r}
hab_smote |>  ggplot(aes(x = OpYear, y = Age, color = Survival)) +
  geom_point()
```

## Performance considerations

-   Pro:
    +   Model doesn't over-align (as much) to noise in minority class
    +   Don't lose (as much) information from majority class
-   Con:
    +   Creating new information out of nowhere
    
## Fitting models

```{r}
oversamp_fit <- workflow() |> add_recipe(upsample_recipe) |> 
  add_model(lr_model) |> fit(hab_train)
downsamp_fit <- workflow() |> add_recipe(downsample_recipe) |> 
  add_model(lr_model)  |> fit(hab_train)
smote_fit <- workflow() |> add_recipe(smote_recipe) |> 
  add_model(lr_model)  |> fit(hab_train)
```

## Evaluate Performance

```{r}
oversamp_fit |> augment(new_data = hab_test) |> 
  roc_auc(truth = Survival, .pred_Died) |> kable()
downsamp_fit |> augment(new_data = hab_test) |> 
  roc_auc(truth = Survival, .pred_Died) |> kable()
smote_fit |> augment(new_data = hab_test) |> 
  roc_auc(truth = Survival, .pred_Died) |> kable()
```

## Evaluate Performance {.smaller}

```{r}
#| output-location: column

oversamp_fit |> augment(new_data = hab_test) |> 
  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()
```

```{r}
#| output-location: column
downsamp_fit |> augment(new_data = hab_test) |> 
  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()
```

```{r}
#| output-location: column
smote_fit |> augment(new_data = hab_test) |> 
  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()
```

# Weighting Observations/Objective Function

## Creating Importance Weights

```{r}
library(hardhat)
hab_train <- hab_train |> 
  mutate(weights = ifelse(Survival == "Died", 4, 1),
         weights = importance_weights(weights))

hab_train |> head() |> kable()
```

## Weighted Workflow

```{r}
weighted_wf <- workflow() |> 
  add_model(lr_model) |> 
  add_recipe(recipe(Survival ~ ., data = hab_train)) |> 
  add_case_weights(weights)

weighted_fit <- weighted_wf |> 
  fit(hab_train)
```

## Model Performance

```{r}
weighted_fit |> augment(new_data = hab_test) |> 
  conf_mat(truth = Survival, estimate = .pred_class) |> autoplot("heatmap")
```

## Model Performance

```{r}
weighted_fit |> augment(new_data = hab_test) |> 
  hab_metrics(truth = Survival, estimate = .pred_class) |> kable()

weighted_fit |> augment(new_data = hab_test) |> 
  roc_auc(truth = Survival, .pred_Died) |> kable()
```

# Final Note / Alternative Metrics

## Model vs Decisions {.smaller}

-   Helpful framework for thinking about this:
    -   Divide model predictions from decisions
    -   Usually, model predicts a probability, then you make a classification based on that probability
    -   Choosing the best model probably means (1) calibrating your probabilities correctly, then (2) making classifications/decisions to optimze your use-case

## Scoring Rules {.smaller}

-   Scoring rule: metric that evaluates probabilities
-   Notation:
    +   $\hat{p}_{ik}$: predicted probability observation $i$ is in class $k$
    +   $y_{ik}$: 1 if observation $i$ is in class $k$, 0 otherwise
    +   $K$: number of classes
    +   $N$: number of observations
-   Brier Score: think MSE for probabilities 
    +   Binary: $\frac{1}{N} \sum_{i=1}^{N} (\hat{p}_i - y_i)^2$
    +   Multiclass: $\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} (\hat{p}_{ik} - y_{ik})^2$
-   Logorithmic Score:
    +   Binary: $-\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right]$
    +   Multi-class: $-\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log(\hat{p}_{ik})$

## Scoring our models

```{r}
hab_scores <- metric_set(brier_class, mn_log_loss, roc_auc)
all_scores <- lr_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = "Logistic") |> 
  bind_rows(oversamp_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = "Oversample")) |> 
  bind_rows(downsamp_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = "Undersample")) |> 
  bind_rows(smote_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = "SMOTE")) |> 
  bind_rows(weighted_fit |> augment(new_data = hab_test) |> hab_scores(truth = Survival, .pred_Died) |> mutate(model = "Weighted"))
```

## Scores

```{r}
all_scores |>
  select(-.estimator) |> 
  pivot_wider(names_from = .metric, values_from = .estimate) |> 
  kable()
```

## Conclusion

-   Many different approaches and strategies depending on data
-   First strategy: tresholding
-   Many times method depends on model algorithm
-   Make sure to ask "Is imbalance really a problem here?"
