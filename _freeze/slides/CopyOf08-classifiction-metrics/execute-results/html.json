{
  "hash": "7b599e76aca5fa6407d56f8142d6468e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Evaluating Classification Models'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(janitor) # for next contingency tables\nlibrary(kableExtra)\nlibrary(ISLR2)\n\ntidymodels_prefer()\n```\n:::\n\n\n\n\n\n## Default Dataset {.smaller}\n\n::: columns\n::: column\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Default) |> kable()  # print first six observations\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|    income|\n|:-------|:-------|---------:|---------:|\n|No      |No      |  729.5265| 44361.625|\n|No      |Yes     |  817.1804| 12106.135|\n|No      |No      | 1073.5492| 31767.139|\n|No      |No      |  529.2506| 35704.494|\n|No      |No      |  785.6559| 38463.496|\n|No      |Yes     |  919.5885|  7491.559|\n\n\n:::\n:::\n\n\n:::\n\n::: column\n**Response Variable**: `default`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDefault |> \n  tabyl(default) |>  # class frequencies\n  kable()           # Make it look nice\n```\n\n::: {.cell-output-display}\n\n\n|default |    n| percent|\n|:-------|----:|-------:|\n|No      | 9667|  0.9667|\n|Yes     |  333|  0.0333|\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Split the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\ndefault_split <- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<6000/4000/10000>\n```\n\n\n:::\n\n```{.r .cell-code}\ndefault_train <- training(default_split)\ndefault_test <- testing(default_split)\n```\n:::\n\n\n\n## [K-Nearest Neighbors Classifier: Build Model]{.r-fit-text}\n\n-   **Response** ($Y$): `default`\n-   **Predictor** ($X$): `balance`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit <- nearest_neighbor(neighbors = 10) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\") |>  \n  fit(default ~ balance, data = Default)   # fit 10-nn model\n```\n:::\n\n\n\n## [K-Nearest Neighbors Classifier: Predictions]{.r-fit-text} {.smaller}\n\n::: panel-tabset\n## Class labels\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(knnfit, new_data = Default, type = \"class\") |> head() |> kable()   # obtain predictions as classes\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n\n\n:::\n:::\n\n\n\n## Probabilities\n\n-   Predicts class w/ maximum probability\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(knnfit, new_data = Default, type = \"prob\") |> head() |> kable() # obtain predictions as probabilities\n```\n\n::: {.cell-output-display}\n\n\n| .pred_No| .pred_Yes|\n|--------:|---------:|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n|        1|         0|\n\n\n:::\n:::\n\n\n:::\n\n## Fitting a logistic regression\n\nFitting a logistic regression model with `default` as the response and `balance` as the predictor:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogregfit <- logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |> kable()  # obtain results\n```\n\n::: {.cell-output-display}\n\n\n|term        |    estimate| std.error| statistic| p.value|\n|:-----------|-----------:|---------:|---------:|-------:|\n|(Intercept) | -10.6926385| 0.4659035| -22.95033|       0|\n|balance     |   0.0055327| 0.0002841|  19.47329|       0|\n\n\n:::\n:::\n\n\n\n## Making predictions in R\n\n::: panel-tabset\n\n## Class Labels\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |> kable()   # obtain class predictions\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n\n\n:::\n:::\n\n\n\n## Log-Odds\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |> kable()   # obtain log-odds predictions\n```\n\n::: {.cell-output-display}\n\n\n|         x|\n|---------:|\n| -6.819727|\n\n\n:::\n:::\n\n\n\n## Probabilities\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |> kable()  # obtain probability predictions\n```\n\n::: {.cell-output-display}\n\n\n|  .pred_No| .pred_Yes|\n|---------:|---------:|\n| 0.9989092| 0.0010908|\n\n\n:::\n:::\n\n\n\n:::\n\n\n# Assessing Performance of Classifiers\n\n## Binary Classifiers\n\n-   Start with binary classification scenarios\n-   With binary classification, designate one category as \"Success/Positive\" and the other as \"Failure/Negative\"\n    +   If relavent to your problem: \"Positive\" should be the thing you're trying to predict/care more about\n    +   Note: \"Positive\" $\\neq$ \"Good\"\n    +   For `default`: \"Yes\" is Positive\n-   Some metrics weight \"Positives\" more and viceversa\n\n## Confusion Matrix\n\n|                                  | Actual Positive/Event | Actual Negative/Non-event |\n|:--------------------------------:|:---------------------:|:-------------------------:|\n|   **Predicted Positive/Event**   |     True Negative (TN)     |      False Positive  (FP)     |\n| **Predicted Negative/Non-event** |    False Negative (FN)    |       True Positive  (TP)     |\n\n## Adding predictions to tibble\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds <- default_test |> \n  mutate(\n    knn_preds = predict(knnfit, new_data = default_test, type = \"class\")$.pred_class,\n    logistic_preds = predict(logregfit, new_data = default_test, type = \"class\")$.pred_class\n  )\n\ndefault_test_wpreds |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income|knn_preds |logistic_preds |\n|:-------|:-------|---------:|--------:|:---------|:--------------|\n|No      |No      |  729.5265| 44361.63|No        |No             |\n|No      |Yes     |  808.6675| 17600.45|No        |No             |\n|No      |Yes     | 1220.5838| 13268.56|No        |No             |\n|No      |No      |  237.0451| 28251.70|No        |No             |\n|No      |No      |  606.7423| 44994.56|No        |No             |\n|No      |No      |  286.2326| 45042.41|No        |No             |\n\n\n:::\n:::\n\n\n\n## KNN: Confusion Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  3854   80\n       Yes   17   49\n```\n\n\n:::\n:::\n\n\n\n## KNN: Confusion Matrix (Sexy)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](CopyOf08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Logistic Regression: Confusion Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = logistic_preds) |> \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](CopyOf08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n## Classification Metrics {.smaller}\n\n-   Accuracy: proportion of your classes that are correct $$(TP + TN)/Total$$\n-   Recall/Sensitivity: proportion of true positives correct (true positive rate) $$TP/(TP+FN)$$\n-   Precision/Positive Predictive Value (PPV): proportion of predicted positive that are correct $$TP/(TP+FP)$$\n-   Specificity: proportion of true negatives correct (true negative rate) $$TN/(TN+FP)$$\n-   Negative Predictive Value (NPV): proportion of predicted negatives that are correct $$TN/(TN+FN)$$\n\n## KNN: Performance {.smaller}\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = knn_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](CopyOf08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: column\n-   Accuracy: $(3854+49)/4000 = .976 = 97.6\\%$\n-   Recall/Sensitivity: $49/(49+80) = 0.380 = 38.0\\%$\n-   Precision/Positive Predictive Value (PPV): $49/(49+17) = .742 = 74.2\\%$\n-   Specificity: $3854/(3854+17) = 0.996 = 99.6\\%$\n-   Negative Predictive Value (NPV): $3854/(3854+80) = 98.0$\n:::\n::::\n\n## Logistic Regression: Performance {.smaller}\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |>\n  conf_mat(truth = default, estimate = logistic_preds) |> \n  autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](CopyOf08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: column\nCompute the following and write your answers on the board:\n\n-   Accuracy\n-   Recall/Sensitivity\n-   Precision/Positive Predictive Value (PPV)\n-   Specificity\n-   Negative Predictive Value (NPV)\n:::\n::::\n\n## Performance Metrics with `yardstick`\n\n-   `yardstick` is a package that ships with `tidymodels` meant for model evaluation\n-   Typical syntax: `metricname(data, truth, estimate, ...)`\n    +   Bind original data with predicted observations\n    +   Put true response in for `truth` and predicted values in for `estimate`\n    \n## Logistic Regression: Accuracy {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  accuracy(truth = default, estimate = logistic_preds) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |binary     |     0.973|\n\n\n:::\n:::\n\n\n\n## Two More Metrics {.smaller}\n\n-   Matthews correlation coefficient (MCC): similar to $R^2$ but for classification\n$$\\frac{TP\\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n    +   Good for imbalanced data\n    +   Considers both positives and negatives\n-   F-Measure: harmonic mean of recall and precision\n$$\\frac{2}{recall^{-1} + precision^{-1}} = \\frac{2TP}{2TP+FP+FN}$$\n    +   Focuses more on positives\n    +   bad of imbalanced data\n\n## Metric Sets\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinary_metrics <- metric_set(accuracy, recall, precision, specificity,\n                             npv, mcc, f_meas)\n```\n:::\n\n\n\n-   Can apply this to compute a bunch of metrics\n\n\n## KNN: Performance {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  binary_metrics(truth = default, estimate = knn_preds, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric     |.estimator | .estimate|\n|:-----------|:----------|---------:|\n|accuracy    |binary     | 0.9757500|\n|recall      |binary     | 0.3798450|\n|precision   |binary     | 0.7424242|\n|specificity |binary     | 0.9956084|\n|npv         |binary     | 0.9796645|\n|mcc         |binary     | 0.5206828|\n|f_meas      |binary     | 0.5025641|\n\n\n:::\n:::\n\n\n\n## Logistic Regression: Performance {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wpreds |> \n  binary_metrics(truth = default, estimate = logistic_preds, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric     |.estimator | .estimate|\n|:-----------|:----------|---------:|\n|accuracy    |binary     | 0.9730000|\n|recall      |binary     | 0.3023256|\n|precision   |binary     | 0.6842105|\n|specificity |binary     | 0.9953500|\n|npv         |binary     | 0.9771747|\n|mcc         |binary     | 0.4437097|\n|f_meas      |binary     | 0.4193548|\n\n\n:::\n:::\n\n\n\n## Discussion\n\n-   For each of the following metrics, brainstorm a situation in which that metric is probably the most important:\n    +   Recall\n    +   Precision\n    +   Accuracy\n\n# Thresholding\n\n\n## Using a threshold {.smaller}\n\n- Step 1: Predict **probabilities** for all observations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_test_wprobs <- default_test |> \n  mutate(\n    knn_probs = predict(knnfit, new_data = default_test, type = \"prob\")$.pred_Yes,\n    logistic_probs = predict(logregfit, new_data = default_test, type = \"prob\")$.pred_Yes\n  )\n\ndefault_test_wprobs |> head() |> kable()   # obtain probability predictions\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income| knn_probs| logistic_probs|\n|:-------|:-------|---------:|--------:|---------:|--------------:|\n|No      |No      |  729.5265| 44361.63|         0|      0.0012842|\n|No      |Yes     |  808.6675| 17600.45|         0|      0.0019883|\n|No      |Yes     | 1220.5838| 13268.56|         0|      0.0190870|\n|No      |No      |  237.0451| 28251.70|         0|      0.0000843|\n|No      |No      |  606.7423| 44994.56|         0|      0.0006514|\n|No      |No      |  286.2326| 45042.41|         0|      0.0001107|\n\n\n:::\n:::\n\n\n\n## Using a threshold {.smaller}\n\n- Step 1: Predict **probabilities** for all observations\n- Step 2: Set a threshold to obtain **class labels** (0.5 below)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.5   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs > threshold, \"Yes\", \"No\"))\n  )\n\ndefault_test_wprobs |> head() |> kable()      \n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income| knn_probs| logistic_probs|knn_preds |logistic_preds |\n|:-------|:-------|---------:|--------:|---------:|--------------:|:---------|:--------------|\n|No      |No      |  729.5265| 44361.63|         0|      0.0012842|No        |No             |\n|No      |Yes     |  808.6675| 17600.45|         0|      0.0019883|No        |No             |\n|No      |Yes     | 1220.5838| 13268.56|         0|      0.0190870|No        |No             |\n|No      |No      |  237.0451| 28251.70|         0|      0.0000843|No        |No             |\n|No      |No      |  606.7423| 44994.56|         0|      0.0006514|No        |No             |\n|No      |No      |  286.2326| 45042.41|         0|      0.0001107|No        |No             |\n\n\n:::\n:::\n\n\n\n## Using a threshold {.smaller}\n\n- Step 1: Predict **probabilities** for all observations\n- Step 2: Set a threshold to obtain **class labels** (0.5 below)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.5   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\")),\n         logistic_preds = as_factor(if_else(logistic_probs > threshold, \"Yes\", \"No\"))\n  )\n\ndefault_test_wprobs |> head() |> kable()      \n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|   income| knn_probs| logistic_probs|knn_preds |logistic_preds |\n|:-------|:-------|---------:|--------:|---------:|--------------:|:---------|:--------------|\n|No      |No      |  729.5265| 44361.63|         0|      0.0012842|No        |No             |\n|No      |Yes     |  808.6675| 17600.45|         0|      0.0019883|No        |No             |\n|No      |Yes     | 1220.5838| 13268.56|         0|      0.0190870|No        |No             |\n|No      |No      |  237.0451| 28251.70|         0|      0.0000843|No        |No             |\n|No      |No      |  606.7423| 44994.56|         0|      0.0006514|No        |No             |\n|No      |No      |  286.2326| 45042.41|         0|      0.0001107|No        |No             |\n\n\n:::\n:::\n\n\n\n## Performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_metrics <- metric_set(accuracy, sensitivity, specificity)\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.976\n2 sensitivity binary         0.996\n3 specificity binary         0.380\n```\n\n\n:::\n:::\n\n\n\n## Low Threshold\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.1   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\"))\n  )\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.923\n2 sensitivity binary         1    \n3 specificity binary         0.921\n```\n\n\n:::\n:::\n\n\n\n## High Threshold\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.9   # set threshold\ndefault_test_wprobs <- default_test_wprobs |> \n  mutate(knn_preds = as_factor(if_else(knn_probs > threshold, \"Yes\", \"No\"))\n  )\n\nroc_metrics(default_test_wprobs, truth = default, estimate = knn_preds, event_level = \"second\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary        0.968 \n2 sensitivity binary        0.0233\n3 specificity binary        1     \n```\n\n\n:::\n:::\n\n\n\n## ROC Curve and AUC\n\n- **ROC (Receiver Operating Characteristics) curve**: popular graphic for comparing different classifiers across all possible thresholds\n  + Plots the (1-Specificity) along the x-axis and the Sensitivity (true positive rate) along the y-axis\n- **AUC**: area under the AUC curve\n  + Ideal ROC curve will hug the top left corner\n\n## ROC Curve\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |> \n  head() |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n| .threshold| specificity| sensitivity|\n|----------:|-----------:|-----------:|\n|       -Inf|   0.0000000|           1|\n|     0.0000|   0.0000000|           1|\n|     0.0145|   0.8915009|           1|\n|     0.0415|   0.8997675|           1|\n|     0.0560|   0.9062258|           1|\n|     0.0655|   0.9064841|           1|\n\n\n:::\n:::\n\n\n\n## ROC Curve: Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_curve(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](CopyOf08-classifiction-metrics_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n## ROC Curve\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_auc(default_test_wprobs, truth = default, knn_probs, event_level = \"second\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |binary     | 0.9838903|\n\n\n:::\n:::\n\n\n\n\n\n<!-- ## Logistic Regression vs KNN {.smaller} -->\n\n<!-- - Logistic regression $\\implies$ parametric , KNN $\\implies$ non-parametric. -->\n<!-- - Logistic regression $\\implies$ only for classification problems ($Y$ categorical), KNN $\\implies$ both regression and classification. -->\n<!-- - Logistic regression is interpretable, KNN is not. -->\n<!-- - Logistic regression allows qualitative predictors. Euclidean distance with KNN does not allow for qualitative predictors. -->\n<!-- - Prediction: KNN can be pretty good for small $p$, that is, $p \\le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - curse of dimensionality. -->\n\n<!-- ## [Decision Boundaries: Simulated observations]{.r-fit-text} -->\n\n<!-- ```{r} -->\n<!-- #| echo: FALSE -->\n\n<!-- set.seed(208) -->\n\n<!-- x1 <- runif(50, -1, 1) -->\n\n<!-- x2 <- runif(50, -1, 1) -->\n\n<!-- y <- as.factor(ifelse(x1>0, 1,2)) -->\n\n<!-- df <- data.frame(x1=x1, x2=x2, y = y) -->\n\n<!-- df$y[c(1,43, 8, 42, 44)] <- 2 -->\n\n<!-- df$y[c(24, 14)] <- 1 -->\n\n<!-- alldf <- expand.grid(x1=seq(-1, 1, 0.005), x2=seq(-1, 1, 0.005)) -->\n\n<!-- knnfit1 <- knn3(y ~ x1+x2, data = df, k = 1) -->\n\n<!-- knnfit9 <- knn3(y ~ x1+x2, data = df, k = 9) -->\n\n<!-- lgfit <- glm(y ~ x1 + x2, data = df, family = binomial) -->\n\n<!-- knn_class_preds_1 <- predict(knnfit1, newdata = alldf, type = \"class\") -->\n\n<!-- knn_class_preds_actual_1 <- predict(knnfit1, newdata = df, type = \"class\") -->\n\n<!-- knn_class_preds_9 <- predict(knnfit9, newdata = alldf, type = \"class\") -->\n\n<!-- knn_class_preds_actual_9 <- predict(knnfit9, newdata = df, type = \"class\") -->\n\n<!-- p <- predict(lgfit, newdata = df, type = \"response\") -->\n\n<!-- alldf$knn1_preds <- knn_class_preds_1 -->\n\n<!-- df$knn1_preds <- knn_class_preds_actual_1 -->\n\n<!-- alldf$knn9_preds <- knn_class_preds_9 -->\n\n<!-- df$knn9_preds <- knn_class_preds_actual_9 -->\n\n<!-- df$logreg_preds_50 <- ifelse(p >= 0.5, 2, 1) -->\n<!-- df$logreg_preds_25 <- ifelse(p >= 0.25, 2, 1) -->\n\n<!-- alldf$logreg_probs <- predict(lgfit, newdata = alldf, type = \"response\") -->\n\n<!-- alldf$logreg_preds_50 <- factor(ifelse(alldf$logreg_probs>0.5, 2, 1)) -->\n<!-- alldf$logreg_preds_25 <- factor(ifelse(alldf$logreg_probs>0.25, 2, 1)) -->\n\n<!-- g1 <- ggplot(data = alldf)+ -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2), alpha = 0.2, size = 0.3) + theme_minimal() + -->\n\n<!--   geom_point(data = df, mapping = aes(x=x1, y=x2, color = y), alpha = 5) + -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2, color = knn1_preds), alpha = 0.4, size = 0.3) + -->\n\n<!--   # stat_contour(aes(x=x1, y=x2, z=as.numeric(knn_class_preds_1)), bins=1) + -->\n\n<!--   labs(x = \"x1\", y = \"x2\", title = \"1-NN\") + -->\n\n<!--   theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank()) -->\n\n<!-- g2 <- ggplot(data = alldf)+ -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2), alpha = 0.2, size = 0.3) + theme_minimal() + -->\n\n<!--   geom_point(data = df, mapping = aes(x=x1, y=x2, color = y), alpha = 5) + -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2, color = knn9_preds), alpha = 0.4, size = 0.3) + -->\n\n<!--   # stat_contour(aes(x=x1, y=x2, z=as.numeric(knn_class_preds_9)), bins=1) + -->\n\n<!--   labs(x = \"x1\", y = \"x2\", title = \"9-NN\") + -->\n\n<!--   theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank()) -->\n\n<!-- # 0.3917 + (-4.3088 * x1) + (-0.6084 * x2) = 0 -->\n\n<!-- # (-0.6084 * x2) = (4.3088 * x1) - 0.3917 -->\n\n<!-- # x2 = (-7.082183 * x1) + 0.6438199 -->\n\n<!-- g3 <- ggplot(data = alldf)+ -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2), alpha = 0.2, size = 0.3) + theme_minimal() + -->\n\n<!--   geom_point(data = df, mapping = aes(x=x1, y=x2, color = y), alpha = 5) + -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2, color = logreg_preds_50), alpha = 0.4, size = 0.3) + -->\n\n<!--   labs(x = \"x1\", y = \"x2\", title = \"Logistic Regression (Thresh = 0.5)\") + -->\n\n<!--   theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank()) -->\n\n<!-- g4 <- ggplot(data = alldf)+ -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2), alpha = 0.2, size = 0.3) + theme_minimal() + -->\n\n<!--   geom_point(data = df, mapping = aes(x=x1, y=x2, color = y), alpha = 5) + -->\n\n<!--   geom_point(mapping = aes(x=x1, y=x2, color = logreg_preds_25), alpha = 0.4, size = 0.3) + -->\n\n<!--   labs(x = \"x1\", y = \"x2\", title = \"Logistic Regression (Thresh = 0.25)\") + -->\n\n<!--   theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank()) -->\n\n<!-- grid.arrange(g1, g2, g3, g4, ncol=2) -->\n\n<!-- ``` -->\n\n\n<!-- ## The Machine Learning Process -->\n\n<!-- For the next 2-3 class periods, we are going to discuss the overall ML process. Note that, the procedures we will discuss are not ML models, rather the models go through this process to obtain the best (optimal) predictive model. -->\n\n<!-- **Ames Housing dataset** -->\n\n<!-- ```{r,message=FALSE} -->\n\n<!-- ames <- readRDS(\"AmesHousing.rds\")   # load dataset -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- # we won't use the entire dataset now (that's for later) -->\n\n<!-- # select the variables to work with for this class (04/18) -->\n\n<!-- ames <- ames %>% select(Sale_Price, Garage_Area, Year_Built)  -->\n\n<!-- ``` -->\n\n<!-- ## Exploratory Data Analysis -->\n\n<!-- **Ames Housing dataset** -->\n\n<!-- ```{r} -->\n\n<!-- summary(ames)   # summary of the variables -->\n\n<!-- ``` -->\n\n<!-- ## Exploratory Data Analysis -->\n\n<!-- **Ames Housing dataset** -->\n\n<!-- ```{r, fig.align='center'} -->\n\n<!-- library(GGally) -->\n\n<!-- ggpairs(ames)   # correlation plot -->\n\n<!-- ``` -->\n\n<!-- ## Data Splitting -->\n\n<!-- Available data split into **training** and **test** datasets. -->\n\n<!-- * **Training set:** these data are used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose an optimal model. -->\n\n<!-- * **Test set:** having chosen a final optimal model, these data are used to obtain an unbiased estimate of the modelâ€™s performance. -->\n\n<!-- **It is critical that the test set not be used prior to selecting your final model.** Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process. -->\n\n<!-- ## Data Splitting -->\n\n<!-- ```{r, message = FALSE} -->\n\n<!-- # split data -->\n\n<!-- set.seed(041824)  # fix the random number generator for reproducibility -->\n\n<!-- library(caret)  # load library -->\n\n<!-- # split available data into 80% training and 20% test datasets -->\n\n<!-- train_index <- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE)  -->\n\n<!-- ames_train <- ames[train_index,]   # training data -->\n\n<!-- ames_test <- ames[-train_index,]   # test data -->\n\n<!-- ``` -->\n\n<!-- <!-- ## Resampling Methods -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\> <!-- <!-- # load required packages --> --\\>\n<!-- <!-- library(tidyverse) --> --\\> <!-- <!-- library(caret) --> --\\>\n<!-- <!-- library(ISLR) --> --\\>\n\n<!-- <!-- data(\"Auto\")   # load 'Auto' daatset -->\n\n--\\>\n\n<!-- <!-- # split available data into training and test data -->\n\n--\\>\n\n<!-- <!-- set.seed(04192022)   # fix the random number generator for reproducibility -->\n\n--\\>\n\n<!-- <!-- # response: 'mpg' -->\n\n--\\>\n<!-- <!-- train_index <- createDataPartition(Auto$mpg, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets -->\n--\\>\n\n<!-- <!-- Auto_train <- Auto[train_index,]   # training data, we will work with this to choose our final model -->\n\n--\\>\n\n<!-- <!-- Auto_test <- Auto[-train_index,]   # test data, KEEP IT ASIDE, use only after choosing final model -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- ## Resampling Methods -->\n\n<!-- * **Idea:** Repeatedly draw samples from the training data and refit a model on each sample, and evaluate its performance on the other parts. -->\n\n<!-- * **Objective:** To obtain additional information about the fitted model. -->\n\n<!-- * **Cross-Validation (CV)** is probably the most widely used resampling method. It is a general approach that can be applied to almost any statistical learning method. -->\n\n<!-- ## Cross-Validation (CV) -->\n\n<!-- Used for -->\n\n<!-- * **model selection**: select the optimum level of flexibility (tune hyperparameters) or compare different models to choose the best one -->\n\n<!-- * **model assessment**: evaluate the performance of a model (estimate the error and variability) -->\n\n<!-- We will talk about -->\n\n<!-- <!-- * Validation Set Approach -->\n\n--\\>\n\n<!-- * Leave-One-Out Cross-Validation (LOOCV) -->\n\n<!-- * $k$-Fold Cross-Validation -->\n\n<!-- <!-- ## Training Error vs Test Error -->\n\n--\\>\n\n<!-- <!-- * **Training Error**: Calculated by applying the statistical learning method to the observations used in its training. -->\n\n--\\>\n\n<!-- <!-- * **Test Error**: Average error that results from using a -->\n\n--\\>\n<!-- <!-- statistical learning method to predict the response on a new unseen observation. -->\n--\\>\n\n<!-- <!-- **Test Error Estimates** -->\n\n--\\>\n\n<!-- <!-- * From a large designated test set. -->\n\n--\\>\n\n<!-- <!-- * Making a mathematical adjustment to the training error. (Chapter 6) -->\n\n--\\>\n\n<!-- <!-- * By **holding out** a subset of the training dataset, then assessing model performance on the held out observations. -->\n\n--\\>\n\n<!-- <!-- ## Training Error vs Test Error -->\n\n--\\>\n\n<!-- <!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->\n\n--\\> <!-- <!-- knitr::include_graphics(\"EFT/SL_C5_1.PNG\") --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Validation Set Approach -->\n\n--\\>\n\n<!-- <!-- * Randomly divide the available set of observations into: a **training set** and a **validation/hold-out set**. -->\n\n--\\>\n\n<!-- <!-- * Model fit on the training set. Fitted model is used to predict the responses for the observations in the -->\n\n--\\> <!-- <!-- validation set. --> --\\>\n\n<!-- <!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->\n\n--\\> <!-- <!-- knitr::include_graphics(\"EFT/5.1.PNG\") --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Validation Set Approach -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r, echo=FALSE, fig.align='center'} -->\n\n--\\> <!-- <!-- data(\"Auto\") --> --\\>\n<!-- <!-- plot(Auto$horsepower,Auto$mpg,ylab=\"mpg\",xlab=\"horsepower\") -->\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- Randomly split the 392 observations into two sets, a training set containing 196 data points, and a validation set containing the remaining 196 observations. -->\n\n--\\>\n\n<!-- <!-- ## Validation Set Approach -->\n\n--\\>\n\n<!-- <!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->\n\n--\\> <!-- <!-- knitr::include_graphics(\"EFT/5.2.PNG\") --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- Potential drawbacks: -->\n\n--\\>\n\n<!-- <!-- * The validation set estimate of the test error can be highly variable. -->\n\n--\\>\n\n<!-- <!-- * Only a subset of the observations (those that are in the training set) are used to fit the model. This suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set. -->\n\n--\\>\n\n<!-- <!-- * Yields different results due to randomness in training and validation datasets. -->\n\n--\\>\n\n<!-- ## Leave-One-Out Cross-Validation (LOOCV) -->\n\n<!-- <!-- * Closely related to the validation set approach. Attempts to address its drawbacks. -->\n\n--\\>\n\n<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->\n\n<!-- knitr::include_graphics(\"EFT/5.3.PNG\") -->\n\n<!-- ``` -->\n\n<!-- ## <span style=\"color:blue\">Your Turn!!!</span>  -->\n\n<!-- Suppose you implement LOOCV on a dataset with $n=100$ observations. -->\n\n<!-- 1. What is the size (number of observations) of each training set? -->\n\n<!-- 2. What is the size of each validation set? -->\n\n<!-- 3. How many steps/iterations are required to complete the overall LOOCV process? -->\n\n<!-- <!-- ```{r , echo=FALSE,  fig.align='center', out.width = '30%'} -->\n\n--\\> <!-- <!-- knitr::include_graphics(\"EFT/e5.1.PNG\") --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Leave-One-Out Cross-Validation (LOOCV) -->\n\n--\\>\n\n<!-- <!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->\n\n--\\>\n<!-- <!-- # comparing 4 polynomial (regression) models with LOOCV (response: 'mpg', predictor: 'horsepower') -->\n--\\>\n\n<!-- <!-- ggplot(data = Auto, aes(x = horsepower, y = mpg)) +   # quick visual check -->\n\n--\\> <!-- <!--   geom_point() --> --\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->\n\n--\\>\n\n<!-- <!-- ```{r,message=FALSE} -->\n\n--\\> <!-- <!-- ames <- readRDS(\"AmesHousing.rds\")   # load dataset -->\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- Consider `Sale_Price` as the response variable. Split the data into training and test data. -->\n\n--\\>\n\n<!-- <!-- ```{r, message = FALSE} -->\n\n--\\>\n<!-- <!-- set.seed(012423)  # fix the random number generator for reproducibility -->\n--\\>\n\n<!-- <!-- library(caret)  # load library -->\n\n--\\>\n\n<!-- <!-- train_index <- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets -->\n\n--\\>\n\n<!-- <!-- ames_train <- ames[train_index,]   # training data, use this dataset to build model -->\n\n--\\>\n\n<!-- <!-- ames_test <- ames[-train_index,]   # test data, use this dataset to evaluate model's performance -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->\n\n--\\>\n\n<!-- <!-- Define CV specifications. -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\>\n<!-- <!-- cv_specs_loocv <- trainControl(method = \"LOOCV\")   # specify CV method -->\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- We will compare the following three linear regression models: -->\n\n--\\>\n\n<!-- <!-- * with `Garage_Area` as the only predictor; -->\n\n--\\>\n\n<!-- <!-- * with `Overall_Qual` as the only predictor; -->\n\n--\\>\n\n<!-- <!-- * with `Garage_Area`, `Year_Built`, and `Overall_Qual` as predictors. -->\n\n--\\>\n\n<!-- <!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->\n\n--\\>\n\n<!-- <!-- Implement LOOCV with the first model. -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\>\n<!-- <!-- m1 <- train(form = Sale_Price ~ Garage_Area,    # specify model -->\n--\\> <!-- <!--             data = ames_train,   # specify dataset -->\n--\\>\n<!-- <!--             method = \"lm\",       # specify type of model -->\n--\\>\n<!-- <!--             trControl = cv_specs_loocv, # CV specifications -->\n--\\>\n<!-- <!--             metric = \"RMSE\")   # metric to evaluate model -->\n--\\>\n\n<!-- <!-- m1   # summary of LOOCV -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->\n\n--\\>\n\n<!-- <!-- Implement LOOCV with the second model. -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\> <!-- <!-- m2 <- train(form = Sale_Price ~ Overall_Qual,   --> --\\>\n<!-- <!--             data = ames_train,           --> --\\>\n<!-- <!--             method = \"lm\",               --> --\\>\n<!-- <!--             trControl = cv_specs_loocv,        --> --\\>\n<!-- <!--             metric = \"RMSE\")            --> --\\>\n\n<!-- <!-- m2 -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->\n\n--\\>\n\n<!-- <!-- Implement LOOCV with the third model. -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\>\n<!-- <!-- m3 <- train(form = Sale_Price ~ Garage_Area + Year_Built + Overall_Qual,   -->\n--\\> <!-- <!--             data = ames_train, --> --\\>\n<!-- <!--             method = \"lm\", --> --\\>\n<!-- <!--             trControl = cv_specs_loocv, --> --\\>\n<!-- <!--             metric = \"RMSE\") --> --\\>\n\n<!-- <!-- m3 -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Leave-One-Out Cross-Validation (LOOCV): Results -->\n\n--\\>\n\n<!-- <!-- Compare LOOCV results for different models. -->\n\n--\\>\n\n<!-- <!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->\n\n--\\> <!-- <!-- # create data frame to plot results --> --\\>\n\n<!-- <!-- df <- data.frame(model_number = 1:3, RMSE = c(m1$results$RMSE,   -->\n\n--\\>\n<!-- <!--                                              m2$results$RMSE, -->\n--\\>\n<!-- <!--                                              m3$results$RMSE)) -->\n--\\>\n\n<!-- <!-- # plot results from LOOCV -->\n\n--\\>\n\n<!-- <!-- ggplot(data = df, aes(x = model_number, y =  RMSE)) +    -->\n\n--\\> <!-- <!--   geom_point() + geom_line() --> --\\>\n\n<!-- <!-- ``` -->\n\n--\\>\n\n<!-- ## Leave-One-Out Cross-Validation (LOOCV) -->\n\n<!-- **Advantages** -->\n\n<!-- * LOOCV will give approximately unbiased estimates of the test error, since each training set contains $nâˆ’1$ observations, which is almost as many as the number of observations in the full training dataset. -->\n\n<!-- <!-- * Has far less bias than the validation set approach, since the training sets (used to fit the model) are almost as big as the original dataset. -->\n\n--\\>\n\n<!-- * Performing LOOCV multiple times will always yield the same results. -->\n\n<!-- **Disadvantages** -->\n\n<!-- * Can be potentially expensive to implement, specially for large $n$. -->\n\n<!-- * LOOCV error estimate can have high variance.  -->\n\n<!-- ## $k$-Fold Cross-Validation -->\n\n<!-- * Randomly divide the training data into $k$ groups or **folds** (approximately equal size). -->\n\n<!-- * Consider one of these folds as the validation set. Fit the model on the remaining $k-1$ folds combined, and obtain predictions for the $k^{th}$ fold. Repeat for all $k$ folds. -->\n\n<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->\n\n<!-- knitr::include_graphics(\"EFT/5.5.PNG\") -->\n\n<!-- ``` -->\n\n<!-- ## <span style=\"color:blue\">Your Turn!!!</span>  -->\n\n<!-- Suppose you implement 10-fold CV on a dataset with $n=100$ observations. -->\n\n<!-- 1. What is the size (number of observations) of each training set? -->\n\n<!-- 2. What is the size of each validation set? -->\n\n<!-- 3. How many steps/iterations are required to complete the overall CV process? -->\n\n<!-- ## $k$-Fold Cross-Validation: Implementation -->\n\n<!-- **Ames Housing dataset** -->\n\n<!-- <!-- ```{r,message=FALSE} -->\n\n--\\> <!-- <!-- ames <- readRDS(\"AmesHousing.rds\")   # load dataset -->\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- Consider `Sale_Price` as the response variable. We will compare the following three linear regression models: -->\n\n<!-- * with `Garage_Area` as the only predictor; -->\n\n<!-- * with `Year_Built` as the only predictor; -->\n\n<!-- * with `Garage_Area` and `Year_Built` as predictors. -->\n\n<!-- <!-- ## $k$-Fold Cross-Validation: Implementation -->\n\n--\\>\n\n<!-- <!-- **Ames Housing dataset** -->\n\n--\\>\n\n<!-- <!-- Split the data into training and test data. -->\n\n--\\>\n\n<!-- <!-- ```{r, message = FALSE} -->\n\n--\\>\n<!-- <!-- set.seed(042023)  # fix the random number generator for reproducibility -->\n--\\>\n\n<!-- <!-- library(caret)  # load library -->\n\n--\\>\n\n<!-- <!-- train_index <- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets -->\n\n--\\>\n\n<!-- <!-- ames_train <- ames[train_index,]   # training data -->\n\n--\\>\n\n<!-- <!-- ames_test <- ames[-train_index,]   # test data -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- ## $k$-Fold Cross-Validation: Implementation -->\n\n<!-- **Ames Housing dataset** -->\n\n<!-- Define CV specifications. -->\n\n<!-- ```{r} -->\n\n<!-- cv_specs <- trainControl(method = \"repeatedcv\",   # CV method -->\n\n<!--                          number = 10,    # number of folds -->\n\n<!--                          repeats = 5)     # number of repeats -->\n\n<!-- ``` -->\n\n<!-- ## $k$-Fold Cross-Validation: Implementation -->\n\n<!-- **Ames Housing dataset** - Implement 10-fold CV with the first model. -->\n\n<!-- ```{r} -->\n\n<!-- set.seed(041824) -->\n\n<!-- m1 <- train(form = Sale_Price ~ Garage_Area,    # specify model -->\n\n<!--             data = ames_train,   # specify dataset -->\n\n<!--             method = \"lm\",       # specify type of model -->\n\n<!--             trControl = cv_specs, # CV specifications -->\n\n<!--             metric = \"RMSE\")   # metric to evaluate model -->\n\n<!-- m1   # summary of CV -->\n\n<!-- m1$results  # estimate and variability of metrics -->\n\n<!-- ``` -->\n\n<!-- ## $k$-Fold Cross-Validation: Implementation -->\n\n<!-- **Ames Housing dataset** - Implement 10-fold CV with the second model. -->\n\n<!-- ```{r} -->\n\n<!-- set.seed(041824) -->\n\n<!-- m2 <- train(form = Sale_Price ~ Year_Built,   -->\n\n<!--             data = ames_train,           -->\n\n<!--             method = \"lm\",               -->\n\n<!--             trControl = cv_specs,        -->\n\n<!--             metric = \"RMSE\")            -->\n\n<!-- m2 -->\n\n<!-- m2$results -->\n\n<!-- ``` -->\n\n<!-- ## $k$-Fold Cross-Validation: Implementation -->\n\n<!-- **Ames Housing dataset** - Implement 10-fold CV with the third model. -->\n\n<!-- ```{r} -->\n\n<!-- set.seed(041824) -->\n\n<!-- m3 <- train(form = Sale_Price ~ Garage_Area + Year_Built,   -->\n\n<!--             data = ames_train, -->\n\n<!--             method = \"lm\", -->\n\n<!--             trControl = cv_specs, -->\n\n<!--             metric = \"RMSE\") -->\n\n<!-- m3 -->\n\n<!-- m3$results -->\n\n<!-- ``` -->\n\n<!-- ## $k$-Fold Cross-Validation: Results  -->\n\n<!-- **Ames Housing dataset** -->\n\n<!-- Compare CV results for different models. -->\n\n<!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->\n\n<!-- # create data frame to plot results -->\n\n<!-- df <- data.frame(model_number = 1:3, RMSE = c(m1$results$RMSE,   -->\n\n<!--                                              m2$results$RMSE, -->\n\n<!--                                              m3$results$RMSE)) -->\n\n<!-- # plot results from CV -->\n\n<!-- ggplot(data = df, aes(x = model_number, y =  RMSE)) +    -->\n\n<!--   geom_point() + geom_line() -->\n\n<!-- ``` -->\n\n<!-- <!-- ## $k$-Fold Cross-Validation -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\>\n<!-- <!-- # comparing 4 polynomial (regression) models with k-fold CV (response: 'mpg', predictor: 'horsepower') -->\n--\\>\n\n<!-- <!-- set.seed(041920221)   # fix the random number generator for reproducibility -->\n\n--\\>\n\n<!-- <!-- # CV specifications (method, number of folds k, number of repetitions) -->\n\n--\\>\n<!-- <!-- cv_specs <- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)  -->\n--\\>\n\n<!-- <!-- m1 <- train(mpg ~ horsepower,   # model: y = beta_0 + beta_1 x -->\n\n--\\> <!-- <!--             data = Auto_train, --> --\\>\n<!-- <!--             method = \"lm\", --> --\\>\n<!-- <!--             trControl = cv_specs, --> --\\>\n<!-- <!--             metric = \"RMSE\") --> --\\>\n\n<!-- <!-- m1   # summary of model with LOOCV -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## $k$-Fold Cross-Validation -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\>\n<!-- <!-- m2 <- train(mpg ~ poly(horsepower,2),   # model: y = beta_0 + beta_1 x + beta_2 x^2 -->\n--\\> <!-- <!--             data = Auto_train, --> --\\>\n<!-- <!--             method = \"lm\", --> --\\>\n<!-- <!--             trControl = cv_specs, --> --\\>\n<!-- <!--             metric = \"RMSE\") --> --\\>\n\n<!-- <!-- m3 <- train(mpg ~ poly(horsepower, 3),   # model: y = beta_0 + beta_1 x + beta_2 x^2 + beta_3 x^3 -->\n\n--\\> <!-- <!--             data = Auto_train, --> --\\>\n<!-- <!--             method = \"lm\", --> --\\>\n<!-- <!--             trControl = cv_specs, --> --\\>\n<!-- <!--             metric = \"RMSE\") --> --\\>\n\n<!-- <!-- m4 <- train(mpg ~ poly(horsepower, 4),   # model: y = beta_0 + beta_1 x + beta_2 x^2 + beta_3 x^3 + beta_4 x^4 -->\n\n--\\> <!-- <!--             data = Auto_train, --> --\\>\n<!-- <!--             method = \"lm\", --> --\\>\n<!-- <!--             trControl = cv_specs, --> --\\>\n<!-- <!--             metric = \"RMSE\") --> --\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## $k$-Fold Cross-Validation -->\n\n--\\>\n\n<!-- <!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->\n\n--\\>\n<!-- <!-- df <- data.frame(poly_degree = 1:4, RMSE = c(m1$results$RMSE,   # create data frame to plot results -->\n--\\>\n<!-- <!--                                              m2$results$RMSE, -->\n--\\>\n<!-- <!--                                              m3$results$RMSE, -->\n--\\>\n<!-- <!--                                              m4$results$RMSE)) -->\n--\\>\n\n<!-- <!-- ggplot(data = df, aes(x = poly_degree, y =  RMSE)) +    # plot results from LOOCV -->\n\n--\\> <!-- <!--   geom_point() + geom_line() --> --\\> <!-- <!-- ``` -->\n--\\>\n\n<!-- ## Final Model and Prediction Error Estimate -->\n\n<!-- **Ames Housing dataset** -->\n\n<!-- ```{r} -->\n\n<!-- # after choosing final (optimal) model, refit final model using ALL training data -->\n\n<!-- m3$finalModel     -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n\n<!-- # obtain estimate of prediction error from test data -->\n\n<!-- final_model_preds <- predict(m3, newdata = ames_test)   # obtain predictions on test data -->\n\n<!-- pred_error_est <- sqrt(mean((ames_test$Sale_Price - final_model_preds)^2))    # calculate RMSE (estimate of prediction error) from test data -->\n\n<!-- pred_error_est   # test set RMSE -->\n\n<!-- ``` -->\n\n<!-- <!-- ## Variable Importance -->\n\n--\\>\n\n<!-- <!-- **Ames Housing dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->\n\n--\\> <!-- <!-- # variable importance --> --\\>\n\n<!-- <!-- library(vip) -->\n\n--\\>\n\n<!-- <!-- vip(object = m3,         # CV object  -->\n\n--\\>\n<!-- <!--     num_features = 20,   # maximum number of predictors to show importance for -->\n--\\>\n<!-- <!--     method = \"model\")            # model-specific VI scores -->\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## $k$-Fold Cross-Validation -->\n\n--\\>\n\n<!-- <!-- Let $n_i$ be the number of observations in  $i^{th}, (i=1,\\ldots,k)$ fold. If $n$ is a multiple of $k$, then $n_i=\\frac{n}{k}$. -->\n\n--\\>\n\n<!-- <!-- ```{r , echo=FALSE,  fig.align='center', out.width = '40%'} -->\n\n--\\> <!-- <!-- knitr::include_graphics(\"EFT/e5.3.PNG\") --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- If the folds are not of equal size, -->\n\n--\\>\n\n<!-- <!-- $$CV_{(k)}=\\displaystyle \\sum_{i=1}^k \\dfrac{n_i}{n} MSE_{i}$$ -->\n\n--\\>\n\n<!-- <!-- A good choice is $k=5$ or $10$. LOOCV is a special case of $k$-fold CV. -->\n\n--\\>\n\n<!-- <!-- ## LOOCV and $k$-Fold CV -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->\n\n--\\> <!-- <!-- knitr::include_graphics(\"EFT/5.4.PNG\") --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## Performance of LOOCV and $k$-Fold CV -->\n\n--\\>\n\n<!-- <!-- <!-- **Figures 2.9-2.11 in Chapter 2** -->\n\n--\\>\n\n<!-- <!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->\n\n--\\> <!-- <!-- knitr::include_graphics(\"EFT/5.6.PNG\") --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- ## Bias-Variance Trade-off for LOOCV and $k$-fold CV -->\n\n<!-- * LOOCV has very less bias. Using $k=5$ or $10$ yields more bias than LOOCV. -->\n\n<!-- <!-- , but less than validation set approach. -->\n\n--\\>\n\n<!-- * For LOOCV, the error estimates for each fold are highly (positively) correlated. $k$-fold CV error estimates are somewhat less correlated. LOOCV error estimate has higher variance than $k$-fold CV error estimate. -->\n\n<!-- * Typically, $k=5$ or $10$ is chosen. -->\n\n<!-- <!-- ## CV to Tune Hyperparameter -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r,message=FALSE} -->\n\n--\\> <!-- <!-- library(ISLR2)  # load library --> --\\>\n\n<!-- <!-- data(\"Auto\")   # load dataset -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- We will consider `mpg` as the response and `horsepower` as the predictor.  -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\> <!-- <!-- # select the variables to work with --> --\\>\n\n<!-- <!-- Auto <- Auto %>% select(mpg, horsepower) -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## CV to Tune Hyperparameter -->\n\n--\\>\n\n<!-- <!-- **Objective**: Find the optimum choice of $K$ in the KNN approach with 5-fold CV repeated 5 times. We will use the following steps. -->\n\n--\\>\n\n<!-- <!-- * Perform EDA (Exploratory Data Analysis) -->\n\n--\\>\n\n<!-- <!-- * Split the data into training and test data (80-20 split). -->\n\n--\\>\n\n<!-- <!-- * Specify CV specifications using **trainControl**. -->\n\n--\\>\n\n<!-- <!-- * Create an object **k_grid** using the following code. -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\>\n<!-- <!-- k_grid <- expand.grid(k = seq(1, 100, by = 1))  # creates a grid of k values to be used (1 to 100 in this case) -->\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- * Use the **train** function to run CV. Use **method = \"knn\"**, **tuneGrid = k_grid**, and **metric = \"RMSE\"**. -->\n\n--\\>\n\n<!-- <!-- * Obtain the results and plot them. What is the optimum $k$ chosen? -->\n\n--\\>\n\n<!-- <!-- * Create the final model using the optimum $k$ and estimate its prediction error from the test data. -->\n\n--\\>\n\n<!-- <!-- ## CV to Tune Hyperparameter: EDA {.smaller} -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\> <!-- <!-- summary(Auto)   # summary of the variables --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- ```{r, fig.align='center', fig.height=4, fig.width=4} -->\n\n--\\> <!-- <!-- library(GGally) --> --\\>\n<!-- <!-- ggpairs(Auto)    # correlation plot --> --\\> <!-- <!-- ``` -->\n--\\>\n\n<!-- <!-- ## CV to Tune Hyperparameter: Split Data -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r, message = FALSE} -->\n\n--\\>\n<!-- <!-- set.seed(041824)  # fix the random number generator for reproducibility -->\n--\\>\n\n<!-- <!-- library(caret)  # load library -->\n\n--\\>\n\n<!-- <!-- train_index <- createDataPartition(y = Auto$mpg, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets -->\n\n--\\>\n\n<!-- <!-- Auto_train <- Auto[train_index,]   # training data -->\n\n--\\>\n\n<!-- <!-- Auto_test <- Auto[-train_index,]   # test data -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## CV to Tune Hyperparameter: Perform CV -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\>\n<!-- <!-- set.seed(041824)  # fix the random number generator for reproducibility -->\n--\\>\n\n<!-- <!-- # CV specifications -->\n\n--\\>\n<!-- <!-- cv_specs <- trainControl(method = \"repeatedcv\", number = 5, repeats = 5) -->\n--\\>\n\n<!-- <!-- # specify grid of 'k' values to search over -->\n\n--\\> <!-- <!-- k_grid <- expand.grid(k = seq(1, 100, by = 1)) --> --\\>\n\n<!-- <!-- # train the KNN model using CV  to find optimal 'k' -->\n\n--\\> <!-- <!-- knn_cv <- train(form = mpg ~ horsepower, --> --\\>\n<!-- <!--                  data = Auto_train, --> --\\>\n<!-- <!--                  method = \"knn\", --> --\\>\n<!-- <!--                  trControl = cv_specs, --> --\\>\n<!-- <!--                  tuneGrid = k_grid, --> --\\>\n<!-- <!--                  metric = \"RMSE\") --> --\\> <!-- <!-- ``` -->\n--\\>\n\n<!-- <!-- ## CV to Tune Hyperparameter: Compare CV Results -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r, eval=FALSE} -->\n\n--\\> <!-- <!-- knn_cv   # CV results, shows RMSE for all K --> --\\>\n<!-- <!-- ``` --> --\\>\n\n<!-- <!-- ```{r, fig.align='center'} -->\n\n--\\> <!-- <!-- ggplot(knn_cv)   # plot CV results for different 'k' -->\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ## CV to Tune Hyperparameter: Final Model -->\n\n--\\>\n\n<!-- <!-- **Auto dataset** -->\n\n--\\>\n\n<!-- <!-- ```{r} -->\n\n--\\> <!-- <!-- # final model with optimal 'k' chosen from CV --> --\\>\n\n<!-- <!-- knn_cv$bestTune     # optimal value of K -->\n\n--\\>\n\n<!-- <!-- knn_cv$finalModel   # final model -->\n\n--\\> <!-- <!-- ``` --> --\\>\n\n<!-- <!-- ```{r} -->\n\n--\\> <!-- <!-- # obtain predictions on test data --> --\\>\n<!-- <!-- final_model_preds <- predict(knn_cv, newdata = Auto_test) -->\n--\\>\n\n<!-- <!-- # estimate test set prediction error -->\n\n--\\>\n<!-- <!-- sqrt(mean((Auto_test$mpg - final_model_preds)^2))    # test set RMSE -->\n--\\> <!-- <!-- ``` --> --\\>\n",
    "supporting": [
      "CopyOf08-classifiction-metrics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}