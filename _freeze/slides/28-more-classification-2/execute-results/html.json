{
  "hash": "2bdd2c5cb328461fab59b64ac4cbf4ca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: More More on Classification'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: false\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n\n\n## Data: Voter Frequency\n\n-   [Info about data](https://github.com/fivethirtyeight/data/tree/master/non-voters)\n-   Goal: Identify individuals who are unlikely to vote to help organization target \"get out the vote\" effort.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoter_data <- read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv')\n\nvoter_clean <- voter_data |>\n  select(-RespId, -weight, -Q1) |>\n  mutate(\n    educ = factor(educ, levels = c(\"High school or less\", \"Some college\", \"College\")),\n    income_cat = factor(income_cat, levels = c(\"Less than $40k\", \"$40-75k \",\n                                               \"$75-125k\", \"$125k or more\")),\n    voter_category = factor(voter_category, levels = c(\"rarely/never\", \"sporadic\", \"always\"))\n  ) |>\n  filter(Q22 != 5 | is.na(Q22)) |>\n  mutate(Q22 = as_factor(Q22),\n         Q22 = if_else(is.na(Q22), \"Not Asked\", Q22),\n         across(Q28_1:Q28_8, ~if_else(.x == -1, 0, .x)),\n         across(Q28_1:Q28_8, ~ as_factor(.x)),\n         across(Q28_1:Q28_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n         across(Q29_1:Q29_10, ~if_else(.x == -1, 0, .x)),\n         across(Q29_1:Q29_10, ~ as_factor(.x)),\n         across(Q29_1:Q29_8, ~if_else(is.na(.x) , \"Not Asked\", .x)),\n        Party_ID = as_factor(case_when(\n          Q31 == 1 ~ \"Strong Republican\",\n          Q31 == 2 ~ \"Republican\",\n          Q32 == 1  ~ \"Strong Democrat\",\n          Q32 == 2 ~ \"Democrat\",\n          Q33 == 1 ~ \"Lean Republican\",\n          Q33 == 2 ~ \"Lean Democrat\",\n          TRUE ~ \"Other\"\n        )),\n        Party_ID = factor(Party_ID, levels =c(\"Strong Republican\", \"Republican\", \"Lean Republican\",\n                                                \"Other\", \"Lean Democrat\", \"Democrat\", \"Strong Democrat\")),\n        across(!ppage, ~as_factor(if_else(.x == -1, NA, .x))))\n```\n:::\n\n\n\n## Split Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\nvoter_splits <- initial_split(voter_clean, prop = 0.7, strata = voter_category)\nvoter_train <- training(voter_splits)\nvoter_test <- testing(voter_splits)\n```\n:::\n\n\n\n## Problem: More than two categories\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoter_train |> \n  ggplot(aes(x = voter_category)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](28-more-classification-2_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n## Define Model: Multinomial Regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmn_reg_model <- multinom_reg(mixture = 1, penalty = 0.005) |> # I chose this penalty arbitrarily\n  set_engine(\"glmnet\", family = \"multinomial\") |> \n  set_mode(\"classification\")\n```\n:::\n\n\n\n## Define Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_recipe <- recipe(voter_category ~ . , data = voter_train) |>\n  step_zv(all_predictors()) |>\n  step_integer(educ, income_cat, Party_ID, Q2_2:Q4_6, Q6, Q8_1:Q9_4, Q14:Q17_4,\n               Q25:Q26) |>\n  step_impute_median(all_numeric_predictors()) |>\n  step_impute_mode(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Define Workflow and Fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit <- workflow() |>\n  add_model(mn_reg_model) |>\n  add_recipe(mr_recipe) |> \n  fit(voter_train)\n```\n:::\n\n\n\n## Look at Predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> slice_sample(n=10) |> select(1:4) |> head() |>  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class | .pred_rarely/never| .pred_sporadic| .pred_always|\n|:-----------|------------------:|--------------:|------------:|\n|sporadic    |          0.0317402|      0.5122327|    0.4560271|\n|always      |          0.0463060|      0.4464935|    0.5072005|\n|sporadic    |          0.0553279|      0.6529393|    0.2917328|\n|always      |          0.0388506|      0.4351109|    0.5260385|\n|always      |          0.0361359|      0.4164662|    0.5473979|\n|always      |          0.0991499|      0.4310622|    0.4697879|\n\n\n:::\n:::\n\n\n\n## Confusion Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> \n  conf_mat(truth = voter_category, estimate = .pred_class) |> autoplot(\"heatmap\")\n```\n\n::: {.cell-output-display}\n![](28-more-classification-2_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n## Last Time {.smaller}\n\n-   No \"Positive\" and \"Negative\" anymore\n-   Most of our metrics were based on having \"Positive\" vs. \"Negative\"\n-   Solution 1: 1-vs-all metrics\n    \n## Evaluating Multiclass Models\n\n-   Solution 2: Average metrics across labels\n    +   *Macro-averaging* average one-versus-all metrics\n        +   Recall: $\\frac{0.66+0.72+0.41}{3} \\approx 0.60$\n    +   *Macro-weighted averaging* same but weight by class size\n        +   Recall: $\\frac{430\\times 0.66+773\\times 0.72+543\\times 0.41}{1746} \\approx 0.61$\n    +   *Micro-averaging* compute contribution for each class, aggregates them, then computes a single metric\n        +   Recall: $\\frac{285+559+221}{430+779+543} \\approx 0.61$\n        \n## Questions From Last Time\n\n-   Multinomial Logistic regression:\n    +   Each class $k$: $$\\log \\frac{\\text{Prob. class } k}{\\text{Prob. class } K} = \\beta_{0k} + \\beta_{1k}X_1 + \\cdots + \\beta_{pk}X_p$$\n-   Shuba: Micro-averaged recall is the same as accuracy... CORRECT!\n    +   Same is true of precision AND $F_1$\n    +   Useful if you have a \"multi-label\" classification problem (not covered in this class)\n    \n## Rabin's Question {.smaller}\n\n-   \"What does a medium sized data set mean?\"\n-   Idea behind ML: identify patterns in data and use them to make predictions\n-   As data gets \"bigger\":\n    +   Patterns become clearer\n    +   Computational complexity increases\n-   Informal definitions:\n    +   Small data: not enough data to fully represent patterns\n    +   Big data: all the info is there but special approaches need to be taken to handle all the data\n    \n## Thinking about small data\n\n-   Patterns not fully represented in data $\\Rightarrow$ restrict the set of possible patterns and give model less flexibility and freedom\n    +   Logistic regression (probably with regularization)\n    +   Support vector machines (we haven't talked about these yet)\n    +   To a lesser extent: Decision Trees\n    +   NOT KNN\n\n## Thinking about big data {.smaller}\n\n-   Patterns are definitely there but size introduces computational problems\n    +   Data set can't fit in memory\n        +   Solution 1: Use a high-memory HPC cluster node\n        +   Solution 2: Modify algorithms to use parts of data instead of full data set (e.g. Stochastic gradient descent)\n    +   Algorithm scales with size of data and will take too long to run/fit\n        +   Solution 1: Run in parallel if possible using HPC cluster\n        +   Solution 2: Develop faster algorithms\n            +   Implement in a compiled language like C\n            +   Develop (faster) approximate solution\n-   Curse of dimensionalality\n    +   If $p$ is too-big $\\Rightarrow$ too much space and things are too far apart $\\Rightarrow$ similar impact to small data but without computational benefit\n-   Note: big $n$ vs. big $p$ can present different issues\n            \n## Medium Data\n\n-   Data that's big enough to have (most) of the information you need but not so big that it presents computational issues\n-   KNN sweet spot\n    +   Enough data the \"nearest-neighbors\" are actually \"near\"\n    +   Not so much data that it takes forever to make predictions\n    \n## Exploring with App\n\n-   [App 1](https://efriedlander.shinyapps.io/ClassificationMetrics/)\n-   [App 2](https://efriedlander.shinyapps.io/BVTappClassification/)\n        \n## Macro-Averaging in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoter_metrics <- metric_set(accuracy, precision, recall)\n\nmr_fit |> augment(new_data = voter_test) |> \n  voter_metrics(truth = voter_category, estimate = .pred_class, estimator = \"macro\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator | .estimate|\n|:---------|:----------|---------:|\n|accuracy  |multiclass | 0.6099656|\n|precision |macro      | 0.6375886|\n|recall    |macro      | 0.5976485|\n\n\n:::\n:::\n\n\n        \n## Macro-Weighted Averaging in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> \n  voter_metrics(truth = voter_category, estimate = .pred_class, estimator = \"macro_weighted\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator     | .estimate|\n|:---------|:--------------|---------:|\n|accuracy  |multiclass     | 0.6099656|\n|precision |macro_weighted | 0.6176222|\n|recall    |macro_weighted | 0.6099656|\n\n\n:::\n:::\n\n\n\n## Micro-Averaging in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> \n  voter_metrics(truth = voter_category, estimate = .pred_class, estimator = \"micro\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator | .estimate|\n|:---------|:----------|---------:|\n|accuracy  |multiclass | 0.6099656|\n|precision |micro      | 0.6099656|\n|recall    |micro      | 0.6099656|\n\n\n:::\n:::\n\n\n\n## What if output is probability?\n\n-   For binary case we used ROC curve and AUC...\n-   Similar ideas apply here:\n    +   One vs. all\n    +   Macro Averaging\n    +   NO MICRO AVERAGING!\n    +   Hand and Till extension of AUC\n\n## Plotting one-vs.-all\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> \n  roc_curve(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always) |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](28-more-classification-2_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n## Macro Averaged AUC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always, \n          estimator = \"macro\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |macro      | 0.7802495|\n\n\n:::\n:::\n\n\n\n## Macro-Weighted Averaged AUC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always, \n          estimator = \"macro_weighted\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator     | .estimate|\n|:-------|:--------------|---------:|\n|roc_auc |macro_weighted | 0.7636223|\n\n\n:::\n:::\n\n\n\n## Hand and Till AUC {.smaller}\n\n-   Idea behind traditional AUC: \"How well are my classes being separated?\"\n-   Hand and Till: Extend this idea to multiple-classes\n-   [Paper](https://link.springer.com/article/10.1023/A:1010920819831)\n    +   Basic Idea: Do pairwise comparison of classes and average\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmr_fit |> augment(new_data = voter_test) |> \n  roc_auc(truth = voter_category, `.pred_rarely/never`, .pred_sporadic, .pred_always) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|roc_auc |hand_till  | 0.7967478|\n\n\n:::\n:::\n\n\n\n## Discussion\n\n-   Why do you compute averages/means?\n\n. . .\n\n-   How would heavily imbalanced classes impact each type of averaging?\n    +   Which type(s) of averaging weight(s) each class equally regardless of balance?\n    +   Which type(s) of averaging favor(s) larger classes?\n    +   Does this imply that one is better than the others?\n    \n## Exploring with App {.smaller}\n\n-   [App](https://efriedlander.shinyapps.io/ClassificationMetrics/)\n    +   Break into groups\n    +   Investigate how your performance metrics change between balanced data and unbalanced data\n    +   Additional Considerations:\n        +   Impact of boundaries/models?\n        +   Impact of sample size?\n        +   Impact of noise level?\n    +   Please write down observations so we can discuss\n    \n",
    "supporting": [
      "28-more-classification-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}