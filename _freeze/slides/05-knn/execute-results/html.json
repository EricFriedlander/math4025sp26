{
  "hash": "b116ba4c4e501a7092450f95976d2f80",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MAT-427: Data Splitting + KNN'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n\n## Computational Setup\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(readODS)\nlibrary(modeldata) # contains ames dataset\n\ntidymodels_prefer()\n\nmlr_model <- linear_reg() |> \n  set_engine(\"lm\")\n```\n:::\n\n\n\n\n## Comparing Models: Data Splitting {.smaller}\n\n- Split `ames` data set into two parts\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Test set: randomly selected proportion $1-p$ of data used for estimating prediction error\n- If comparing A LOT of models, split into *three* parts to prevent **information leakage**\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Validation set: randomly selected proportion $q$ (typically 20-30%) of data used to choosing tuning parameters\n  + Test set: randomly selected proportion $1-p-q$ of data used for estimating prediction error\n- Idea: use data your model hasn't seen to get more accurate estimate of error and prevent overfitting\n\n## Comparing Models: Data Splitting with `tidymodels` {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427) # Why?\n\names_split <- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<2049/881/2930>\n```\n\n\n:::\n\n```{.r .cell-code}\names_train <- training(ames_split) # get training data\names_test <- testing(ames_split) # get test data\n```\n:::\n\n\n\n\n- `strata` not necessary but good practice\n  + `strata` will use *stratified sampling* on the variable you specify (very little downside) \n\n## Linear Regression: Comparing Models {.smaller}\n\n- Let's create three models with `Sale_Price` as the response:\n  + **fit1**: a linear regression model with `Bedroom_AbvGr`  as the only predictor\n  + **fit2**: a linear regression model with `Gr_Liv_Area` as the only predictor\n  + **fit3** (similar to model in previous slides): a multiple regression model with `Gr_Liv_Area` and `Bedroom_AbvGr` as predictors\n  + **fit4**: super flexible model which fits a 10th degree polynomial to `Gr_Liv_Area` and a 2nd degree polynomial to `Bedroom_AbvGr`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- mlr_model |> fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set\nfit2 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)\nfit3 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)\nfit4 <- mlr_model |> fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)\n```\n:::\n\n\n\n\n## Computing MSE {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 1\nfit1_train_mse <- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)\nfit1_test_mse <- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)\n\n# Fit 2\nfit2_train_mse <- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)\nfit2_test_mse <- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit3_train_mse <- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)\nfit3_test_mse <- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit4_train_mse <- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)\nfit4_test_mse <- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)\n```\n:::\n\n\n\n\n## [Question]{style=\"color:blue\"}\n\nWithout looking at the numbers\n\n1. Do we know which of the following is the smallest: `fit1_train_mse`, `fit2_train_mse`, `fit3_train_mse`, `fit4_train_mse`? [Yes, `fit4_train_mse`]{.fragment .fade-in}\n2. Do we know which of the following is the smallest: `fit1_test_mse`, `fit2_test_mse`, `fit3_test_mse`, `fit4_test_mse`? [No]{.fragment .fade-in}\n\n## Choosing a Model {.smaller}\n\n::::{.columns}\n:::{.column}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training Errors\nc(fit1_train_mse, fit2_train_mse, \n  fit3_train_mse, fit4_train_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6213135279 3188099910 2781293767 2472424544\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_train_mse, fit2_train_mse, \n            fit3_train_mse, fit4_train_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# test Errors\nc(fit1_test_mse, fit2_test_mse, \n  fit3_test_mse, fit4_test_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.329031e+09 3.203895e+09 2.732389e+09 2.726084e+12\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_test_mse, fit2_test_mse, \n            fit3_test_mse, fit4_test_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n\n:::\n:::{.column}\n- `fit4` has the lowest training MSE (to be expected)\n- `fit3` has the lowest test MSE\n  + We would choose `fit3`\n- Anything else interesting we see?\n:::\n::::\n\n# K-Nearest Neighbors\n\n## Regression: Conditional Averaging {.smaller}\n\n**Restaurant Outlets Profit dataset**\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n\n\nWhat is a good value of $\\hat{f}(x)$ (expected profit), say at $x=6$?\n\nA possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.\n\n\n## K-Nearest Neighbors (KNN) Regression  {.smaller}\n\n- Non-parametric approach\n- Formally: Given a value for $K$ and a test data point $x_0$,\n$$\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) $$\nwhere $\\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.\n- Informally, average together the $K$ \"closest\" observations in your training set\n- \"Closeness\": usually use the **Euclidean metric** to measure distance\n- Euclidean distance between $\\mathbf{X}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})$ and $\\mathbf{x}_j=(x_{j1}, x_{j2}, \\ldots, x_{jp})$:\n$$||\\mathbf{x}_i-\\mathbf{x}_j||_2 = \\sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \\ldots + (x_{ip}-x_{jp    })^2}$$\n\n## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}\n\n::::{.columns}\n:::{.column}\n**$K=1$**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit1 <- nearest_neighbor(neighbors = 1) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"regression\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|   .pred|\n|-------:|\n| 0.92695|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n\n\n:::\n:::{.column}\n**$K=5$**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit5 <- nearest_neighbor(neighbors = 5) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"regression\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|    .pred|\n|--------:|\n| 4.113736|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n:::\n::::\n\n## Regression Methods: Comparison\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## <span style=\"color:blue\">Question!!!</span>\n\nAs $K$ in KNN regression increases:\n\n- the flexibility of the fit ([decreases]{.fragment .highlight-red} /increases)\n- the bias of the fit (decreases/[increases]{.fragment .highlight-red} )\n- the variance of the fit ([decreases]{.fragment .highlight-red}/increases)\n\n\n## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}\n\n- Let's look at the `ames` data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |>\n  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |>\n  head() |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n| Sale_Price| Gr_Liv_Area| Bedroom_AbvGr|\n|----------:|-----------:|-------------:|\n|     215000|        1656|             3|\n|     105000|         896|             2|\n|     172000|        1329|             3|\n|     244000|        2110|             3|\n|     189900|        1629|             3|\n|     195500|        1604|             3|\n\n\n:::\n:::\n\n\n\n:::{.fragment}\n:::{.incremental}\n- Should 1 square foot count the same as 1 bedroom?\n- Need to **center and scale** (freq. just say scale)\n  + subtract mean from each predictor\n  + divide by standard deviation of each predictor\n  + compares apples-to-apples\n:::\n:::\n\n## Scaling in R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# scale predictors\names_scaled <- tibble(size_scaled = scale(ames$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),\n                                  price = ames$Sale_Price)\n\nhead(ames_scaled) |> kable()  # first six observations\n```\n\n::: {.cell-output-display}\n\n\n| size_scaled| num_bedrooms_scaled|  price|\n|-----------:|-------------------:|------:|\n|   0.3092123|           0.1760642| 215000|\n|  -1.1942232|          -1.0320576| 105000|\n|  -0.3376606|           0.1760642| 172000|\n|   1.2073172|           0.1760642| 244000|\n|   0.2558008|           0.1760642| 189900|\n|   0.2063456|           0.1760642| 195500|\n\n\n:::\n:::\n\n\n\n\n## Question...\n\n:::{.incremental}\n-   What about the training and test sets?\n-   Need to scale BOTH sets based on the mean and standard deviation of the training set...\n-   Discussion: Why?\n-   Discussion: Why don't I need to center and scale `Sale_Price`?\n:::\n\n## Scaling Revisited\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_train_scaled <- tibble(size_scaled = scale(ames_train$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames_train$Bedroom_AbvGr),\n                                  price = ames_train$Sale_Price)\n\names_test_scaled <- tibble(size_scaled = (ames_test$Gr_Liv_Area - mean(ames_train$Gr_Liv_Area)/sd(ames_train$Gr_Liv_Area)),\n                                  num_bedrooms_scaled = (ames_test$Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr),\n                                  price = ames_test$Sale_Price)\n```\n:::\n\n\n\n\n-   Next time: using `recipe`'s in `tidymodels` to simplify this process\n\n## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit10 <- nearest_neighbor(neighbors = 10) |>   # 10-nn regression\n  set_engine(\"kknn\") |> \n  set_mode(\"regression\") |> \n  fit(price ~ size_scaled + num_bedrooms_scaled, data = ames_train_scaled)\n```\n:::\n\n\n\n\n- Test Point: `Gr_Liv_area` = 2000 square feet, and `Bedroom_AbvGr` = 3, then\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtain 10-nn prediction\n\npredict(knnfit10, new_data = tibble(size_scaled = (2000 - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area),\n                                     num_bedrooms_scaled = (3 - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n   .pred\n   <dbl>\n1 256380\n```\n\n\n:::\n:::\n",
    "supporting": [
      "05-knn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}