{
  "hash": "390c063714dfe3caec2836f9d7dd4415",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Preprocessing, Missing Data, and Resampling'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\nlibrary(readODS)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n# Repeated Cross-Validation\n\n## Repeated K-Fold Cross-Validation (CV) {.smaller}\n\n:::{.incremental}\n-   Partition your data into $K$ randomly selected non-overlapping \"folds\"\n    +   Folds don't overlap and every training observation is in one fold\n    +   Each fold contained $1/K$ of the training data\n-   Looping through the folds $k = 1, \\ldots, K$:\n    +   Treat fold $k$ as the assessment set\n    +   Treat all folds except for $k$ as the analysis set\n    +   Fit model to analysis set (use whole modeling workflow)\n    +   Compute error metrics on assessment set\n-   After loop, you will have $K$ copies of each error metrics\n-   Average them together to get performance estimate\n-   Can also look at distribution of performance metrics\n-   Repeat this process from the beginning selected $K$ different randomly chosen folds\n-   If we want more accurate estimate you can perform repeated CV with different randomly chosen folds\n\n:::\n\n# CV Workflow in R\n\n## Data: Ames Housing Prices {.smaller}\n\nA data set from [De Cock (2011)](https://jse.amstat.org/v19n3/decock.pdf) has 82 fields were recorded for 2,930 properties in Ames IA. This version is copies from the `AmesHousing` package but does not include a few quality columns that appear to be outcomes rather than predictors.\n\nGoal: Predict `Sale_Price`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,930\nColumns: 74\n$ MS_SubClass        <fct> One_Story_1946_and_Newer_All_Styles, One_Story_1946â€¦\n$ MS_Zoning          <fct> Residential_Low_Density, Residential_High_Density, â€¦\n$ Lot_Frontage       <dbl> 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,â€¦\n$ Lot_Area           <int> 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005â€¦\n$ Street             <fct> Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pavâ€¦\n$ Alley              <fct> No_Alley_Access, No_Alley_Access, No_Alley_Access, â€¦\n$ Lot_Shape          <fct> Slightly_Irregular, Regular, Slightly_Irregular, Reâ€¦\n$ Land_Contour       <fct> Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, Lâ€¦\n$ Utilities          <fct> AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, Allâ€¦\n$ Lot_Config         <fct> Corner, Inside, Corner, Corner, Inside, Inside, Insâ€¦\n$ Land_Slope         <fct> Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gâ€¦\n$ Neighborhood       <fct> North_Ames, North_Ames, North_Ames, North_Ames, Gilâ€¦\n$ Condition_1        <fct> Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, Noâ€¦\n$ Condition_2        <fct> Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norâ€¦\n$ Bldg_Type          <fct> OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twnâ€¦\n$ House_Style        <fct> One_Story, One_Story, One_Story, One_Story, Two_Stoâ€¦\n$ Overall_Cond       <fct> Average, Above_Average, Above_Average, Average, Aveâ€¦\n$ Year_Built         <int> 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199â€¦\n$ Year_Remod_Add     <int> 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199â€¦\n$ Roof_Style         <fct> Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, Gâ€¦\n$ Roof_Matl          <fct> CompShg, CompShg, CompShg, CompShg, CompShg, CompShâ€¦\n$ Exterior_1st       <fct> BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylSâ€¦\n$ Exterior_2nd       <fct> Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylSâ€¦\n$ Mas_Vnr_Type       <fct> Stone, None, BrkFace, None, None, BrkFace, None, Noâ€¦\n$ Mas_Vnr_Area       <dbl> 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6â€¦\n$ Exter_Cond         <fct> Typical, Typical, Typical, Typical, Typical, Typicaâ€¦\n$ Foundation         <fct> CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConcâ€¦\n$ Bsmt_Cond          <fct> Good, Typical, Typical, Typical, Typical, Typical, â€¦\n$ Bsmt_Exposure      <fct> Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,â€¦\n$ BsmtFin_Type_1     <fct> BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, Uâ€¦\n$ BsmtFin_SF_1       <dbl> 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, â€¦\n$ BsmtFin_Type_2     <fct> Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Uâ€¦\n$ BsmtFin_SF_2       <dbl> 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0â€¦\n$ Bsmt_Unf_SF        <dbl> 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,â€¦\n$ Total_Bsmt_SF      <dbl> 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, â€¦\n$ Heating            <fct> GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gasâ€¦\n$ Heating_QC         <fct> Fair, Typical, Typical, Excellent, Good, Excellent,â€¦\n$ Central_Air        <fct> Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, â€¦\n$ Electrical         <fct> SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBâ€¦\n$ First_Flr_SF       <int> 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, â€¦\n$ Second_Flr_SF      <int> 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,â€¦\n$ Gr_Liv_Area        <int> 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616â€¦\n$ Bsmt_Full_Bath     <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, â€¦\n$ Bsmt_Half_Bath     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Full_Bath          <int> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, â€¦\n$ Half_Bath          <int> 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, â€¦\n$ Bedroom_AbvGr      <int> 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, â€¦\n$ Kitchen_AbvGr      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ TotRms_AbvGrd      <int> 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,â€¦\n$ Functional         <fct> Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Tâ€¦\n$ Fireplaces         <int> 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, â€¦\n$ Garage_Type        <fct> Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attâ€¦\n$ Garage_Finish      <fct> Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, Fâ€¦\n$ Garage_Cars        <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, â€¦\n$ Garage_Area        <dbl> 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4â€¦\n$ Garage_Cond        <fct> Typical, Typical, Typical, Typical, Typical, Typicaâ€¦\n$ Paved_Drive        <fct> Partial_Pavement, Paved, Paved, Paved, Paved, Pavedâ€¦\n$ Wood_Deck_SF       <int> 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48â€¦\n$ Open_Porch_SF      <int> 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0â€¦\n$ Enclosed_Porch     <int> 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ Three_season_porch <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Screen_Porch       <int> 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, â€¦\n$ Pool_Area          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Pool_QC            <fct> No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Pooâ€¦\n$ Fence              <fct> No_Fence, Minimum_Privacy, No_Fence, No_Fence, Miniâ€¦\n$ Misc_Feature       <fct> None, None, Gar2, None, None, None, None, None, Nonâ€¦\n$ Misc_Val           <int> 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, â€¦\n$ Mo_Sold            <int> 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, â€¦\n$ Year_Sold          <int> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201â€¦\n$ Sale_Type          <fct> WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , Wâ€¦\n$ Sale_Condition     <fct> Normal, Normal, Normal, Normal, Normal, Normal, Norâ€¦\n$ Sale_Price         <int> 215000, 105000, 172000, 244000, 189900, 195500, 213â€¦\n$ Longitude          <dbl> -93.61975, -93.61976, -93.61939, -93.61732, -93.638â€¦\n$ Latitude           <dbl> 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4â€¦\n```\n\n\n:::\n:::\n\n\n\n## Initial Data Split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\ndata_split <- initial_split(ames, strata = \"Sale_Price\")\names_train <- training(data_split)\names_test  <- testing(data_split)\n```\n:::\n\n\n\n## Define Folds\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_folds <- vfold_cv(ames_train, v = 10, repeats = 10)\names_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation repeated 10 times \n# A tibble: 100 Ã— 3\n   splits             id       id2   \n   <list>             <chr>    <chr> \n 1 <split [1977/220]> Repeat01 Fold01\n 2 <split [1977/220]> Repeat01 Fold02\n 3 <split [1977/220]> Repeat01 Fold03\n 4 <split [1977/220]> Repeat01 Fold04\n 5 <split [1977/220]> Repeat01 Fold05\n 6 <split [1977/220]> Repeat01 Fold06\n 7 <split [1977/220]> Repeat01 Fold07\n 8 <split [1978/219]> Repeat01 Fold08\n 9 <split [1978/219]> Repeat01 Fold09\n10 <split [1978/219]> Repeat01 Fold10\n# â„¹ 90 more rows\n```\n\n\n:::\n:::\n\n\n\n## Define Model(s)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model <- linear_reg() |>\n  set_engine('lm')\n\nknn5_model <- nearest_neighbor(neighbors = 5) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"regression\")\n\nknn10_model <- nearest_neighbor(neighbors = 10) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"regression\")\n```\n:::\n\n\n\n## Define Preprocessing: Linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preproc <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_dummy(all_nominal_predictors()) |> # Convert categorical data into dummy variables\n  step_zv(all_predictors()) |> # remove zero-variance predictors (i.e. predictors with one value)\n  step_corr(all_predictors(), threshold = 0.5) |> # remove highly correlated predictors\n  step_lincomb(all_predictors()) # remove variables that have exact linear combinations\n```\n:::\n\n\n\n## Define Preprocessing: KNN\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_preproc <- recipe(Sale_Price ~ ., data = ames_train) |> # only uses ames_train for data types\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> # Convert categorical data into dummy variables\n  step_zv(all_predictors()) |> # remove zero-variance predictors (i.e. predictors with one value)\n  step_normalize(all_predictors())\n```\n:::\n\n\n\n## Define Workflows\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_wf <- workflow() |> add_model(lm_model) |> add_recipe(lm_preproc)\nknn5_wf <- workflow() |> add_model(knn5_model) |> add_recipe(knn_preproc)\nknn10_wf <- workflow() |> add_model(knn10_model) |> add_recipe(knn_preproc)\n```\n:::\n\n\n\n## Define Metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_metrics <- metric_set(rmse, rsq)\n```\n:::\n\n\n\n## Fit and Assess Models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_results <- lm_wf |> fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn5_results <- knn5_wf |> fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn10_results <- knn10_wf |> fit_resamples(resamples = ames_folds, metrics = ames_metrics)\n```\n:::\n\n\n\n\n## Collecting Metrics {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lm_results) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |         mean|   n|     std_err|.config              |\n|:-------|:----------|------------:|---:|-----------:|:--------------------|\n|rmse    |standard   | 3.308697e+04| 100| 519.8344225|Preprocessor1_Model1 |\n|rsq     |standard   | 8.293514e-01| 100|   0.0044723|Preprocessor1_Model1 |\n\n\n:::\n\n```{.r .cell-code}\ncollect_metrics(knn5_results) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |         mean|   n|    std_err|.config              |\n|:-------|:----------|------------:|---:|----------:|:--------------------|\n|rmse    |standard   | 4.029708e+04| 100| 438.177121|Preprocessor1_Model1 |\n|rsq     |standard   | 7.480995e-01| 100|   0.004546|Preprocessor1_Model1 |\n\n\n:::\n\n```{.r .cell-code}\ncollect_metrics(knn10_results) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |         mean|   n|    std_err|.config              |\n|:-------|:----------|------------:|---:|----------:|:--------------------|\n|rmse    |standard   | 3.827391e+04| 100| 431.911500|Preprocessor1_Model1 |\n|rsq     |standard   | 7.774301e-01| 100|   0.003866|Preprocessor1_Model1 |\n\n\n:::\n:::\n\n\n\n## Final Model\n\n- After choosing best model/workflow, fit on full training set and assess on test set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- lm_wf |> fit(data = ames_train)\nfinal_fit_perf <- final_fit |> \n  predict(new_data = ames_test) |> \n  bind_cols(ames_test) |> \n  ames_metrics(truth = Sale_Price, estimate = .pred)\n\nfinal_fit_perf |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |    .estimate|\n|:-------|:----------|------------:|\n|rmse    |standard   | 3.980410e+04|\n|rsq     |standard   | 7.609099e-01|\n\n\n:::\n:::\n\n\n\n## Extracting CV Metrics\n\n-   `collect_metrics` averages over all 100 models\n-   set `summarize = FALSE` to get all the individual errors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lm_results, summarize = FALSE) |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|id       |id2    |.metric |.estimator |    .estimate|.config              |\n|:--------|:------|:-------|:----------|------------:|:--------------------|\n|Repeat01 |Fold01 |rmse    |standard   | 3.879626e+04|Preprocessor1_Model1 |\n|Repeat01 |Fold01 |rsq     |standard   | 8.281876e-01|Preprocessor1_Model1 |\n|Repeat01 |Fold02 |rmse    |standard   | 2.811550e+04|Preprocessor1_Model1 |\n|Repeat01 |Fold02 |rsq     |standard   | 8.581329e-01|Preprocessor1_Model1 |\n|Repeat01 |Fold03 |rmse    |standard   | 2.930500e+04|Preprocessor1_Model1 |\n|Repeat01 |Fold03 |rsq     |standard   | 8.533908e-01|Preprocessor1_Model1 |\n\n\n:::\n:::\n\n\n\n## Combining CV Metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_metrics <- bind_cols(method = \"lm\", collect_metrics(lm_results, summarize = FALSE)) |>\n  bind_rows(bind_cols(method = \"knn5\", collect_metrics(knn5_results, summarize = FALSE))) |>\n  bind_rows(bind_cols(method = \"knn10\", collect_metrics(knn10_results, summarize = FALSE)))\n```\n:::\n\n\n\n## Visualizing CV Metrics\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_metrics |> \n  filter(.metric == \"rmse\") |> \n  ggplot(aes(x = method, y = .estimate)) +\n  geom_boxplot() +\n  geom_hline(yintercept=final_fit_perf |> filter(.metric == \"rmse\") |> pull(.estimate), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](12-preproc-missing-cv_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_metrics |> \n  filter(.metric == \"rsq\") |> \n  ggplot(aes(x = method, y = .estimate)) +\n  geom_boxplot() +\n  geom_hline(yintercept=final_fit_perf |> filter(.metric == \"rsq\") |> pull(.estimate), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](12-preproc-missing-cv_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n\n# Pre-processing\n\n\n## Data: Different Ames Housing Prices {.smaller}\n\nGoal: Predict `Sale_Price`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names <- read_rds(\"../data/AmesHousing.rds\")\names |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 881\nColumns: 20\n$ Sale_Price    <int> 244000, 213500, 185000, 394432, 190000, 149000, 149900, â€¦\n$ Gr_Liv_Area   <int> 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, â€¦\n$ Garage_Type   <fct> Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, â€¦\n$ Garage_Cars   <dbl> 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,â€¦\n$ Garage_Area   <dbl> 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7â€¦\n$ Street        <fct> Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Paâ€¦\n$ Utilities     <fct> AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, â€¦\n$ Pool_Area     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ Neighborhood  <fct> North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwestâ€¦\n$ Screen_Porch  <int> 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Overall_Qual  <fct> Good, Very_Good, Above_Average, Excellent, Above_Averageâ€¦\n$ Lot_Area      <int> 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101â€¦\n$ Lot_Frontage  <dbl> 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6â€¦\n$ MS_SubClass   <fct> One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_â€¦\n$ Misc_Val      <int> 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ Open_Porch_SF <int> 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,â€¦\n$ TotRms_AbvGrd <int> 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7â€¦\n$ First_Flr_SF  <int> 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15â€¦\n$ Second_Flr_SF <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11â€¦\n$ Year_Built    <int> 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20â€¦\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Today\n\nWell cover some common pre-processing tasks:\n\n- Dealing with zero-variance (zv) and/or near-zero variance (nzv) variables\n- Imputing missing entries\n- Label encoding ordinal categorical variables\n- Standardizing (centering and scaling) numeric predictors\n- Lumping predictors\n- One-hot/dummy encoding categorical predictor\n\n## Pre-Split Cleaning {.smaller}\n\n- Before you split your data: make sure data is in correct format\n- This may mean different things for different data sets\n- Common examples:\n    + Fixing names of columns\n    + Ensure all variable types are correct\n    + Ensure all factor levels are correct and in order (if applicable)\n    + Remove any variables that are not important (or harmful) to your analysis\n    + Ensure missing values are coded as such (i.e. as `NA` instead of 0 or -1 or \"missing\")\n    + Filling in missing values where you know what the answer should be (i.e. if a missing value really means 0 instead of missing)\n\n## Example: Factor Levels in Wrong Order\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |> pull(Overall_Qual) |> levels()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Above_Average\"  \"Average\"        \"Below_Average\"  \"Excellent\"     \n [5] \"Fair\"           \"Good\"           \"Poor\"           \"Very_Excellent\"\n [9] \"Very_Good\"      \"Very_Poor\"     \n```\n\n\n:::\n:::\n\n\n\n## Re-Factoring\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names <- ames |> \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")))\names |> pull(Overall_Qual) |> levels()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Very_Poor\"      \"Poor\"           \"Fair\"           \"Below_Average\" \n [5] \"Average\"        \"Above_Average\"  \"Good\"           \"Very_Good\"     \n [9] \"Excellent\"      \"Very_Excellent\"\n```\n\n\n:::\n:::\n\n\n\n## Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables {.smaller}\n\nHeuristic for detecting near-zero variance features is:\n\n- The fraction of unique values over the sample size is low (say â‰¤ 10%).\n- The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent\nvalue is large (say â‰¥ 20%).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nnearZeroVar(ames, saveMetrics = TRUE) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|              |  freqRatio| percentUnique|zeroVar |nzv   |\n|:-------------|----------:|-------------:|:-------|:-----|\n|Sale_Price    |   1.000000|    55.7321226|FALSE   |FALSE |\n|Gr_Liv_Area   |   1.333333|    62.9965948|FALSE   |FALSE |\n|Garage_Type   |   2.196581|     0.6810443|FALSE   |FALSE |\n|Garage_Cars   |   1.970213|     0.5675369|FALSE   |FALSE |\n|Garage_Area   |   2.250000|    38.0249716|FALSE   |FALSE |\n|Street        | 219.250000|     0.2270148|FALSE   |TRUE  |\n|Utilities     | 880.000000|     0.2270148|FALSE   |TRUE  |\n|Pool_Area     | 876.000000|     0.6810443|FALSE   |TRUE  |\n|Neighborhood  |   1.476744|     2.9511918|FALSE   |FALSE |\n|Screen_Porch  | 199.750000|     6.6969353|FALSE   |TRUE  |\n|Overall_Qual  |   1.119816|     1.1350738|FALSE   |FALSE |\n|Lot_Area      |   1.071429|    79.7956867|FALSE   |FALSE |\n|Lot_Frontage  |   1.617021|    11.5777526|FALSE   |FALSE |\n|MS_SubClass   |   1.959064|     1.7026107|FALSE   |FALSE |\n|Misc_Val      | 141.833333|     1.9296254|FALSE   |TRUE  |\n|Open_Porch_SF |  23.176471|    19.2962543|FALSE   |FALSE |\n|TotRms_AbvGrd |   1.311225|     1.2485812|FALSE   |FALSE |\n|First_Flr_SF  |   1.777778|    63.7911464|FALSE   |FALSE |\n|Second_Flr_SF |  64.250000|    31.3280363|FALSE   |FALSE |\n|Year_Built    |   1.175000|    11.9182747|FALSE   |FALSE |\n\n\n:::\n:::\n\n\n\n## Recipe: Near-Zero Variance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- recipe(Sale_Price ~ ., data = ames) |> \n  step_nzv(all_predictors()) # remove zero or near-zero variable predictors\n```\n:::\n\n\n\n## Missing Data {.smaller}\n\n-   Many times, you can't just drop missing data\n-   Even if you can, dropping missing values can generate *biased* data/models\n-   Sometimes missing data gives you more information\n-   Types of missing data:\n    +   Missing completely at random (MCAR): there is no pattern to your missing values\n    +   Missing at random (MAR): missing values are dependent on other values in the data set\n    +   Missing not at random (MNAR): missing values are dependent on the value that is missing\n-   Structured missingness (SM):  when the missingness of certain values are depends on one another, regardless of whether the missing values are MCAR, MAR, or MNAR\n\n## MCAR: Examples\n\n-   Sensor data: occasionally sensors break so you're missing data randomly\n-   Survey data: sometimes people just randomly skip questions\n-   Survey data: customers are randomly given 5 questions from a bank of 100 questions\n\n## MAR: Examples\n\n-   Men are less likely to respond to surveys about depression\n-   Medical study: patients who miss follow-up appointments are more likely to be young\n-   Survey responses: ESL respondents may be more likely to skip certain questions that are difficult to interpret (only MAR if you know they are ESL)\n-   Measure of student performance: students who score lower are more likely to skip questions\n\n## MNAR: Examples\n\n-   Survey on income: respondent may be less likely to report their income if they are poor\n-   Survey about political beliefs: respondent may be more likely to skip questions when their answer is perceived as undesirable\n-   Customer satisfaction: only customers who feel strongly respond\n-   Medical study: patients refuse to report unhealthy habits\n\n## Structurally Missing: Examples\n\n-   Health survey: all questions related to pregnancy are left blank by males\n-   Bank data set: combination of home, auto, and credit cards... not all customer have all three so have missing data in certain portions\n-   Survey: many respondents by stop the survey early so all questions after a certain point are missing\n-   Netflix: customers may only watch similar movies and TV shows\n\n## Remedies for Missing Data\n\n-   Lot of complicated ways that you can read about\n-   Can drop column of too much of the data is missing\n-   Imputing:\n    +  `step_impute_median`: used for numeric (especially discrete) variables\n    +  `step_impute_mean`: used for numeric variables\n    +  `step_impute_knn`: used for both numeric and categorical variables (computationally expensive)\n    +  `step_impute_mode`: used for nominal (having no order) categorical variable\n    \n## Exploring Missing Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |> \n  summarize(across(everything(), ~ sum(is.na(.)))) |> \n  pivot_longer(everything()) |> \n  filter(value > 0) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|name        | value|\n|:-----------|-----:|\n|Gr_Liv_Area |   113|\n|Garage_Type |    54|\n|Year_Built  |    41|\n\n\n:::\n:::\n\n\n\n## Missing Data: Garage_Type\n\n-   The reason that `Garage_Type` is missing is because there is no basement\n    +   Solution: replace `NA`s with `No_Garage`\n    +   Do this before data splitting\n    \n## Fixing Garage_Type\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names <- ames |> \n  mutate(Garage_Type = as_factor(if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type)))\n```\n:::\n\n\n    \n## Missing Data: Year_Built\n\n-   MCAR\n    +   Solution 1: Impute with mean or median\n    +   Solution 2: Impute with KNN... maybe we can infer what the values are based on other values in the data set?\n    \n## Missing Data: Gr_Liv_Area\n\n-   MCAR\n    +   Solution 1: Impute with mean or median\n    +   Solution 2: Impute with KNN... maybe we can infer what the values are based on other values in the data set?\n  \n## Recipe: Missing Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- recipe(Sale_Price ~ ., data = ames) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) # impute missing values in Overall_Qual and Year_Built\n```\n:::\n\n\n\n-   Note: `step_imput_knn` uses the \"Gower's Distance\" so don't need to worry about normalizing\n\n## Encoding Ordinal Features\n\nTwo types of categorical features:\n\n-   Ordinal (order is important)\n-   Nominal (order is not important)\n\n## Encoding Ordinal Features\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |> pull(Overall_Qual) |> levels()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Very_Poor\"      \"Poor\"           \"Fair\"           \"Below_Average\" \n [5] \"Average\"        \"Above_Average\"  \"Good\"           \"Very_Good\"     \n [9] \"Excellent\"      \"Very_Excellent\"\n```\n\n\n:::\n:::\n\n\n- `Very_Poor` = 1, `Poor` = 2, `Fair` = 3, etc...\n\n## Recipe: Encoding Ordinal Features\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- recipe(Sale_Price ~ ., data = ames) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) # convert Overall_Qual into ordinal encoding\n```\n:::\n\n\n\n## Lump Small Categories Together\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |> count(Neighborhood) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Neighborhood                            |   n|\n|:---------------------------------------|---:|\n|North_Ames                              | 127|\n|College_Creek                           |  86|\n|Old_Town                                |  83|\n|Edwards                                 |  49|\n|Somerset                                |  50|\n|Northridge_Heights                      |  52|\n|Gilbert                                 |  47|\n|Sawyer                                  |  49|\n|Northwest_Ames                          |  41|\n|Sawyer_West                             |  31|\n|Mitchell                                |  33|\n|Brookside                               |  33|\n|Crawford                                |  22|\n|Iowa_DOT_and_Rail_Road                  |  28|\n|Timberland                              |  21|\n|Northridge                              |  22|\n|Stone_Brook                             |  17|\n|South_and_West_of_Iowa_State_University |  21|\n|Clear_Creek                             |  16|\n|Meadow_Village                          |  14|\n|Briardale                               |  10|\n|Bloomington_Heights                     |  10|\n|Veenker                                 |   9|\n|Northpark_Villa                         |   3|\n|Blueste                                 |   3|\n|Greens                                  |   4|\n\n\n:::\n:::\n\n\n\n## Lump Small Categories Together\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |> mutate(Neighborhood = fct_lump_prop(Neighborhood, 0.05)) |> \n  count(Neighborhood) |>  kable()\n```\n\n::: {.cell-output-display}\n\n\n|Neighborhood       |   n|\n|:------------------|---:|\n|North_Ames         | 127|\n|College_Creek      |  86|\n|Old_Town           |  83|\n|Edwards            |  49|\n|Somerset           |  50|\n|Northridge_Heights |  52|\n|Gilbert            |  47|\n|Sawyer             |  49|\n|Other              | 338|\n\n\n:::\n:::\n\n\n\n## Recipe: Lumping Small Factors Together\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- recipe(Sale_Price ~ ., data = ames) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") # lump all categories with less than 1% representation into a category called Other for each variable\n```\n:::\n\n\n\n## One-hot/dummy encoding categorical predictors\n\n![Figure 3.9: [Machine Learning with R](https://bradleyboehmke.github.io/HOML/engineering.html)](images/12/ohe-vs-dummy.png)\n\n## Recipe: Dummy Variables\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- recipe(Sale_Price ~ ., data = ames) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)  # in general use one_hot unless doing linear regression\n```\n:::\n\n\n\n## Receipe: Center and scale\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- recipe(Sale_Price ~ ., data = ames) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(Neighborhood, threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # in general use one_hot unless doing linear regression\n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Order of Preprocessing Step {.smaller}\n\nQuestions to ask:\n\n1. Should this be done before or after data splitting?\n2. If I do step_A first what is the impact on step_B? For example, do you want to encode categorical variables before or after normalizing?\n3. What data format is required by the model I'm fitting and how will my model react to these changes?\n4. Is this step part of my \"model\"? I.e. is this a decision I'm making based on the data or based on subject matter expertise?\n5. Do I have access to my test predictors?\n\n## Questions\n\n-   Should I lump before or after dummy coding?\n-   Should I dummy code before or after normalizing?\n-   Should I lump before my initial split?\n-   How does ordinal encoding impact linear regression vs. KNN?\n\n# Final R Workflow\n\n## Clean Data Set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names <- ames |> \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")),\n         Garage_Type = if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type),\n         Garage_Type = as_factor(Garage_Type)\n         )\n```\n:::\n\n\n\n## Initial Data Split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\ndata_split <- initial_split(ames, strata = \"Sale_Price\")\names_train <- training(data_split)\names_test  <- testing(data_split)\n```\n:::\n\n\n\n## Define Folds\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_folds <- vfold_cv(ames_train, v = 10, repeats = 10)\names_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation repeated 10 times \n# A tibble: 100 Ã— 3\n   splits           id       id2   \n   <list>           <chr>    <chr> \n 1 <split [594/66]> Repeat01 Fold01\n 2 <split [594/66]> Repeat01 Fold02\n 3 <split [594/66]> Repeat01 Fold03\n 4 <split [594/66]> Repeat01 Fold04\n 5 <split [594/66]> Repeat01 Fold05\n 6 <split [594/66]> Repeat01 Fold06\n 7 <split [594/66]> Repeat01 Fold07\n 8 <split [594/66]> Repeat01 Fold08\n 9 <split [594/66]> Repeat01 Fold09\n10 <split [594/66]> Repeat01 Fold10\n# â„¹ 90 more rows\n```\n\n\n:::\n:::\n\n\n\n## Define Model(s)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model <- linear_reg() |>\n  set_engine('lm')\n\nknn5_model <- nearest_neighbor(neighbors = 5) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"regression\")\n\nknn10_model <- nearest_neighbor(neighbors = 10) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"regression\")\n```\n:::\n\n\n\n## Define Preprocessing: Linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preproc <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |> # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations\n```\n:::\n\n\n\n## Define Preprocessing: KNN\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_preproc <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Define Workflows\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_wf <- workflow() |> add_model(lm_model) |> add_recipe(lm_preproc)\nknn5_wf <- workflow() |> add_model(knn5_model) |> add_recipe(knn_preproc)\nknn10_wf <- workflow() |> add_model(knn10_model) |> add_recipe(knn_preproc)\n```\n:::\n\n\n\n## Define Metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_metrics <- metric_set(rmse, rsq)\n```\n:::\n\n\n\n## Fit and Assess Models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_results <- lm_wf |> fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn5_results <- knn5_wf |> fit_resamples(resamples = ames_folds, metrics = ames_metrics)\nknn10_results <- knn10_wf |> fit_resamples(resamples = ames_folds, metrics = ames_metrics)\n```\n:::\n\n\n\n\n## Collecting Metrics {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(lm_results) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |         mean|   n|     std_err|.config              |\n|:-------|:----------|------------:|---:|-----------:|:--------------------|\n|rmse    |standard   | 3.968417e+04| 100| 764.5524121|Preprocessor1_Model1 |\n|rsq     |standard   | 7.559368e-01| 100|   0.0088924|Preprocessor1_Model1 |\n\n\n:::\n\n```{.r .cell-code}\ncollect_metrics(knn5_results) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |         mean|   n|     std_err|.config              |\n|:-------|:----------|------------:|---:|-----------:|:--------------------|\n|rmse    |standard   | 3.999696e+04| 100| 956.4824660|Preprocessor1_Model1 |\n|rsq     |standard   | 7.596873e-01| 100|   0.0062576|Preprocessor1_Model1 |\n\n\n:::\n\n```{.r .cell-code}\ncollect_metrics(knn10_results) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |         mean|   n|      std_err|.config              |\n|:-------|:----------|------------:|---:|------------:|:--------------------|\n|rmse    |standard   | 3.952087e+04| 100| 1008.0467220|Preprocessor1_Model1 |\n|rsq     |standard   | 7.681888e-01| 100|    0.0065365|Preprocessor1_Model1 |\n\n\n:::\n:::\n\n\n\n## Final & Evaluate Final Model\n\n- After choosing best model/workflow, fit on full training set and assess on test set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- knn10_wf |> fit(data = ames_train)\nfinal_fit_perf <- final_fit |> \n  predict(new_data = ames_test) |> \n  bind_cols(ames_test) |> \n  ames_metrics(truth = Sale_Price, estimate = .pred)\n\nfinal_fit_perf |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |    .estimate|\n|:-------|:----------|------------:|\n|rmse    |standard   | 4.941178e+04|\n|rsq     |standard   | 7.181459e-01|\n\n\n:::\n:::\n\n\n\n\n## Tips\n\n-   Can try out different pre-processing to see if it improves your model!\n-   Process can be intense for you computer, so might take a while\n-   No 100% correct way to do it, although there are some 100% incorrect ways to do it\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}