{
  "hash": "b25215de3f84ddd2423eec96d13da5ce",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Regularization & Model Tuning'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(fivethirtyeight) # for candy rankings data\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n## Data: Candy {.smaller}\n\nThe data for this lecture comes from the article FiveThirtyEight [*The Ultimate Halloween Candy Power Ranking*](https://fivethirtyeight.com/features/the-ultimate-halloween-candy-power-ranking) by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy match-ups (e.g. Reese's vs. Skittles). Click [here](http://walthickey.com/2017/10/18/whats-the-best-halloween-candy/) to check out some of the match ups.\n\nThe data set contains 12 characteristics and win percentage from 85 candies in the experiment.\n\n## Data: Candy\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 13\n$ competitorname   <chr> \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarterâ€¦\n$ chocolate        <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31â€¦\n$ pricepercent     <dbl> 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n## Data Cleaning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_rankings_clean <- candy_rankings |> \n  select(-competitorname) |> \n  mutate(sugarpercent = sugarpercent*100, # convert proportions into percentages\n         pricepercent = pricepercent*100, # convert proportions into percentages\n         across(where(is.logical), ~ factor(.x, levels = c(\"FALSE\", \"TRUE\")))) # convert logicals into factors\n```\n:::\n\n\n\n## Data Cleaning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 12\n$ chocolate        <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <fct> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <fct> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <fct> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 73.2, 60.4, 1.1, 1.1, 90.6, 46.5, 60.4, 31.3, 90.6, 6â€¦\n$ pricepercent     <dbl> 86.0, 51.1, 11.6, 51.1, 51.1, 76.7, 76.7, 51.1, 32.5,â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n## Data Splitting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_split <- initial_split(candy_rankings_clean, strata = winpercent)\ncandy_train <- training(candy_split)\ncandy_test <- testing(candy_split)\n```\n:::\n\n\n\n# Model Tuning\n\n## Tuning Parameters\n\n-   **Tuning Parameters** or **Hyperparameters** are parameters that cannot (or should not) be estimated when a model is being trained\n-   These parameters control something about the learning process and changing them will result in a different model when fit to the full training data (i.e. after cross-validation)\n-   Frequently: tuning parameters control model complexity\n-   Example: $\\lambda$ in LASSO and Ridge regression\n-   Today: How to choose our tuning parameters?\n\n## Question\n\n-   Which of the following are tuning parameters:\n    +   $\\beta_0$: the intercept of linear regression\n    +   $k$ in KNN\n    +   step size in gradient descent\n    +   The number of folds in cross-validation\n    +   Type of distance to use in KNN (i.e. rectangular vs. Gower's vs. weighted etc)\n\n## Basic Idea\n\n-   Use CV to try out a bunch of different tuning parameters and choose the \"best\" one\n-   How do we choose which tuning parameters to try?\n-   Two general approaches:\n    +   Grid Search\n    +   Iterative Search\n    \n# Grid Search\n    \n## Grid Search\n\n-   Create a grid of tuning parameters and try out each combination\n-   Types of grids:\n    +   Regular Grid: tuning parameter values are spaced *deterministically* using a linear or logarithmic scale and all combinations of parameters are used (mostly what you want to use in this class)\n    +   Irregular Grids: tuning parameter values are chosen *stochastically*\n        -   Use when you have A LOT of parameters\n    \n## Grid Search in R\n\n-   Take advantage of package `dials` which is part of the `tidyverse`\n    +   Set every tuning variable equal to `tune()`\n    \n## LASSO and Ridge in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols <- linear_reg() |> \n  set_engine(\"lm\")\n\nridge <- linear_reg(mixture = 0, penalty = tune()) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\n\nlasso <- linear_reg(mixture = 1, penalty = tune()) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\n```\n:::\n\n\n    \n## Create Recipe\n\n-   Note: no tuning variables in this case\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preproc <- recipe(winpercent ~ . , data = candy_train) |> \n  step_dummy(all_nominal_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Create workflows\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeneric_wf <- workflow() |> add_recipe(lm_preproc)\nols_wf <- generic_wf |> add_model(ols)\nridge_wf <- generic_wf |> add_model(ridge)\nlasso_wf <- generic_wf |> add_model(lasso)\n```\n:::\n\n\n\n## Create Metric Set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_metrics <- metric_set(rmse, rsq)\n```\n:::\n\n\n\n## Create Folds\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Since sample size is so small, not using stratification\ncandy_folds <- vfold_cv(candy_train, v = 2, repeats = 10)\n```\n:::\n\n\n\n## Grid Search in R\n\n-   Take advantage of package `dials` which is part of the `tidyverse`\n    +   Set every tuning variable equal to `tune()`\n    +   Generate grid for hyperparameters\n\n## Generate Grid\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# note that dials treats penalty on a log10 scale\npenalty_grid <- grid_regular(penalty(range = c(-10, 2)), # I had to play around with these \n                             levels = 10)\n```\n:::\n\n\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenalty_grid |> head() |>  kable(digits = 11, format.args = list(scientific = TRUE))\n```\n\n::: {.cell-output-display}\n\n\n|      penalty|\n|------------:|\n| 1.000000e-10|\n| 2.150000e-09|\n| 4.642000e-08|\n| 1.000000e-06|\n| 2.154435e-05|\n| 4.641589e-04|\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenalty_grid |> tail() |>  kable()\n```\n\n::: {.cell-output-display}\n\n\n|     penalty|\n|-----------:|\n|   0.0000215|\n|   0.0004642|\n|   0.0100000|\n|   0.2154435|\n|   4.6415888|\n| 100.0000000|\n\n\n:::\n:::\n\n\n:::\n::::\n\n\n## Regular Grid Search in R\n\n-   Take advantage of package `dials` which is part of the `tidyverse`\n    +   Set every tuning variable equal to `tune()`\n    +   Generate grid for hyperparameters using `grid_regular`\n    +   Tune your model: fit all hyperparameter combination on resamples using `tune_grid`\n\n## Tune Models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuning_ridge_results <- tune_grid(\n  ridge_wf,\n  resamples= candy_folds,\n  grid = penalty_grid\n)\n\ntuning_lasso_results <- tune_grid(\n  lasso_wf,\n  resamples = candy_folds,\n  grid = penalty_grid\n)\n```\n:::\n\n\n\n## Regular Grid Search in R\n\n-   Take advantage of package `dials` which is part of the `tidyverse`\n    +   Set every tuning variable equal to `tune()`\n    +   Generate grid for hyperparameters using `grid_regular`\n    +   Tune your model: fit all hyperparameter combination on resamples using `tune_grid`\n    +   Visualize and Choose final model\n\n## Visualizing Results: Ridge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_ridge_results)\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n## Selecting Best Model: Ridge\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rmse_ridge <- tuning_ridge_results |> \n  select_best(metric = \"rmse\")\nbest_rmse_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|    4.64|Preprocessor1_Model09 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rsq_ridge <- tuning_ridge_results |> \n  select_best(metric = \"rsq\",)\nbest_rsq_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|     100|Preprocessor1_Model10 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_rmse_final <- finalize_workflow(ridge_wf, best_rmse_ridge)\nridge_rmse_fit <- fit(ridge_rmse_final, data = candy_train)\ntidy(ridge_rmse_fit) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term                   |   estimate|  penalty|\n|:----------------------|----------:|--------:|\n|(Intercept)            | 49.8221817| 4.641589|\n|sugarpercent           |  1.6105235| 4.641589|\n|pricepercent           | -0.8755351| 4.641589|\n|chocolate_TRUE.        |  5.5626786| 4.641589|\n|fruity_TRUE.           |  0.8290067| 4.641589|\n|caramel_TRUE.          |  0.9577679| 4.641589|\n|peanutyalmondy_TRUE.   |  2.8412296| 4.641589|\n|nougat_TRUE.           |  0.5018516| 4.641589|\n|crispedricewafer_TRUE. |  1.6202899| 4.641589|\n|hard_TRUE.             | -1.2156466| 4.641589|\n|bar_TRUE.              |  1.4669984| 4.641589|\n|pluribus_TRUE.         |  0.4265386| 4.641589|\n\n\n:::\n:::\n\n\n\n## Visualizing Results: LASSO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_lasso_results)\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n## Selecting Best Model: LASSO\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rmse_lasso <- tuning_lasso_results |> \n  select_best(metric = \"rmse\")\nbest_rmse_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|    4.64|Preprocessor1_Model09 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rsq_lasso <- tuning_lasso_results |> \n  select_best(metric = \"rsq\")\nbest_rsq_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|    4.64|Preprocessor1_Model09 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_rmse_final <- finalize_workflow(lasso_wf, best_rmse_lasso)\nlasso_rmse_fit <- fit(lasso_rmse_final, data = candy_train)\ntidy(lasso_rmse_fit) |> \n  filter(estimate != 0) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term            |  estimate|  penalty|\n|:---------------|---------:|--------:|\n|(Intercept)     | 49.822182| 4.641589|\n|chocolate_TRUE. |  5.165031| 4.641589|\n\n\n:::\n:::\n\n\n\n## Using Parsimony as a Tie-Breaker\n\n-   Good heuristic: **One-Standard Error Rule**\n    +   Use resampling to estimate error metrics\n    +   Compute standard error for error metrics\n    +   Select most parsimonious model that is within one standard error of the best performance metric\n    \n\n\n## Selecting Best Model: Ridge\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_ose_rmse_ridge <- tuning_ridge_results |> \n  select_by_one_std_err(metric = \"rmse\", desc(penalty)) # why are we descending\nbest_ose_rmse_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|    4.64|Preprocessor1_Model09 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbse_ose_rsq_ridge <- tuning_ridge_results |> \n  select_by_one_std_err(metric = \"rsq\", desc(penalty))\nbse_ose_rsq_ridge |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|     100|Preprocessor1_Model10 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Visualizing Results: Ridge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_ridge_results)\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_rmse_final <- finalize_workflow(ridge_wf, best_ose_rmse_ridge)\nridge_rmse_fit <- fit(ridge_rmse_final, data = candy_train)\ntidy(ridge_rmse_fit) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term                   |   estimate|  penalty|\n|:----------------------|----------:|--------:|\n|(Intercept)            | 49.8221817| 4.641589|\n|sugarpercent           |  1.6105235| 4.641589|\n|pricepercent           | -0.8755351| 4.641589|\n|chocolate_TRUE.        |  5.5626786| 4.641589|\n|fruity_TRUE.           |  0.8290067| 4.641589|\n|caramel_TRUE.          |  0.9577679| 4.641589|\n|peanutyalmondy_TRUE.   |  2.8412296| 4.641589|\n|nougat_TRUE.           |  0.5018516| 4.641589|\n|crispedricewafer_TRUE. |  1.6202899| 4.641589|\n|hard_TRUE.             | -1.2156466| 4.641589|\n|bar_TRUE.              |  1.4669984| 4.641589|\n|pluribus_TRUE.         |  0.4265386| 4.641589|\n\n\n:::\n:::\n\n\n\n## Visualizing Results: LASSO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_lasso_results)\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n\n## Selecting Best Model: Ridge\n\n:::: columns\n::: column\n\nBest RMSE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbse_ose_rmse_lasso <- tuning_lasso_results |> \n  select_by_one_std_err(metric = \"rmse\", desc(penalty)) # why are we descending\nbse_ose_rmse_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|    4.64|Preprocessor1_Model09 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\nBest $R^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbse_ose_rsq_lasso <- tuning_lasso_results |> \n  select_by_one_std_err(metric = \"rsq\", desc(penalty)) # why are we descending\nbse_ose_rsq_lasso |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| penalty|.config               |\n|-------:|:---------------------|\n|    4.64|Preprocessor1_Model09 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n-   Which should be use?\n\n## Finalize Model {.smaller}\n\n-   RMSE estimate is less flexible and seems to be sacrificing less $R^2$ than the opposite\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_rmse_final <- finalize_workflow(lasso_wf, bse_ose_rmse_lasso)\nlasso_rmse_fit <- fit(lasso_rmse_final, data = candy_train)\ntidy(lasso_rmse_fit) |> \n  filter(estimate != 0) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term            |  estimate|  penalty|\n|:---------------|---------:|--------:|\n|(Intercept)     | 49.822182| 4.641589|\n|chocolate_TRUE. |  5.165031| 4.641589|\n\n\n:::\n:::\n\n\n\n## Using the Test Set as a Tie Breaker\n\n-   Once you've found you \"best\" candidate from several different classes of model, it's ok to compare on test set\n-   In this case, we have our best ridge and our best lasso model\n-   Main this to avoid... LOTS of comparisons on your test set\n\n## Using the Test Set as a Tie Breaker {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_test_wpreds <- candy_test |> \n  mutate(ridge_preds = predict(ridge_rmse_fit, new_data = candy_test)$.pred,\n         lasso_preds = predict(lasso_rmse_fit, new_data = candy_test)$.pred)\ncandy_test_wpreds |> rmse(estimate = ridge_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        10.6\n```\n\n\n:::\n\n```{.r .cell-code}\ncandy_test_wpreds |> rmse(estimate = lasso_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        12.0\n```\n\n\n:::\n\n```{.r .cell-code}\ncandy_test_wpreds |> rsq(estimate = ridge_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.476\n```\n\n\n:::\n\n```{.r .cell-code}\ncandy_test_wpreds |> rsq(estimate = lasso_preds, truth = winpercent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.341\n```\n\n\n:::\n:::\n\n\n\n-   Ridge Wins!\n\n## Good strategies\n\n-   First use coarse grid with large range to find general area\n    +   Then use fine grid with small range to fine tune\n    +   Use iterative method to fine tune\n    \n# Iterative Methods (Bonus Content)\n\n## Brief Overview\n\n-   Basic Idea: iterative select parameter values to choose based on how previous ones have done\n-   TMWR gives two examples:\n    +   Bayesian search\n    +   Stochastic Annealing\n-   Both require an understanding of multivariate probability distributions\n\n## Bayesian Search: GP {.smaller}\n\n::: incremental\n-   Assume performance metrics follow a Gaussian process... [dafuq]{.fragment}\n-   Consider two sets of tuning parameters: $x_1 = (\\lambda_1, k_1)$ and $x_2 = (\\lambda_2, k_2)$\n-   Let $Y_1$ and $Y_2$ be random variables representing the performance metric at these $x$'s\n-   Assume $\\text{Cov}(Y_1, Y_2)$ depends on how far apart $x_1$ and $x_2$\n    +   Covariance should decrease the further apart $x_1$ and $x_2$ are\n    +   This covariance function is typically parameterized and estimated from an initial tuning grid\n:::\n\n## Bayesian Search: Acquisition Function {.smaller}\n\n::: incremental\n-   GP process gives us predicted mean and variance of performance metrics\n-   Imagine two scenario's:\n    +   1: Predicted mean is slightly better than current parameter choice and variance is low\n        -   Low-Risk, Low-Reward\n    +   2: Predicted mean is slightly worst than current parameter choice but variance is high\n        -   High-Risk, High-Reward\n-   How do we balance?\n-   Two competing goals: *Exploration* (got toward high variance) and *Exploitation* (go toward best mean)\n-   Acquisition function: balances these two goals (several to choose from)\n-   [tmwr](https://www.tmwr.org/iterative-search)\n:::\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_params <- ridge_wf |> \n  extract_parameter_set_dials() |> \n  update(penalty = penalty(c(-2, 1)))\n\nbayes_ridge <- ridge_wf |> \n  tune_bayes(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metrics is what's optimized (rmse in this case)\n    initial = tuning_ridge_results,\n    param_info = ridge_params,\n    iter = 25\n  )\n```\n:::\n\n\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bayes_ridge, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bayes_ridge, type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(bayes_ridge) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|  penalty|.metric |.estimator |     mean|  n|   std_err|.config | .iter|\n|--------:|:-------|:----------|--------:|--:|---------:|:-------|-----:|\n| 9.999186|rmse    |standard   | 13.20055| 20| 0.2720582|Iter11  |    11|\n| 9.999034|rmse    |standard   | 13.20055| 20| 0.2720587|Iter8   |     8|\n| 9.996720|rmse    |standard   | 13.20057| 20| 0.2720676|Iter7   |     7|\n| 9.996350|rmse    |standard   | 13.20057| 20| 0.2720690|Iter9   |     9|\n| 9.995326|rmse    |standard   | 13.20058| 20| 0.2720729|Iter10  |    10|\n\n\n:::\n:::\n\n\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_params <- lasso_wf |> \n  extract_parameter_set_dials() |> \n  update(penalty = penalty(c(-2,1)))\n\nbayes_lasso <- lasso_wf |> \n  tune_bayes(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metrics is what's optimized (rmse in this case)\n    initial = tuning_lasso_results,\n    param_info = lasso_params,\n    iter = 25\n  )\n```\n:::\n\n\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bayes_lasso, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bayes_lasso, type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n\n\n## Bayesian Search: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(bayes_lasso) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|  penalty|.metric |.estimator |     mean|  n|   std_err|.config | .iter|\n|--------:|:-------|:----------|--------:|--:|---------:|:-------|-----:|\n| 2.936401|rmse    |standard   | 12.76520| 20| 0.3059454|Iter16  |    16|\n| 2.937813|rmse    |standard   | 12.76520| 20| 0.3059406|Iter20  |    20|\n| 2.934824|rmse    |standard   | 12.76520| 20| 0.3059506|Iter19  |    19|\n| 2.940832|rmse    |standard   | 12.76521| 20| 0.3059305|Iter25  |    25|\n| 2.932606|rmse    |standard   | 12.76521| 20| 0.3059580|Iter22  |    22|\n\n\n:::\n:::\n\n\n\n## Simulated Annealing {.smaller}\n\n::: incremental\n-   Does anyone know what annealing is in Physics/Material Science?\n-   Start with initial parameter combination $x_0$(think gradient descent)\n-   Embark on random walk... in each step $i$\n    +   Consider small perturbation from $x_{i-1}$... let's call it $x_{i-1}^{\\epsilon}$\n    +   Get performance for $x_{i-1}^\\epsilon$\n    +   If performance is better set $x_i = x_{i-1}^\\epsilon$\n    +   If performance is worse set $x_i = x_{i-1}^\\epsilon$ with probability $\\exp(-c\\times D_i\\times i)$\n        -   $c$ is user specified constant called cooling coefficient\n        -   $D_i$ percent difference between old and new\n    +   Otherwise $x_i = x_{i-1}$\n-   Idea: \n    +   start hot: likely to accept suboptimal parameter combinations and move around\n    +   cools over time: decrease number sub-optimal acceptances\n:::\n\n## Simulated Annealing: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(finetune)\nridge_sim_anneal <- ridge_wf |> \n  tune_sim_anneal(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metric is what's optimized (rmse in this case)\n    initial = tuning_ridge_results,\n    param_info = ridge_params,\n    iter = 50\n  )\n```\n:::\n\n\n\n## Simulated Annealing `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(ridge_sim_anneal, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-41-1.png){width=960}\n:::\n:::\n\n\n\n## Simulated Annealing `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(ridge_sim_anneal, type = \"parameters\")\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-42-1.png){width=960}\n:::\n:::\n\n\n\n## Simulated Annealing `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(ridge_sim_anneal) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|   penalty|.metric |.estimator |     mean|  n|   std_err|.config | .iter|\n|---------:|:-------|:----------|--------:|--:|---------:|:-------|-----:|\n| 10.000000|rmse    |standard   | 13.20054| 20| 0.2720551|Iter19  |    19|\n|  9.513817|rmse    |standard   | 13.20542| 20| 0.2739433|Iter42  |    42|\n|  9.079220|rmse    |standard   | 13.21026| 20| 0.2758191|Iter31  |    31|\n|  8.818478|rmse    |standard   | 13.21363| 20| 0.2770621|Iter9   |     9|\n|  8.415657|rmse    |standard   | 13.21916| 20| 0.2790967|Iter45  |    45|\n\n\n:::\n:::\n\n\n\n## Simulated Annealing: `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_sim_anneal <- ridge_wf |> \n  tune_sim_anneal(\n    resamples = candy_folds,\n    metrics = candy_metrics, # first metrics is what's optimized (rmse in this case)\n    initial = tuning_lasso_results,\n    param_info = lasso_params,\n    iter = 50\n  )\n```\n:::\n\n\n\n## Simulated Annealing `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lasso_sim_anneal, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-45-1.png){width=960}\n:::\n:::\n\n\n\n## Simulated Annealing `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lasso_sim_anneal, type = \"parameters\") +\n  scale_x_continuous(trans = 'log10')\n```\n\n::: {.cell-output-display}\n![](18-regularization-tuning_files/figure-revealjs/unnamed-chunk-46-1.png){width=960}\n:::\n:::\n\n\n\n## Simulated Annealing `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(lasso_sim_anneal) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|  penalty|.metric |.estimator |     mean|  n|   std_err|.config                       | .iter|\n|--------:|:-------|:----------|--------:|--:|---------:|:-----------------------------|-----:|\n| 4.641589|rmse    |standard   | 13.09389| 20| 0.3774972|initial_Preprocessor1_Model09 |     0|\n| 9.934398|rmse    |standard   | 13.20114| 20| 0.2723052|Iter39                        |    39|\n| 9.189444|rmse    |standard   | 13.20896| 20| 0.2752983|Iter25                        |    25|\n| 8.786187|rmse    |standard   | 13.21407| 20| 0.2772191|Iter37                        |    37|\n| 8.333199|rmse    |standard   | 13.22037| 20| 0.2795509|Iter26                        |    26|\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}