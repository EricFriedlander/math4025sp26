{
  "hash": "cf26c34147ea0356a9ca41bc94af69d6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Regularization & Model Tuning'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n\n## Computational Set-Up\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(fivethirtyeight) # for candy rankings data\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n\n## Data: Candy {.smaller}\n\nThe data for this lecture comes from the article FiveThirtyEight [*The Ultimate Halloween Candy Power Ranking*](https://fivethirtyeight.com/features/the-ultimate-halloween-candy-power-ranking) by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy match-ups (e.g. Reese's vs. Skittles). Click [here](http://walthickey.com/2017/10/18/whats-the-best-halloween-candy/) to check out some of the match ups.\n\nThe data set contains 12 characteristics and win percentage from 85 candies in the experiment.\n\n## Data: Candy\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 13\n$ competitorname   <chr> \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarterâ€¦\n$ chocolate        <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31â€¦\n$ pricepercent     <dbl> 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n\n## Data Cleaning\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_rankings_clean <- candy_rankings |> \n  select(-competitorname) |> \n  mutate(sugarpercent = sugarpercent*100, # convert proportions into percentages\n         pricepercent = pricepercent*100, # convert proportions into percentages\n         across(where(is.logical), ~ factor(.x, levels = c(\"FALSE\", \"TRUE\")))) # convert logicals into factors\n```\n:::\n\n\n\n\n## Data Cleaning\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(candy_rankings_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 85\nColumns: 12\n$ chocolate        <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ fruity           <fct> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSEâ€¦\n$ caramel          <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ peanutyalmondy   <fct> FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, â€¦\n$ nougat           <fct> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,â€¦\n$ crispedricewafer <fct> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSEâ€¦\n$ hard             <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSâ€¦\n$ bar              <fct> TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, Fâ€¦\n$ pluribus         <fct> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUEâ€¦\n$ sugarpercent     <dbl> 73.2, 60.4, 1.1, 1.1, 90.6, 46.5, 60.4, 31.3, 90.6, 6â€¦\n$ pricepercent     <dbl> 86.0, 51.1, 11.6, 51.1, 51.1, 76.7, 76.7, 51.1, 32.5,â€¦\n$ winpercent       <dbl> 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.â€¦\n```\n\n\n:::\n:::\n\n\n\n\n## Data Splitting\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncandy_split <- initial_split(candy_rankings_clean, strata = winpercent)\ncandy_train <- training(candy_split)\ncandy_test <- testing(candy_split)\n```\n:::\n\n\n\n\n# Lasso & Ridge Regression\n\n## Question\n\n-   What criteria do we use to fit a linear regression model? Write down an expression with $\\hat{\\beta_i}$'s, $x_{ij}$'s, and $y_j$'s in it.\n\n## OLS {.smaller}\n\n-   Ordinary Least Squares Regression:\n\n$$\n\\begin{aligned}\n\\hat{\\beta} =\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}SSE(\\hat{\\beta}) &= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{y}_j)^2\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2\n\\end{aligned}\n$$\n\n-   $\\hat{\\beta} = (\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p)$ is the vector of all my coefficients\n-   $\\operatorname{argmin}$ is a function (operator) that returns the *arguments* that minimize the quantity it's being applied to\n\n## Ridge Regression {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_2^2\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\right)\n\\end{aligned}\n$$\n\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients? [[THEY SHRINK!]{.span style=\"color:red;\"}]{.fragment .fade-in}\n-   $\\|\\cdot\\|_2$ is called the *$L_2$-norm*\n\n## But... but... but why?!? {.smaller}\n\n::: incremental\n-   Recall the Bias-Variance Trade-Off\n-   Our reducible error can partitioned into:\n    +   Bias: how much $\\hat{f}$ misses $f$ by *on average*\n    +   Variance: how much $\\hat{f}$ moves around from sample to sample\n-   Ridge: increase bias a little bit in exchange for large decrease in variance\n-   As we increase $\\lambda$ do we increase or decrease the penalty for large coefficients?\n-   As we increase $\\lambda$ do we increase or decrease the *flexibility* of our model?\n:::\n\n## LASSO {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_1\\right) \\\\\n&= \\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\\\\\n&=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}\\left(\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\right)\n\\end{aligned}\n$$\n\n-   LASSO: **L**east **A**bsolute **S**hrinkage and **S**election **O**perator\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients [THEY SHRINK!]{.span style=\"color:red;\"}\n-   $\\|\\cdot\\|_1$ is called the *$L_1$-norm*\n\n## Question\n\n-   What should happen to our coefficients as we *increase* $\\lambda$?\n\n## Ridge vs. LASSO\n\n::: incremental\n-   Ridge has a *closed-form solution*... how might we calculate it?\n-   Ridge has some nice linear algebra properties that makes it EXTREMELY FAST to fit\n-   LASSO has no *closed-form solution*... why?\n-   LASSO coefficients estimated *numerically*... how?\n    +   Gradient descent works (kind of) but something called **coordinate descent** is typically better\n-   MOST IMPORTANT PROPERTY OF LASSO: it *induces sparsity* while Ridge does not\n:::\n\n## Sparsity in Applied Math {.smaller}\n\n-   **sparse** typically means \"most things are zero\"\n-   Example: sparse matrices are matrices where most entries are zero\n    +   for large matrices this can provide HUGE performance gains\n-   LASSO *induces sparsity* by setting most of the parameter estimates to zero\n    +   this means it fits the model and does feature selection SIMULTANEOUSLY\n-   Let's do some board work to see why this is...\n\n## LASSO and Ridge in R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols <- linear_reg() |> \n  set_engine(\"lm\")\nridge_0 <- linear_reg(mixture = 0, penalty = 0) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_1 <- linear_reg(mixture = 0, penalty = 1) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_10 <- linear_reg(mixture = 0, penalty = 10) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_0 <- linear_reg(mixture = 1, penalty = 0) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_1 <- linear_reg(mixture = 1, penalty = 1) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_10 <- linear_reg(mixture = 1, penalty = 10) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n\n## Create Recipe\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preproc <- recipe(winpercent ~ . , data = candy_train) |> \n  step_dummy(all_nominal_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n\n## Question\n\n-   Why do we need to normalize our data?\n\n## Create workflows\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeneric_wf <- workflow() |> add_recipe(lm_preproc)\nols_wf <- generic_wf |> add_model(ols)\nridge0_wf <- generic_wf |> add_model(ridge_0)\nridge1_wf <- generic_wf |> add_model(ridge_1)\nridge10_wf <- generic_wf |> add_model(ridge_10)\nlasso0_wf <- generic_wf |> add_model(lasso_0)\nlasso1_wf <- generic_wf |> add_model(lasso_1)\nlasso10_wf <- generic_wf |> add_model(lasso_10)\n```\n:::\n\n\n\n\n## Fit Models\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_fit <- ols_wf |> fit(candy_train)\nridge0_fit <- ridge0_wf |> fit(candy_train)\nridge1_fit <- ridge1_wf |> fit(candy_train)\nridge10_fit <- ridge10_wf |> fit(candy_train)\nlasso0_fit <- lasso0_wf |> fit(candy_train)\nlasso1_fit <- lasso1_wf |> fit(candy_train)\nlasso10_fit <- lasso10_wf |> fit(candy_train)\n```\n:::\n\n\n\n\n## Collect coefficients\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs <- bind_cols(model = \"ols\", tidy(ols_fit)) |> \n  bind_rows(bind_cols(model = \"ridge0\", tidy(ridge0_fit))) |> \n  bind_rows(bind_cols(model = \"ridge1\", tidy(ridge1_fit))) |> \n  bind_rows(bind_cols(model = \"ridge10\", tidy(ridge10_fit))) |> \n  bind_rows(bind_cols(model = \"lasso0\", tidy(lasso0_fit))) |> \n  bind_rows(bind_cols(model = \"lasso1\", tidy(lasso1_fit))) |> \n  bind_rows(bind_cols(model = \"lasso10\", tidy(lasso10_fit)))\n```\n:::\n\n\n\n\n## Question\n\n::: incremental\n-   What should we expect from `ols`, `ridge0`, and `lasso0`?\n-   They should be the same!\n:::\n\n## Visualize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"ridge0\", \"lasso0\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](17-regularization_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n## Uh-Oh!\n\n::: incremental\n-   They're not the same\n-   Any idea why?\n-   Algorithm used to fit model\n    +   `lm` estimates coefficients *analytically*\n    +   `glmnet` estimates coefficients *numerically* using an algorithm named \"coordinate-descent\"\n-   Moral: you must understand theory AND application\n:::\n\n## Visualizing Coefficients: Ridge\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"ridge1\", \"ridge10\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](17-regularization_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n\n## Visualizing Coefficients: LASSO\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"lasso1\", \"lasso10\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](17-regularization_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n\n## Ridge Coefficients vs. Penalty ( $\\lambda$ )\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge0_fit |> extract_fit_engine() |> autoplot()\n```\n\n::: {.cell-output-display}\n![](17-regularization_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n## LASSO Coefficients vs. Penalty ( $\\lambda$ )\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso0_fit |> extract_fit_engine() |> autoplot()\n```\n\n::: {.cell-output-display}\n![](17-regularization_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Why should I learn math for data science?\n\n-   Today:\n    +   Ordinary Least Squares: Linear algebra, Calc 3\n    +   LASSO: Calc 3, Geometry, Numerical Analysis\n    +   Ridge: Linear Algebra, Calc 3\n    +   Bias-Variance Trade-Off: Probability\n    \n## Question\n\n-   How do you think we should choose $\\lambda$?\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}