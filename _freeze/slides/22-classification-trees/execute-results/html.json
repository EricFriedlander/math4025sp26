{
  "hash": "d85fc02ac6296fd26304c26c724d036d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Classification Trees'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n\n\n## Decision Trees {.smaller}\n\n-   Advantages\n    +   Easy to explain and interpret\n    +   Closely mirror human decision-making\n    +   Can be displayed graphically, and are easily interpreted by non-experts\n    +   Does not require standardization of predictors\n    +   Can handle missing data directly\n    +   Can easily capture non-linear patterns\n-   Disadvantages\n    +   Do not have same level of prediction accuracy\n    +   Not very robust\n    \n# Decision Trees for Classification\n\n## Last Time\n\n::: incremental\n-   Regression Trees: Decision Trees for Regression Problems\n-   How are they fit?\n-   What is pruning? Why do we do it?\n-   What tuning parameter did we talk about last time?\n-   Today: Classification Trees\n:::\n\n## Classification Trees\n\n-   Predictions: \n    +   Classes: most common class at terminal node\n    +   Probability: proportion of each class at terminal node\n-   Rest of tree: same as regression tree\n\n## Classification Trees\n\n-   Predictions: \n    +   Classes: most common class at terminal node\n    +   Probability: proportion of each class at terminal node\n-   Rest of tree: same as regression tree\n\n## Exploring Decision Trees w/ App {.smaller}\n\n-   Dr. F will split you into four groups\n-   On one of your computers connect to a tv and [open this app](https://efriedlander.shinyapps.io/ClassificationMetrics/)\n-   Do the following based on your group number:\n    +   1: Choose plane on the first screen\n    +   2: Choose circle on the first screen\n    +   3: Choose parabola on the first screen\n    +   4: Choose sine curve on the first screen\n-   We will generate data from this population... do you think KNN, logistic regression, or a decision tree will yield a better classifier? Why?\n\n## Exploring Decision Trees w/ App {.smaller}\n\n-   Choose one of the populations\n-   Generate some data\n-   Fit a decision tree to the data and see how the different hyper parameters impact the resulting model:\n    +   **complexity parameter (cp)**: the larger the number the more pruning\n    +   **Minimum leaf size**: the minimum number of observations from the training data that must be contained in a leaf\n    +   **Max depth**: the maximum number of splits before a terminal node\n-   Write down any interesting observations\n\n## Exploring Decision Trees w/ App {.smaller}\n\n-   Dr. F will split you into four groups\n-   On one of your computers connect to a tv and [open this app](https://efriedlander.shinyapps.io/ClassificationMetrics/)\n-   Do the following based on your group number:\n    +   1: Choose plane on the first screen\n    +   2: Choose circle on the first screen\n    +   3: Choose parabola on the first screen\n    +   4: Choose sine curve on the first screen\n-   We will generate data from this population... do you think KNN, logistic regression, or a decision tree will yield a better classifier? Why?\n\n## Exploring Decision Trees w/ App {.smaller}\n\n-   Choose one of the populations\n-   Generate some data\n-   Fit a decision tree to the data and see how the different hyper parameters impact the resulting model:\n    +   **complexity parameter (cp)**: the larger the number the more pruning\n    +   **Minimum leaf size**: the minimum number of observations from the training data that must be contained in a leaf\n    +   **Max depth**: the maximum number of splits before a terminal node\n-   Write down any interesting observations\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}