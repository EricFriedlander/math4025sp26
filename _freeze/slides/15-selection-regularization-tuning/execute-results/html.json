{
  "hash": "0e83b5b41f9f59f74cb0fb2dc80e7b12",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Workflow Sets, Feature Selection, and Regularization'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(janitor) # for contingency tables\nlibrary(ISLR2)\nlibrary(readODS)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n\n# Workflow Sets in R\n\n## Data: Different Ames Housing Prices {.smaller}\n\nGoal: Predict `Sale_Price`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names <- read_rds(\"../data/AmesHousing.rds\")\names |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 881\nColumns: 20\n$ Sale_Price    <int> 244000, 213500, 185000, 394432, 190000, 149000, 149900, â€¦\n$ Gr_Liv_Area   <int> 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, â€¦\n$ Garage_Type   <fct> Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, â€¦\n$ Garage_Cars   <dbl> 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,â€¦\n$ Garage_Area   <dbl> 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7â€¦\n$ Street        <fct> Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Paâ€¦\n$ Utilities     <fct> AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, â€¦\n$ Pool_Area     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ Neighborhood  <fct> North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwestâ€¦\n$ Screen_Porch  <int> 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Overall_Qual  <fct> Good, Very_Good, Above_Average, Excellent, Above_Averageâ€¦\n$ Lot_Area      <int> 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101â€¦\n$ Lot_Frontage  <dbl> 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6â€¦\n$ MS_SubClass   <fct> One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_â€¦\n$ Misc_Val      <int> 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ Open_Porch_SF <int> 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,â€¦\n$ TotRms_AbvGrd <int> 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7â€¦\n$ First_Flr_SF  <int> 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15â€¦\n$ Second_Flr_SF <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11â€¦\n$ Year_Built    <int> 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20â€¦\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Clean Data Set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names <- ames |> \n  mutate(Overall_Qual = factor(Overall_Qual, levels = c(\"Very_Poor\", \"Poor\", \n                                                        \"Fair\", \"Below_Average\",\n                                                        \"Average\", \"Above_Average\", \n                                                        \"Good\", \"Very_Good\",\n                                                        \"Excellent\", \"Very_Excellent\")),\n         Garage_Type = if_else(is.na(Garage_Type), \"No_Garage\", Garage_Type),\n         Garage_Type = as_factor(Garage_Type)\n         )\n```\n:::\n\n\n\n## Initial Data Split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_split <- initial_split(ames, strata = \"Sale_Price\")\names_train <- training(data_split)\names_test  <- testing(data_split)\n```\n:::\n\n\n\n## Define Folds\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_folds <- vfold_cv(ames_train, v = 10, repeats = 10)\names_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation repeated 10 times \n# A tibble: 100 Ã— 3\n   splits           id       id2   \n   <list>           <chr>    <chr> \n 1 <split [594/66]> Repeat01 Fold01\n 2 <split [594/66]> Repeat01 Fold02\n 3 <split [594/66]> Repeat01 Fold03\n 4 <split [594/66]> Repeat01 Fold04\n 5 <split [594/66]> Repeat01 Fold05\n 6 <split [594/66]> Repeat01 Fold06\n 7 <split [594/66]> Repeat01 Fold07\n 8 <split [594/66]> Repeat01 Fold08\n 9 <split [594/66]> Repeat01 Fold09\n10 <split [594/66]> Repeat01 Fold10\n# â„¹ 90 more rows\n```\n\n\n:::\n:::\n\n\n\n## Define Model(s)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model <- linear_reg() |> \n  set_engine(\"lm\")\n\nknn5_model <- nearest_neighbor(neighbors = 5) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"regression\")\n\nknn10_model <- nearest_neighbor(neighbors = 10) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"regression\")\n```\n:::\n\n\n\n## Define Preprocessing: Linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_knnimpute <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |> # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations\n```\n:::\n\n\n\n## Define Preprocessing: Linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_meanimpute <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_mean(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |> # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations\n```\n:::\n\n\n\n## Define Preprocessing: Linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_medianimpute <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_median(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression\n  step_corr(all_numeric_predictors(), threshold = 0.5) |> # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations\n```\n:::\n\n\n\n## Define Preprocessing: KNN\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_preproc1 <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_knn(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Define Preprocessing: KNN\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_preproc2 <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_mean(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Define Preprocessing: KNN\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_preproc3 <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_median(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # in general use one_hot unless doing linear regression\n  step_nzv(all_predictors()) |> \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n\n## Workflow Sets\n\n-   Input `list`s of models and `recipe`s\n-   If `cross = TRUE` will try out all combinations\n\n## Create lists\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_preprocessors <- list(\n  knn_knn_impute = knn_preproc1,\n  knn_mean_impute = knn_preproc2,\n  knn_median_imput = knn_preproc3\n)\n\nknn_models <- list(\n  knn5 = knn5_model,\n  knn10 = knn10_model\n)\n```\n:::\n\n\n\n## Create lists\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preprocessors <- list(\n  lm_knn_impute = lm_knnimpute,\n  lm_mean_impute = lm_meanimpute,\n  lm_median_imput = lm_medianimpute\n)\n\nlm_models <- list(\n  lm_model = lm_model\n)\n```\n:::\n\n\n\n## Define Workflow Sets {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_models <- workflow_set(knn_preprocessors, knn_models, cross = TRUE)\nlm_models <-  workflow_set(lm_preprocessors, lm_models, cross = TRUE)\nall_models <- lm_models |> \n  bind_rows(knn_models)\n  \nall_models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A workflow set/tibble: 9 Ã— 4\n  wflow_id                 info             option    result    \n  <chr>                    <list>           <list>    <list>    \n1 lm_knn_impute_lm_model   <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n2 lm_mean_impute_lm_model  <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n3 lm_median_imput_lm_model <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n4 knn_knn_impute_knn5      <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n5 knn_knn_impute_knn10     <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n6 knn_mean_impute_knn5     <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n7 knn_mean_impute_knn10    <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n8 knn_median_imput_knn5    <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n9 knn_median_imput_knn10   <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n```\n\n\n:::\n:::\n\n\n\n## Define Metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_metrics <- metric_set(rmse, rsq)\n```\n:::\n\n\n\n## Fit Resamples {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_fits <- all_models |> \n  workflow_map(\"fit_resamples\",\n               resamples = ames_folds,\n               metrics = ames_metrics)\n```\n:::\n\n\n\n## View Metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(all_fits) |> \n  filter(.metric == \"rmse\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|wflow_id                 |.config              |preproc |model            |.metric |.estimator |     mean|   n|   std_err|\n|:------------------------|:--------------------|:-------|:----------------|:-------|:----------|--------:|---:|---------:|\n|lm_knn_impute_lm_model   |Preprocessor1_Model1 |recipe  |linear_reg       |rmse    |standard   | 39754.78| 100|  724.5845|\n|lm_mean_impute_lm_model  |Preprocessor1_Model1 |recipe  |linear_reg       |rmse    |standard   | 40339.76| 100|  790.8570|\n|lm_median_imput_lm_model |Preprocessor1_Model1 |recipe  |linear_reg       |rmse    |standard   | 40303.18| 100|  792.2404|\n|knn_knn_impute_knn5      |Preprocessor1_Model1 |recipe  |nearest_neighbor |rmse    |standard   | 40865.74| 100| 1168.9516|\n|knn_knn_impute_knn10     |Preprocessor1_Model1 |recipe  |nearest_neighbor |rmse    |standard   | 40585.89| 100| 1196.3147|\n|knn_mean_impute_knn5     |Preprocessor1_Model1 |recipe  |nearest_neighbor |rmse    |standard   | 40973.29| 100| 1169.1127|\n|knn_mean_impute_knn10    |Preprocessor1_Model1 |recipe  |nearest_neighbor |rmse    |standard   | 40749.35| 100| 1197.0999|\n|knn_median_imput_knn5    |Preprocessor1_Model1 |recipe  |nearest_neighbor |rmse    |standard   | 40979.23| 100| 1166.2676|\n|knn_median_imput_knn10   |Preprocessor1_Model1 |recipe  |nearest_neighbor |rmse    |standard   | 40747.77| 100| 1196.2514|\n\n\n:::\n:::\n\n\n\n## View Metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(all_fits) |> \n  filter(.metric == \"rsq\") |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|wflow_id                 |.config              |preproc |model            |.metric |.estimator |      mean|   n|   std_err|\n|:------------------------|:--------------------|:-------|:----------------|:-------|:----------|---------:|---:|---------:|\n|lm_knn_impute_lm_model   |Preprocessor1_Model1 |recipe  |linear_reg       |rsq     |standard   | 0.7728989| 100| 0.0055434|\n|lm_mean_impute_lm_model  |Preprocessor1_Model1 |recipe  |linear_reg       |rsq     |standard   | 0.7661074| 100| 0.0067038|\n|lm_median_imput_lm_model |Preprocessor1_Model1 |recipe  |linear_reg       |rsq     |standard   | 0.7667739| 100| 0.0066014|\n|knn_knn_impute_knn5      |Preprocessor1_Model1 |recipe  |nearest_neighbor |rsq     |standard   | 0.7574393| 100| 0.0080339|\n|knn_knn_impute_knn10     |Preprocessor1_Model1 |recipe  |nearest_neighbor |rsq     |standard   | 0.7662157| 100| 0.0070608|\n|knn_mean_impute_knn5     |Preprocessor1_Model1 |recipe  |nearest_neighbor |rsq     |standard   | 0.7566579| 100| 0.0076762|\n|knn_mean_impute_knn10    |Preprocessor1_Model1 |recipe  |nearest_neighbor |rsq     |standard   | 0.7651128| 100| 0.0068290|\n|knn_median_imput_knn5    |Preprocessor1_Model1 |recipe  |nearest_neighbor |rsq     |standard   | 0.7568404| 100| 0.0076513|\n|knn_median_imput_knn10   |Preprocessor1_Model1 |recipe  |nearest_neighbor |rsq     |standard   | 0.7653082| 100| 0.0068162|\n\n\n:::\n:::\n\n\n\n## Plotting Results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggrepel)\nautoplot(all_fits, metric = \"rmse\") +\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](15-selection-regularization-tuning_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n## Plotting Results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(all_fits, metric = \"rsq\") +\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](15-selection-regularization-tuning_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\n# Feature Selection for Linear Regression\n\n## What is feature selection? {.smaller}\n\n-   How do we choose what variables to include in our model?\n-   Up to now... include all of them... probably not the best\n-   Advantage of linear regression: interpretability\n-   Including every feature decreases interpretability\n-   Reasons for feature selection:\n    +   Improve model performance\n    +   Improve model interpretability\n-   Parsimony: simpler models are called more *parsimonious*\n-   Occam's Razor: more parsimonious models are better than less parsimonious models, holding all else constant\n\n## Types of feature selection\n\n-   Subset selection: Forward/Backward/Best-Subset Selection\n-   Shrinkage-based methods: LASSO and Ridge Regression\n-   Dimension reduction: consider linear combinations of predictors\n\n# Subset Selection\n\n## Exercise\n\n-   With your group, write out the steps for the following algorithms on the board\n  +   Group 1: Forward selection\n  +   Group 2: Backward elimination\n  +   Group 3: Step-wise selection\n  +   Group 4: Best-subset selection\n\n## Subset Selection in R {.smaller}\n\n-   `tidymodels` does not have an implementation for any subset selection techniques\n-   regularization (shrinkage-based) methods almost always perform better\n-   [`colino` package](https://stevenpawley.github.io/colino/) provides `tidymodels` implementation\n-   Other options\n    +   `caret` package\n    +   `olsrr` and `blorr` packages if you don't care about cross-validation\n    +   implement yourself\n    \n## Feature Selection in R\n\n-   When creating your recipe, don't need to always include all variables in your `recipe`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_recipe <- recipe(pred ~ var1 + var2 + var1*var2, data = training_data) |> \n  step_x(...)\n```\n:::\n\n\n\n## Re-using Recipe but changing formula\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnoint_recipe2 <- new_recipe |> \n  remove_formula() |> \n  add_formula(pred ~ var1 + var2)\n```\n:::\n\n\n\n# Shrinkage/Penalized/Regularization Methods\n\n## Question\n\n-   What criteria do we use to fit a linear regression model? Write down an equation with $\\hat{\\beta_i}$'s, $x_{ij}$'s, and $y_j$'s in it.\n\n## OLS {.smaller}\n\n-   Ordinary Least Squares Regression:\n\n$$\n\\begin{aligned}\n\\hat{\\beta} =\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}}SSE(\\hat{\\beta}) &= \\sum_{j=1}^n(y_j-\\hat{y}_j)^2\\\\\n&=\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2\n\\end{aligned}\n$$\n\n-   $\\hat{\\beta} = (\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p)$ is the vector of all my coefficients\n-   $\\operatorname{argmin}$ is a function (operator) that returns the *arguments* that minimize the quantity it's being applied to\n\n## Ridge Regression {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_2^2\\right) \\\\\n&= \\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\\\\\n&=\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p \\hat{\\beta}_i^2\n\\end{aligned}\n$$\n\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients? [[THEY SHRINK!]{.span style=\"color:red;\"}]{.fragment .fade-in}\n-   $\\|\\cdot\\|_2$ is called the *$L_2$-norm*\n\n## But... but... but why?!? {.smaller}\n\n::: incremental\n-   Recall the Bias-Variance Trade-Off\n-   Our reducible error can partitioned into:\n    +   Bias: how much $\\hat{f}$ misses $f$ by *on average*\n    +   Variance: how much $\\hat{f}$ moves around from sample to sample\n-   Ridge: increase bias a little bit in exchange for large decrease in variance\n-   As we increase $\\lambda$ do we increase or decrease the penalty for large coefficients?\n-   As we increase $\\lambda$ do we increase or decrease the *flexibility* of our model?\n:::\n\n## LASSO {.smaller}\n\n$$\n\\begin{aligned}\n\\hat{\\beta} &=\\underset{\\hat{\\beta}_0,\\ldots, \\hat{\\beta}_p}{\\operatorname{argmin}} \\left(SSE(\\hat{\\beta}) + \\lambda\\|\\hat{\\beta}\\|_1\\right) \\\\\n&= \\sum_{j=1}^n(y_j-\\hat{y}_j)^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\\\\\n&=\\sum_{j=1}^n(y_j-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1j} - \\cdots - \\hat{\\beta}_px_{pj})^2 + \\lambda\\sum_{i=1}^p |\\hat{\\beta}_i|\n\\end{aligned}\n$$\n\n-   $\\lambda$ is called a \"tuning parameter\" and is not estimated from the data (more on this later)\n-   Idea: penalize large coefficients\n-   If we penalize large coefficients, what's going to happen to to our estimated coefficients [THEY SHRINK!]{.span style=\"color:red;\"}\n-   $\\|\\cdot\\|_1$ is called the *$L_1$-norm*\n\n## Question\n\n-   What should happen to our coefficients as we *increase* $\\lambda$?\n\n## Ridge vs. LASSO\n\n::: incremental\n-   Ridge has a *closed-form solution*... how might be calculate it?\n-   Ridge has some nice linear algebra properties that makes is EXTREMELY FAST to fit\n-   LASSO has no *closed-form solution*... why?\n-   LASSO coefficients estimated *numerically*... how?\n    +   Gradient descent works but something called **coordinate descent** is typically better\n-   MOST IMPORTANT PROPERTY OF LASSO: it *induces sparsity* while Ridge does not\n:::\n\n## Sparsity in Applied Math {.smaller}\n\n-   **sparse** typically means \"most things are zero\"\n-   Example: sparse matrices are matrices where most entries are zero\n    +   for large matrices this can provide HUGE performance gains\n-   LASSO *induces sparsity* by setting most of the parameter estimates to zero\n    +   this means it fits the model and does feature selection SIMULTANEOUSLY\n-   Let's do some board work to see why this is...\n\n## LASSO and Ridge in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols <- linear_reg() |> \n  set_engine(\"lm\")\nridge_0 <- linear_reg(mixture = 0, penalty = 0) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_1000 <- linear_reg(mixture = 0, penalty = 1000) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nridge_10000 <- linear_reg(mixture = 0, penalty = 10000) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_0 <- linear_reg(mixture = 1, penalty = 0) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_1000 <- linear_reg(mixture = 1, penalty = 1000) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\nlasso_10000 <- linear_reg(mixture = 1, penalty = 10000) |> # penalty set's our lambda\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n## Create Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_meanimpute <- recipe(Sale_Price ~ ., data = ames_train) |> \n  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors\n  step_impute_mean(Year_Built, Gr_Liv_Area) |>  # impute missing values in Overall_Qual and Year_Built\n  step_integer(Overall_Qual) |> # convert Overall_Qual into ordinal encoding\n  step_other(all_nominal_predictors(), threshold = 0.01, other = \"Other\") |> # lump all categories with less than 1% representation into a category called Other for each variable\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression\n  step_normalize(all_numeric_predictors()) |> \n  step_corr(all_numeric_predictors(), threshold = 0.5) |> # remove highly correlated predictors\n  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations\n```\n:::\n\n\n\n## Question\n\n-   Why do we need to normalize our data?\n\n## Create workflows\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeneric_wf <- workflow() |> add_recipe(lm_meanimpute)\nols_wf <- generic_wf |> add_model(ols)\nridge0_wf <- generic_wf |> add_model(ridge_0)\nridge1000_wf <- generic_wf |> add_model(ridge_1000)\nridge10000_wf <- generic_wf |> add_model(ridge_10000)\nlasso0_wf <- generic_wf |> add_model(lasso_0)\nlasso1000_wf <- generic_wf |> add_model(lasso_1000)\nlasso10000_wf <- generic_wf |> add_model(lasso_10000)\n```\n:::\n\n\n\n## Fit Models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_fit <- ols_wf |> fit(ames_train)\nridge0_fit <- ridge0_wf |> fit(ames_train)\nridge1000_fit <- ridge1000_wf |> fit(ames_train)\nridge10000_fit <- ridge10000_wf |> fit(ames_train)\nlasso0_fit <- lasso0_wf |> fit(ames_train)\nlasso1000_fit <- lasso1000_wf |> fit(ames_train)\nlasso10000_fit <- lasso10000_wf |> fit(ames_train)\n```\n:::\n\n\n\n## Collect coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs <- bind_cols(model = \"ols\", tidy(ols_fit)) |> \n  bind_rows(bind_cols(model = \"ridge0\", tidy(ridge0_fit))) |> \n  bind_rows(bind_cols(model = \"ridge1000\", tidy(ridge1000_fit))) |> \n  bind_rows(bind_cols(model = \"ridge10000\", tidy(ridge10000_fit))) |> \n  bind_rows(bind_cols(model = \"lasso0\", tidy(lasso0_fit))) |> \n  bind_rows(bind_cols(model = \"lasso1000\", tidy(lasso1000_fit))) |> \n  bind_rows(bind_cols(model = \"lasso10000\", tidy(lasso10000_fit)))\n```\n:::\n\n\n\n## Question\n\n::: incremental\n-   What should we expect from `ols`, `ridge0`, and `lasso0`?\n-   They should be the same!\n:::\n\n## Visualize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"ridge0\", \"lasso0\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](15-selection-regularization-tuning_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n\n## Uh-Oh!\n\n::: incremental\n-   They're not the same\n-   Any idea why?\n-   Algorithm used to fit model\n    +   `lm` estimates coefficients *analytically*\n    +   `glmnet` estimates coefficients *numerically* using an algorithm named \"coordinate-descent\"\n-   Moral: you must understand theory AND application\n:::\n\n## Visualizing Coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"ridge1000\", \"ridge10000\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](15-selection-regularization-tuning_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n\n## Visualizing Coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_coefs |> \n  filter(model %in% c(\"ols\", \"lasso1000\", \"lasso10000\")) |> \n  ggplot(aes(y = term, x = estimate, fill = model, color = model)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n```\n\n::: {.cell-output-display}\n![](15-selection-regularization-tuning_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}