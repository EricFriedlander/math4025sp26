{
  "hash": "7627be81bcfee6ff819d7b455af2a7d0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Decision Trees Continued'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n## Announcements\n\n-   Job Application Discussion\n-   Job Interviews\n-   Admissions Data Project\n\n## Job Application Discussion\n\n::: incremental\n-   We missed the mark a bit.\n-   Resumes and CVs were good. Sample analysis were... not.\n-   Biggest issue: professionalism and editing.\n-   If the majority of your analysis was code, you probably got a bad grade.\n-   Next time:\n    +   Explain everything that you are doing and justify it!\n    +   Proofread your document!\n    +   Make it look nice!\n:::\n\n## Job Application Discussion\n\n::: incremental\n-   Some common themes:\n    +   Y'all love the word \"hone\"\n    +   Y'all love the phrase \"actionable insights\"... so do I... but still\n    +   I think some of you are overselling yourselves\n    +   In cover letter, use fewer \"buzz-words\" and include more substance\n:::\n\n## Job Interviews\n\n-   [Link to Resources](/jobs/job-interview-resources.qmd)\n-   Two Rounds\n    +   First: Screening interview with Dani (Schedule by Wednesday)\n        +   Due next Friday, April 11th\n    +   Second: Technical interview with me (Due last day of class)\n        +   Note that we'll be doing a lot of stuff between now and then so try to get this done sooner rather than later\n        \n## Project\n\n-   [Project Instructions](/project/project-instructions.qmd)\n-   Brian Bava visiting on Friday to discuss\n-   DO ASAP: Sign data agreement\n-   Do by Wednesday: Load the data and review instructions. We will have a discussion about cleaning the data on Wednesday. You are all expected to contribute.\n-   Do by Friday: Explore the data. Your team is expected to bring at least three substantial questions for Brian.\n\n\n## Computational Set-Up\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dsbox) # dcbikeshare data\nlibrary(knitr)\n\ntidymodels_prefer()\n\nset.seed(427)\n```\n:::\n\n\n\n\n## Last Time\n\n-   Regression Trees in R\n-   Warm-up question: What is cost complexity tuning?\n\n# Regression Trees in R\n\n## Data: `dcbikeshare` {.smaller}\n\nBike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. As of May 2018, there are about over 1600 bike-sharing programs around the world, providing more than 18 million bicycles for public use. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. [Documentation](https://www.kaggle.com/datasets/marklvl/bike-sharing-dataset)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dcbikeshare)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 731\nColumns: 16\n$ instant    <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, â€¦\n$ dteday     <date> 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05â€¦\n$ season     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,â€¦\n$ yr         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ mnth       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,â€¦\n$ holiday    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,â€¦\n$ weekday    <dbl> 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,â€¦\n$ workingday <dbl> 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,â€¦\n$ weathersit <dbl> 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2,â€¦\n$ temp       <dbl> 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.20â€¦\n$ atemp      <dbl> 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.23â€¦\n$ hum        <dbl> 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261,â€¦\n$ windspeed  <dbl> 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.08â€¦\n$ casual     <dbl> 331, 131, 120, 108, 82, 88, 148, 68, 54, 41, 43, 25, 38, 54â€¦\n$ registered <dbl> 654, 670, 1229, 1454, 1518, 1518, 1362, 891, 768, 1280, 122â€¦\n$ cnt        <dbl> 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 126â€¦\n```\n\n\n:::\n:::\n\n\n\n## Cleaning the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndcbikeshare_clean <- dcbikeshare |> \n  select(-instant, -dteday, -casual, -registered, -yr) |> \n  mutate(\n    season = as_factor(case_when(\n      season == 1 ~ \"winter\",\n      season == 2 ~ \"spring\",\n      season == 3 ~ \"summer\",\n      season == 4 ~ \"fall\"\n    )),\n    mnth = as_factor(mnth),\n    weekday = as_factor(weekday),\n    weathersit = as_factor(weathersit)\n  )\n```\n:::\n\n\n\n## Split the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\nbike_split <- initial_split(dcbikeshare_clean, prop = 0.7, strata = cnt)\nbike_train <- training(bike_split)\nbike_test <- testing(bike_split)\n```\n:::\n\n\n\n\n\n## Recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_recipe <- recipe(cnt ~ ., data = bike_train) |>   # set up recipe\n  step_integer(season, mnth, weekday) |>   # numeric conversion of levels of the predictors\n  step_dummy(all_nominal(), one_hot = TRUE)  # one-hot/dummy encode nominal categorical predictors\n```\n:::\n\n\n\n## Define Model Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndec_tree_lowcc <- decision_tree(cost_complexity = 10^(-4)) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\ndec_tree_highcc <- decision_tree(cost_complexity = 0.1) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n```\n:::\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nworkflow() |> \n  add_recipe(bike_recipe) |> \n  add_model(dec_tree_lowcc) |> \n  fit(bike_train) |>\n  extract_fit_engine() |> \n  rpart.plot()\n```\n:::\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-decision-trees-3_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nworkflow() |> \n  add_recipe(bike_recipe) |> \n  add_model(dec_tree_highcc) |> \n  fit(bike_train) |>\n  extract_fit_engine() |> \n  rpart.plot()\n```\n:::\n\n\n\n## Visualize\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](21-decision-trees-3_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n\n\n## Defin Model Workflow with Tuning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndec_tree <- decision_tree(cost_complexity = tune()) |> \n  set_engine(\"rpart\") |> \n  set_mode(\"regression\")\n\ndt_wf <- workflow() |> \n  add_recipe(bike_recipe) |> \n  add_model(dec_tree)\n```\n:::\n\n\n\n## Define Folds and Tuning Grid\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_folds <- vfold_cv(bike_train, v = 5, repeats = 10)\n\ncp_grid <- grid_regular(cost_complexity(range = c(-4, -1)), # I had to play around with these \n                             levels = 20)\n```\n:::\n\n\n\n## Tuning CP\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuning_cp_results <- tune_grid(\n  dt_wf,\n  resamples= bike_folds,\n  grid = cp_grid\n)\n```\n:::\n\n\n\n## Plot Results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tuning_cp_results)\n```\n\n::: {.cell-output-display}\n![](21-decision-trees-3_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Select Best Trees\n\n:::: columns\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tree <- select_best(tuning_cp_results)\nbest_tree |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| cost_complexity|.config               |\n|---------------:|:---------------------|\n|       0.0054556|Preprocessor1_Model12 |\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nose_tree <- select_by_one_std_err(tuning_cp_results, desc(cost_complexity))\nose_tree |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| cost_complexity|.config               |\n|---------------:|:---------------------|\n|       0.0078476|Preprocessor1_Model13 |\n\n\n:::\n:::\n\n\n:::\n::::\n\n## Fit Best Tree\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tree <- select_best(tuning_cp_results)\nbest_wf <- finalize_workflow(dt_wf, best_tree)\nbest_model <- best_wf |> fit(bike_train)\nbest_model |> \n  extract_fit_engine() |> \n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](21-decision-trees-3_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n## Fit OSE Tree\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nose_tree <- select_best(tuning_cp_results)\nose_wf <- finalize_workflow(dt_wf, ose_tree)\nose_model <- ose_wf |> fit(bike_train)\nose_model |> \n  extract_fit_engine() |> \n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](21-decision-trees-3_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n## Questions\n\n-   Why are both models the same but have different RMSE estimates from CV?\n-   What's the difference between encoding `mnth` as an ordinal variable vs. a one-hot encoding?\n\n## Decision Trees {.smaller}\n\n-   Advantages\n    +   Easy to explain and interpret\n    +   Closely mirror human decision-making\n    +   Can be displayed graphically, and are easily interpreted by non-experts\n    +   Does not require standardization of predictors\n    +   Can handle missing data directly\n    +   Can easily capture non-linear patterns\n-   Disadvantages\n    +   Do not have same level of prediction accuracy\n    +   Not very robust\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}