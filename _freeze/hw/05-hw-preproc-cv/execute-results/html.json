{
  "hash": "df6917fb28f8f59adacb235a4b2624a7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Homework 05: Preprocessing and Cross Validation\" \nauthor: \"Your Name\"\neditor: visual\nformat:\n  html:\n    embed-resources: true\n---\n\n\n\n# Introduction\n\nIn this homework you will practice pre-processing data and using cross-validation to evaluate regression models.\n\n## Learning goals\n\nIn this assignment, you will...\n\n-   Use exploratory data analysis to inform feature engineering steps\n-   Pre-process data and impute missing values\n-   Evaluate and compare models using cross-validation\n\n# Getting Started\n\nIn last weeks homework, you learned how to share your work using GitHub and to resolve merge conflicts using branches. From here on out, you are free to use whatever version control strategy you like.\n\n## Teams & Rules\n\nYou can find your team for this assignment on Canvas in the **People** section. The group set is called **HW5**. Your group will consist of 2-3 people and has been randomly generated. You have now been exposed to all of the Git concepts that we will talk about in this class. It is up to you to apply them to complete your homework in any way you see fit. Some rules:\n\n1.  You are all responsible for understanding the work that you turn in.\n2.  All team members must make roughly equal contributions to the homework.\n3.  Any work completed by a team member must be committed and pushed to GitHub by that person.\n\n## Exercise 0\n\nAs in your previous homework's, create your team on GitHub classroom and clone the repository. [Here](https://classroom.github.com/a/VhxbjrFN) is a link to the homework.\n\n## Data: LEGO\n\nThe data for this analysis includes information about LEGO sets from themes produced January 1, 2018 and September 11, 2020. The data were originally scraped from Brickset.com, an online LEGO set guide and were obtained for this assignment from Peterson and Zieglar ([2021](https://www.tandfonline.com/doi/full/10.1080/26939169.2021.1946450)).\n\nYou will work with data on about 400 randomly selected LEGO sets produced during this time period. The primary variables are interest in this analysis are:\n\n-   `Item_Number`: a serial code corresponding to the set.\n-   `Set_Name`: The name of the LEGO set.\n-   `Theme`: Theme of the LEGO set.\n-   `Pieces`: Number of pieces in the set from brickset.com.\n-   `Amazon_Price`: Amazon price of the set scraped from brickset.com (in U.S. dollars).\n-   `Year` : Year the LEGO set was produced.\n-   `Ages`: Variable stating what aged children the set is appropriate for.\n-   `Pages`: Number of pages in the instruction booklet.\n-   `Minifigures`: Number of minifigures (LEGO people) in the set scraped from brickset.com. LEGO sets with no minifigures have been coded as NA. NA's also represent missing data. This is due to how brickset.com reports their data.\n-   `Packaging`: What type of packaging the set came in.\n-   `Weight`: The weight of the set.\n-   `Unique_Pieces`: The number of unique pieces in each set.\n-   `Availability`: Where the set can be purchased.\n-   `Size`: General size of the interlocking bricks (Large = LEGO Duplo sets - which include large brick pieces safe for children ages 1 to 5, Small = LEGO sets which- include the traditional smaller brick pieces created for age groups 5 and - older, e.g., City, Friends).\n\nYour ultimate goal will be to predict `Amazon_Price` from the other features.\n\n# Loading & Cleaning the Data\n\n## Exercise 1\n\n::: {.callout-tip title=\"Question\"}\nThe data are contained in `lego-sample.csv`. Load the data.\n:::\n\n## Exercise 2\n\n::: {.callout-tip title=\"Question\"}\nTwo of the variables in the data set shouldn't be useful because they just serve to identify the different LEGO sets. Which two are they? Remove them.\n:::\n\n## Exercise 3\n\n::: {.callout-tip title=\"Question\"}\nNotice that the `Weight` variable is a bit odd... It seems like it should be numeric but it's a `chr`. Why? Write code to extract the true numerical weight in either lbs or Kgs (your choice). You are encouraged to use the internet and generative AI to help you figure out how to do this. However, make sure you are able to explain your code once you are done.\n:::\n\n## Exercise 4\n\n::: {.callout-tip title=\"Question\"}\nFor each of the 12 features do the following:\n:::\n\n### Exercise 4.1\n\n::: {.callout-tip title=\"Question\"}\nIdentify if they are the correct data type. Are categorical variables coded as factors? Are the factor levels in the correct order if necessary? Are numerical variables coded as numbers? You will need to read descriptions of the data to make this determination.\n:::\n\n### Exercise 4.2\n\n::: {.callout-tip title=\"Question\"}\nIdentify any variables with missing values. Identify and then fix any variables for whom missing values (i.e. `NA`s) indicate something other than that the data is missing (there is at least one). Fill in this missing values appropriately.\n:::\n\n\n### Exercise 4.3\n\n::: {.callout-tip title=\"Question\"}\nFor all of the categorical variables, identify ones that you think may be problematic because they may have near-zero variance. Decide whether to remove them now, or remove them as part of your pre-processor. Make an argument for why your choice is appropriate.\n:::\n\n### Exercise 4.4\n\n::: {.callout-tip title=\"Question\"}\nFor all of the categorical variables, identify ones that you think may be problematic because they have many categories that don't have a lot of observations and likely need to be \"lumped\". Decide whether to remove them now, or remove them as part of your pre-processor. Make an argument for why your choice is appropriate.\n:::\n\n# Data Splitting & Preprocessing\n\n## Exercise 5\n\n::: {.callout-tip title=\"Question\"}\nSplit your data into training and test sets. Use your own judgement to determine training to test split ratio. Make sure to set a seed.\n:::\n\n\n## Exercise 6\n\n::: {.callout-tip title=\"Question\"}\nGenerate at least three different recipes designed to be used with linear regression that treat preprocessing differently. Hint: you'll likely want to try out different missing value imputation or lumping strategies. It's also a good idea to include `step_lincolm`.\n:::\n\n\n## Exercise 7\n\n::: {.callout-tip title=\"Question\"}\nGenerate at least three different recipes designed to be used with $K$-nearest neighbors that treat preprocessing differently. Hint: you'll likely want to try out different missing value imputation or lumping strategies.\n:::\n\n# Model-Fitting & Evaluation\n\n## Exercise 7\n\n::: {.callout-tip title=\"Question\"}\nCreate a `workflow_set` that contains 12 different workflows:\n\n-   three linear regression workflows: one linear regression model with each of the three recipes you created above\n-   nine different KNN workflows: choose three different $K$s for you KNN models and create one workflow for each combination of KNN model and preprocessing recipe\n:::\n\n## Exercise 8\n\n::: {.callout-tip title=\"Question\"}\nUse 5 fold CV with 5 repeats to compute the RMSE and R-squared for each of the 12 workflows you created above. Note that this step may take a few minutes to execute.\n:::\n\n## Exercise 9\n\n::: {.callout-tip title=\"Question\"}\nPlot the results of your cross validation and select your best workflow.\n:::\n\n## Exercise 10\n\n::: {.callout-tip title=\"Question\"}\nRe-fit your best model on the whole training set and estimate your error metrics on the test set.\n:::\n\n# Conceptual Question\n\n## Exercise 11 (Sample interview question)\n\n::: {.callout-tip title=\"Question\"}\nThe time to complete cross-validation can be substantially improved by using parallel processing. Below is the output for the copilot prompt \"Generate pseudo-code in R to do cross-validation with repetition and multiple models\". Which parts of this code can be run in parallel and which can't. Note any changes that you might need to make for this to be parallelizable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the number of folds (k) and the number of repetitions (r)\nk <- 5\nr <- 3\n\n# Define the list of models to evaluate\nmodels <- list(\n    model1 = train_model1,\n    model2 = train_model2,\n    model3 = train_model3\n)\n\n# Initialize a list to store the performance metrics for each model\nall_performance_metrics <- list()\n\n# Loop through each model\nfor (model_name in names(models)) {\n    # Initialize a list to store the performance metrics for this model\n    model_performance_metrics <- list()\n    \n    # Loop through each repetition\n    for (rep in 1:r) {\n        # Create k-fold cross-validation indices for this repetition\n        folds <- createFolds(dataset$target_variable, k = k)\n        \n        # Initialize a list to store the performance metrics for this repetition\n        performance_metrics <- list()\n        \n        # Loop through each fold\n        for (i in 1:k) {\n            # Use the i-th fold as the validation set\n            validation_indices <- folds[[i]]\n            validation_set <- dataset[validation_indices, ]\n            \n            # Use the remaining folds as the training set\n            training_set <- dataset[-validation_indices, ]\n            \n            # Train the model on the training set\n            model <- models[[model_name]](training_set)\n            \n            # Evaluate the model on the validation set\n            performance <- evaluate_model(model, validation_set)\n            \n            # Store the performance metric\n            performance_metrics[[i]] <- performance\n        }\n        \n        # Store the performance metrics for this repetition\n        model_performance_metrics[[rep]] <- performance_metrics\n    }\n    \n    # Store the performance metrics for this model\n    all_performance_metrics[[model_name]] <- model_performance_metrics\n}\n\n# Calculate the average performance metric for each model across all repetitions\naverage_performance <- sapply(all_performance_metrics, function(metrics) mean(unlist(metrics)))\n\n# Output the average performance for each model\nprint(\"Average Performance for each model:\")\nprint(average_performance)\n```\n:::\n\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}